Skip to content
Search or jump to…
Pull requests
Issues
Marketplace
Explore
 
@lucasSintraWarren 
maiyuri
/
atividade_02
Public
Code
Issues
Pull requests
Actions
Projects
Security
Insights
atividade_02/Arquivos_Entrada/acm_2.bib
@maiyuri
maiyuri subindo arquivos da atividade
Latest commit ffa70a9 23 hours ago
 History
 1 contributor
910 lines (860 sloc)  81.5 KB

@inproceedings{10.1145/3312614.3312623,
author = {Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul},
title = {The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges},
year = {2019},
isbn = {9781450366403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312614.3312623},
doi = {10.1145/3312614.3312623},
abstract = {Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.},
booktitle = {Proceedings of the International Conference on Omni-Layer Intelligent Systems},
pages = {19–24},
numpages = {6},
keywords = {data generation, data storage, Big Data, data characteristics},
location = {Crete, Greece},
series = {COINS '19}
}

@inproceedings{10.1145/2983323.2983345,
author = {Zhu, Fangzhou and Luo, Chen and Yuan, Mingxuan and Zhu, Yijian and Zhang, Zhengqing and Gu, Tao and Deng, Ke and Rao, Weixiong and Zeng, Jia},
title = {City-Scale Localization with Telco Big Data},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983345},
doi = {10.1145/2983323.2983345},
abstract = {It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {439–448},
numpages = {10},
keywords = {localization, regression models, telco big data},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/3472163.3472195,
author = {Sahri, Soror and Moussa, Rim},
title = {Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472195},
doi = {10.1145/3472163.3472195},
abstract = {Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi’ trips.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {157–165},
numpages = {9},
keywords = {Big data, Veracity},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3003733.3003767,
author = {Petrou, Charilaos and Paraskevas, Michael},
title = {Signal Processing Techniques Restructure The Big Data Era},
year = {2016},
isbn = {9781450347891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3003733.3003767},
doi = {10.1145/3003733.3003767},
abstract = {Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.},
booktitle = {Proceedings of the 20th Pan-Hellenic Conference on Informatics},
articleno = {52},
numpages = {6},
keywords = {signal processing techniques, statistical learning tools, stochastic approximation, big data, convex optimization},
location = {Patras, Greece},
series = {PCI '16}
}

@inproceedings{10.1145/3524383.3524442,
author = {Cai, Mingjun and Sam, Francis and Asante Boadi, Evans},
title = {Research on the Application of Big Data in University's Public Opinion Monitoring and Processing},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3524442},
doi = {10.1145/3524383.3524442},
abstract = {Higher educational institutions rely on reputation to attract and earn the legitimacy of the public. However, the emerging trend of public misinformation due to the chunk of information available on the internet affects public opinion formation (POF) about universities. Therefore, narrowing down on the fallouts in POF can provide evidence for managerial and policy interventions. This paper explores five-layer POF and its application with big data in university public opinion monitoring and processing. It reveals that big data technology can be actively employed to safeguard the image of university public opinion formation, but this can be inhibited by the lack of commitment to integrating multiple stakeholders on a common platform, privacy, and security concerns. The paper recommends collaboration between universities and the government to increase momentum on big data with requisite actions for mutual benefits.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {121–126},
numpages = {6},
keywords = {public opinion system, Big data, university public opinion, data analysis},
location = {Shanghai, China},
series = {ICBDE '22}
}

@inproceedings{10.1145/3168390.3168425,
author = {Samosir, Ridha Sefina and Hendric, Harco Leslie and Gaol, Ford Lumban and Abdurachman, Edi and Soewito, Benfano},
title = {Measurement Metric Proposed For Big Data Analytics System},
year = {2017},
isbn = {9781450353922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168390.3168425},
doi = {10.1145/3168390.3168425},
abstract = {Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.},
booktitle = {Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence},
pages = {265–269},
numpages = {5},
keywords = {Measurement, Metric, Software, Big Data Analytics},
location = {Jakarta, Indonesia},
series = {CSAI 2017}
}

@article{10.1145/2331042.2331058,
author = {Heer, Jeffrey and Kandel, Sean},
title = {Interactive Analysis of Big Data},
year = {2012},
issue_date = {Fall 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/2331042.2331058},
doi = {10.1145/2331042.2331058},
abstract = {New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.},
journal = {XRDS},
month = {sep},
pages = {50–54},
numpages = {5}
}

@article{10.1145/2935753,
author = {Berti-Equille, Laure and Ba, Mouhamadou Lamine},
title = {Veracity of Big Data: Challenges of Cross-Modal Truth Discovery},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/2935753},
doi = {10.1145/2935753},
journal = {J. Data and Information Quality},
month = {aug},
articleno = {12},
numpages = {3},
keywords = {data fusion, information extraction, Truth discovery, data quality, fact checking}
}

@inproceedings{10.1145/3503928.3503929,
author = {Barzan Abdalla, Hemn and Mustafa, Nasser and Ihnaini, Baha},
title = {Big Data: Finding Frequencies of Faulty Multimedia Data},
year = {2021},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503929},
doi = {10.1145/3503928.3503929},
abstract = {In many health care domains, big data has arrived. How to manage and use big data better has become the focus of all walks of life. Many data sources provide the repeated fault data—the repeated fault data forming the delay of processing time and storage capacity. Big data includes properties like volume, velocity, variety, variability, value, complexity, and performance put forward more challenges. Most healthcare domains face the problem of testing for structured and unstructured data validation in big data. It provides low-quality data and delays in response. In testing process is delay and not provide the correct response. In Proposed, pre-testing and post-testing are used for big data testing. In pre-testing, classify fault data from different data sources. After Classification to group big data using SVM algorithms such as Text, Image, Audio, and Video file. In post-testing, to implement the pre-processing, remove the zero file size, unrelated file extension, and de-duplication after pre-processing to implement the Map-reduce algorithm to find out the big data efficiently. This process reduces the pre-processing time, reduces the server energy, and increases the processing time. To remove the fault data before pre-processing means to increase the processing time and data storage.},
booktitle = {2021 the 6th International Conference on Information Systems Engineering},
pages = {1–6},
numpages = {6},
keywords = {Map-reduce, Classification using SVM, Pre-Processing, Fault data detection, Big Data},
location = {Shanghai, China},
series = {ICISE 2021}
}

@inproceedings{10.1145/3404687.3404693,
author = {Xin, Li and Tianyun, Shi and Xiaoning, Ma},
title = {Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System},
year = {2020},
isbn = {9781450375474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404687.3404693},
doi = {10.1145/3404687.3404693},
abstract = {In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Computing},
pages = {6–12},
numpages = {7},
keywords = {Locomotive system, Key technology, Application platform, Big data, Railway},
location = {Chengdu, China},
series = {ICBDC '20}
}

@article{10.1145/3419634,
author = {Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n},
title = {A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions},
year = {2020},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3419634},
doi = {10.1145/3419634},
abstract = {Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {131},
numpages = {59},
keywords = {big data 2.0, cloud computing in IoT, V’s challenges for IoT big data, cloud IoT services, IoT big data, IoT big data survey}
}

@inproceedings{10.1145/3418688.3418697,
author = {Liou, Teau-Jiuan and Weng, Ming-Wei and Lee, Liza},
title = {The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0},
year = {2020},
isbn = {9781450387866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418688.3418697},
doi = {10.1145/3418688.3418697},
abstract = {The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.},
booktitle = {2020 the 3rd International Conference on Computing and Big Data},
pages = {48–54},
numpages = {7},
keywords = {Digital transformation, Multi-stage assembly, Industrie 4.0, Big data},
location = {Taichung, Taiwan},
series = {ICCBD '20}
}

@inproceedings{10.1145/3472163.3472171,
author = {Bhardwaj, Dave and Ormandjieva, Olga},
title = {Rigorous Measurement Model for Validity of Big Data: MEGA Approach},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472171},
doi = {10.1145/3472163.3472171},
abstract = {Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.},
booktitle = {Proceedings of the 25th International Database Engineering &amp; Applications Symposium},
pages = {285–291},
numpages = {7},
keywords = {Quality Characteristics (V's), Measurement Hierarchical Model, Big Data, Validity, Representational Theory of Measurement,},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/2463676.2463707,
author = {Sumbaly, Roshan and Kreps, Jay and Shah, Sam},
title = {The Big Data Ecosystem at LinkedIn},
year = {2013},
isbn = {9781450320375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463676.2463707},
doi = {10.1145/2463676.2463707},
abstract = {The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.},
booktitle = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
pages = {1125–1134},
numpages = {10},
keywords = {machine learning, hadoop, data mining, offline processing, big data, data pipeline},
location = {New York, New York, USA},
series = {SIGMOD '13}
}

@article{10.1145/3186549.3186559,
author = {Sadiq, Shazia and Dasu, Tamraparni and Dong, Xin Luna and Freire, Juliana and Ilyas, Ihab F. and Link, Sebastian and Miller, Miller J. and Naumann, Felix and Zhou, Xiaofang and Srivastava, Divesh},
title = {Data Quality: The Role of Empiricism},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/3186549.3186559},
doi = {10.1145/3186549.3186559},
abstract = {We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.},
journal = {SIGMOD Rec.},
month = {feb},
pages = {35–43},
numpages = {9}
}

@inproceedings{10.1145/2623330.2623615,
author = {Anagnostopoulos, Christos and Triantafillou, Peter},
title = {Scaling out Big Data Missing Value Imputations: Pythia vs. Godzilla},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623615},
doi = {10.1145/2623330.2623615},
abstract = {Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {651–660},
numpages = {10},
keywords = {big data, missing value, clustering},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1145/3178315.3178323,
author = {Arruda, Darlan},
title = {Requirements Engineering in the Context of Big Data Applications},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3178315.3178323},
doi = {10.1145/3178315.3178323},
abstract = {Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–6},
numpages = {6},
keywords = {big data applications, big data requirements engineering, business goals, empirical software engineering., empirical studies}
}

@inproceedings{10.1145/3297730.3297743,
author = {Liu, Yi and Peng, Jiawen and Yu, Zhihao},
title = {Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297743},
doi = {10.1145/3297730.3297743},
abstract = {With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {31–35},
numpages = {5},
keywords = {Financial technology, time and space data, platform architecture, insurance industry, big data platform},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/3545801.3545802,
author = {Liu, Xiaobao and Li, Qi and Zhu, Shaosong and Wang, Cong and Meng, Lingzhen},
title = {A Multi-Head Attention Mechanism Base Multi-Dimensional Data Quality Evaluation Model},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545802},
doi = {10.1145/3545801.3545802},
abstract = {High-quality power data is the basis for reliable operation of power systems, efficient data processing, and effective mining of the potential value of power data. How to use big data, artificial intelligence and other technologies to evaluate the quality of power data is a hot research topic in the field of electric power. At present, most of the power data quality evaluation methods are simple and lack the research of general data quality evaluation model. Therefore, this paper proposes a multi-dimensional data quality evaluation model based on a multi-head attention mechanism. The model measures multiple indicators such as completeness, accuracy, smoothness, and correlation. The corresponding methods are used to quantify these indicators to form a data quality evaluation index system oriented to multi-dimensional indicators; then, an application feedback mechanism based on a multi-head attention network is used to correct the calculation weights and score outputs, so as to achieve the evaluation of power data quality. Finally, the validation analysis is carried out based on the electricity data of a region in China. The experimental results show that the proposed method can effectively evaluate the quality of electric power data.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {1–5},
numpages = {5},
keywords = {Electric power data, multi-dimensional indicators, multi-head attention, data quality evaluation},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@inproceedings{10.1145/3297730.3297731,
author = {Chen, Rui-Yang},
title = {Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System},
year = {2018},
isbn = {9781450365826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297730.3297731},
doi = {10.1145/3297730.3297731},
abstract = {How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.},
booktitle = {Proceedings of the 2018 International Conference on Big Data Engineering and Technology},
pages = {26–30},
numpages = {5},
keywords = {fuzzy case-based reasoning, big data-streaming, Target data optimization, fuzzy decision tree},
location = {Chengdu, China},
series = {BDET 2018}
}

@inproceedings{10.1145/2513190.2517828,
author = {Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol},
title = {Data Warehousing and OLAP over Big Data: Current Challenges and Future Research Directions},
year = {2013},
isbn = {9781450324120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513190.2517828},
doi = {10.1145/2513190.2517828},
abstract = {In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.},
booktitle = {Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP},
pages = {67–70},
numpages = {4},
keywords = {big multidimensional data, olap, big data, data warehousing},
location = {San Francisco, California, USA},
series = {DOLAP '13}
}

@inproceedings{10.1145/3372454.3372478,
author = {Puangpontip, Supadchaya and Hewett, Rattikorn},
title = {Assessing Reliability of Big Data Stream for Smart City},
year = {2019},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372478},
doi = {10.1145/3372454.3372478},
abstract = {Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.},
booktitle = {Proceedings of the 2019 3rd International Conference on Big Data Research},
pages = {18–23},
numpages = {6},
keywords = {Smart City, Data Reliability, IoT, Theory of evidence},
location = {Cergy-Pontoise, France},
series = {ICBDR 2019}
}

@inproceedings{10.1145/3396956.3398253,
author = {Potiguara Carvalho, Artur and Potiguara Carvalho, Fernanda and Dias Canedo, Edna and Potiguara Carvalho, Pedro Henrique},
title = {Big Data, Anonymisation and Governance to Personal Data Protection},
year = {2020},
isbn = {9781450387910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396956.3398253},
doi = {10.1145/3396956.3398253},
abstract = {In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy.},
booktitle = {The 21st Annual International Conference on Digital Government Research},
pages = {185–195},
numpages = {11},
keywords = {Personal Data Protection, Big Data, Governance, Privacy, Anonymisation},
location = {Seoul, Republic of Korea},
series = {dg.o '20}
}

@inproceedings{10.1145/3415958.3433082,
author = {Dessalk, Yared Dejene and Nikolov, Nikolay and Matskin, Mihhail and Soylu, Ahmet and Roman, Dumitru},
title = {Scalable Execution of Big Data Workflows Using Software Containers},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433082},
doi = {10.1145/3415958.3433082},
abstract = {Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {76–83},
numpages = {8},
keywords = {Big Data workflows, Domain-specific languages, Software containers},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3085228.3085275,
author = {Gong, Yiwei and Janssen, Marijn},
title = {Enterprise Architectures for Supporting the Adoption of Big Data},
year = {2017},
isbn = {9781450353175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3085228.3085275},
doi = {10.1145/3085228.3085275},
abstract = {Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.},
booktitle = {Proceedings of the 18th Annual International Conference on Digital Government Research},
pages = {505–510},
numpages = {6},
keywords = {big data, e-government, enterprise architecture, BOLD, ICT-architecture, open data, infrastructure},
location = {Staten Island, NY, USA},
series = {dg.o '17}
}

@inproceedings{10.1145/3453187.3453340,
author = {Hu, Zhifeng and Zhao, Feng and Zhao, Xiaona},
title = {Research on Smart Education Service Platform Based on Big Data},
year = {2020},
isbn = {9781450389099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453187.3453340},
doi = {10.1145/3453187.3453340},
abstract = {The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.},
booktitle = {Proceedings of the 2020 3rd International Conference on E-Business, Information Management and Computer Science},
pages = {228–233},
numpages = {6},
keywords = {Information-oriented education, Smart education, Big data},
location = {Wuhan, China},
series = {EBIMCS 2020}
}

@inproceedings{10.5555/2840819.2840927,
author = {Zhu, Yada and Xiong, Jinjun},
title = {Modern Big Data Analytics for "Old-Fashioned" Semiconductor Industry Applications},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an "old-fashioned" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {776–780},
numpages = {5},
keywords = {manufacturing, Big data, analytics, semiconductor},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/3433996.3434027,
author = {Ding, Shifu and Liu, Yan and Zhang, Jianjun and Tan, Yaqi and Li, Xiaoxia and Tang, RuiChun},
title = {The Planning and Construction of Healthcare Big Data Platform},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434027},
doi = {10.1145/3433996.3434027},
abstract = {Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates "difficulty and expensive" problem effectively.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {170–176},
numpages = {7},
keywords = {Healthcare, Big Data, SOA, Electronic Health Record, EMR},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/2676536.2676538,
author = {Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng},
title = {High Performance Integrated Spatial Big Data Analytics},
year = {2014},
isbn = {9781450331326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676536.2676538},
doi = {10.1145/2676536.2676538},
abstract = {The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
pages = {11–14},
numpages = {4},
keywords = {spatial analytics, MapReduce, database, data warehouse, GIS},
location = {Dallas, Texas},
series = {BigSpatial '14}
}

@inproceedings{10.1145/3424978.3425010,
author = {Man, Rui and Zhou, Guomin and Fan, Jingchao},
title = {Research on Scientific Data Management in Big Data Era},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425010},
doi = {10.1145/3424978.3425010},
abstract = {Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {32},
numpages = {6},
keywords = {Scientific data, Big data, Opening and sharing of data resource, Scientific data management},
location = {Sanya, China},
series = {CSAE 2020}
}

@article{10.1145/2998575,
author = {Labouseur, Alan G. and Matheus, Carolyn C.},
title = {An Introduction to Dynamic Data Quality Challenges},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/2998575},
doi = {10.1145/2998575},
journal = {J. Data and Information Quality},
month = {jan},
articleno = {6},
numpages = {3},
keywords = {internet of things, graph systems, relational systems, big data, Dynamic data quality}
}

@inproceedings{10.1145/3508259.3508284,
author = {Sun, Yu and Niu, Yanfang and Lu, Le},
title = {Research on Influencing Factors of Government Audit Big Data Capability},
year = {2021},
isbn = {9781450384162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508259.3508284},
doi = {10.1145/3508259.3508284},
abstract = {The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.},
booktitle = {2021 4th Artificial Intelligence and Cloud Computing Conference},
pages = {172–178},
numpages = {7},
keywords = {Big data analysis capability, Government audit, Influencing factors},
location = {Kyoto, Japan},
series = {AICCC '21}
}

@article{10.1145/2481244.2481247,
author = {Lin, Jimmy and Ryaboy, Dmitriy},
title = {Scaling Big Data Mining Infrastructure: The Twitter Experience},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/2481244.2481247},
doi = {10.1145/2481244.2481247},
abstract = {The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on "big data". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life "in the trenches" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall "big picture" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as "plumbing". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.},
journal = {SIGKDD Explor. Newsl.},
month = {apr},
pages = {6–19},
numpages = {14}
}

@article{10.1145/3383464,
author = {Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv},
title = {SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383464},
doi = {10.1145/3383464},
abstract = {Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {46},
numpages = {40},
keywords = {big data analytics application, service layer, SLA, service level agreement, SLA metrics, Big data}
}

@article{10.1145/2932707,
author = {Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.},
title = {Computational Health Informatics in the Big Data Age: A Survey},
year = {2016},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2932707},
doi = {10.1145/2932707},
abstract = {The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.},
journal = {ACM Comput. Surv.},
month = {jun},
articleno = {12},
numpages = {36},
keywords = {Big data analytics, machine learning, 4V challenges, data mining, computational health informatics, clinical decision support, survey}
}

@inproceedings{10.1145/3402569.3409041,
author = {Han, Ping},
title = {Research on Foreign Exchange Management Model Based on Big Data},
year = {2020},
isbn = {9781450377546},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402569.3409041},
doi = {10.1145/3402569.3409041},
abstract = {The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.},
booktitle = {Proceedings of the 5th International Conference on Distance Education and Learning},
pages = {162–165},
numpages = {4},
keywords = {mode, Big data background, foreign exchange management},
location = {Beijing, China},
series = {ICDEL 2020}
}

@inproceedings{10.1145/2791347.2791380,
author = {Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken},
title = {Privacy-Preserving Big Data Publishing},
year = {2015},
isbn = {9781450337090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791347.2791380},
doi = {10.1145/2791347.2791380},
abstract = {The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.},
booktitle = {Proceedings of the 27th International Conference on Scientific and Statistical Database Management},
articleno = {26},
numpages = {11},
location = {La Jolla, California},
series = {SSDBM '15}
}

@inproceedings{10.1145/3374587.3374650,
author = {Shen, Shaoyi and Li, Bin and Li, Situo},
title = {Construction and Application of Big Data Analysis Platform for Enterprise},
year = {2019},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374650},
doi = {10.1145/3374587.3374650},
abstract = {A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {54–58},
numpages = {5},
keywords = {Data asset, Construction, Analysis of big data, Shared, Distributed},
location = {Normal, IL, USA},
series = {CSAI2019}
}

@inproceedings{10.1145/2815782.2815793,
author = {Malaka, Iman and Brown, Irwin},
title = {Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815793},
doi = {10.1145/2815782.2815793},
abstract = {The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {27},
numpages = {9},
keywords = {Big Data, South Africa, Big Data Analytics, Technology Adoption},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@inproceedings{10.1145/3193063.3193069,
author = {Cheng, Susu and Zhao, Haijun},
title = {An Overview of Techniques for Confirming Big Data Property Rights},
year = {2018},
isbn = {9781450363785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193063.3193069},
doi = {10.1145/3193063.3193069},
abstract = {The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.},
booktitle = {Proceedings of the 2018 International Conference on Intelligent Information Technology},
pages = {59–64},
numpages = {6},
keywords = {Confirmation of Information Property, Information property index, Big Data, Method for confirming information property rights},
location = {Ha Noi, Viet Nam},
series = {ICIIT 2018}
}

@inproceedings{10.1145/3501409.3501593,
author = {Diao, Yanhua},
title = {Tourism Prediction Based on Multi-Source Big Data Fusion Technology},
year = {2021},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501593},
doi = {10.1145/3501409.3501593},
abstract = {In the practical application of existing big data tourism prediction, there are some practical problems, such as complicated data sources and difficult fusion, low prediction accuracy and poor guiding practice effect. In view of this situation, this paper intends to build a tourism big data index prediction model suitable for the characteristics of tourism development through core data extraction, multi-source data fusion, complex data modeling and other key technologies. With the help of the improved tourism prediction model based on multi-source big data fusion technology, the tourist flow and consumption characteristics of Shandong province are more accurately identified and predicted. It can provide help for optimizing public service of tourism, strengthening early warning of tourist flow and improving marketing strategy of tourist destination. This study innovatively supplements the effective integration theory of multi-source tourism big data and the organic integration theory of big data and traditional sampling survey data. At the same time, the relevant methods of tourism big data forecasting model are extended.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1030–1036},
numpages = {7},
keywords = {Data fusion technology, Multi-source big data, Tourism prediction},
location = {Xiamen, China},
series = {EITCE 2021}
}

@inproceedings{10.1145/3357292.3357302,
author = {Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang},
title = {Big Data Informatization Applied to Optimization of Human Resource Performance Management},
year = {2019},
isbn = {9781450371445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357292.3357302},
doi = {10.1145/3357292.3357302},
abstract = {With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.},
booktitle = {Proceedings of the 2019 2nd International Conference on Information Management and Management Sciences},
pages = {12–17},
numpages = {6},
keywords = {performance management, human resources, Big data},
location = {Chengdu, China},
series = {IMMS 2019}
}

@inproceedings{10.1145/3167918.3167924,
author = {Al-Mansoori, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav},
title = {A Survey on Big Data Stream Processing in SDN Supported Cloud Environment},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167924},
doi = {10.1145/3167918.3167924},
abstract = {Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {12},
numpages = {11},
keywords = {SDN, cloud computing, big data, cost optimization, resource optimization, big data stream processing},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@inproceedings{10.1145/3297662.3365797,
author = {Musto, Jiri and Dahanayake, Ajantha},
title = {Integrating Data Quality Requirements to Citizen Science Application Design},
year = {2019},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365797},
doi = {10.1145/3297662.3365797},
abstract = {Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {166–173},
numpages = {8},
keywords = {Data Quality requirements, Data Quality Characteristics, Data quality, Citizen science, Conceptual model},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3220228.3220236,
author = {Saraee, Mo and Silva, Charith},
title = {A New Data Science Framework for Analysing and Mining Geospatial Big Data},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220236},
doi = {10.1145/3220228.3220236},
abstract = {Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {98–102},
numpages = {5},
keywords = {machine learning, big data, data mining, geospatial big data, data science},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3194206.3194229,
author = {Zhichao, Xu and Jiandong, Zhao and Huan, Huang},
title = {Based on Hadoop's Tech Big Data Combination and Mining Technology Framework},
year = {2018},
isbn = {9781450363457},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194206.3194229},
doi = {10.1145/3194206.3194229},
abstract = {With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.},
booktitle = {Proceedings of the 2nd International Conference on Innovation in Artificial Intelligence},
pages = {59–63},
numpages = {5},
keywords = {tech big data, Hadoop, combination, mining},
location = {Shanghai, China},
series = {ICIAI '18}
}

@inproceedings{10.1145/3524383.3524431,
author = {Wu, Min and Hao, Xinxin and Wan, Xuehong and Ma, Chenwei and Wu, Yu},
title = {Opportunities and Challenges of Joint Training of Postgraduate Students by the University-Industry Collaboration Institutions in Big Data Era},
year = {2022},
isbn = {9781450395793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524383.3524431},
doi = {10.1145/3524383.3524431},
abstract = {University-industry cooperative education is an important way to cultivate graduate students' innovative ability and practical ability. However, there are some problems in the traditional joint training model of graduate students, such as low efficiency, conflict of objectives of cooperative subjects, a mismatch between supply and demand of cooperative entities, and so on. The big data technology has brought new opportunities and challenges to the joint training of graduate students by university-industry cooperation institutions. Based on analyzing the connotation and characteristics of the big data era, the paper points out that the arrival of the big data era can improve the information integration efficiency of university-industry cooperation institutions, optimize the traditional joint training model of graduate students, and provide an effective evaluation mechanism of educational quality for university-industry cooperation institutions. At the same time, the paper discusses the difficulties of data collection and disclosure of data privacy faced by university-industry cooperative education in the big data era. The paper also discusses how to deal with the challenges from the perspective of the government, colleges and universities, scientific research institutions and enterprises.},
booktitle = {Proceedings of the 5th International Conference on Big Data and Education},
pages = {194–198},
numpages = {5},
keywords = {big data, cooperative education of university-industry institutions, postgraduate education},
location = {Shanghai, China},
series = {ICBDE '22}
}

@article{10.1145/2094114.2094129,
author = {Bizer, Christian and Boncz, Peter and Brodie, Michael L. and Erling, Orri},
title = {The Meaningful Use of Big Data: Four Perspectives -- Four Challenges},
year = {2012},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0163-5808},
url = {https://doi.org/10.1145/2094114.2094129},
doi = {10.1145/2094114.2094129},
abstract = {Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.},
journal = {SIGMOD Rec.},
month = {jan},
pages = {56–60},
numpages = {5}
}

@inproceedings{10.1145/3321454.3321474,
author = {Yu, Bangbo and Zhao, Haijun},
title = {Research on the Construction of Big Data Trading Platform in China},
year = {2019},
isbn = {9781450366335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321454.3321474},
doi = {10.1145/3321454.3321474},
abstract = {As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.},
booktitle = {Proceedings of the 2019 4th International Conference on Intelligent Information Technology},
pages = {107–112},
numpages = {6},
keywords = {regulatory construction, big data trading platform, Data assets},
location = {Da, Nang, Viet Nam},
series = {ICIIT '19}
}

@inproceedings{10.1145/3234698.3234723,
author = {El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha},
title = {Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234723},
doi = {10.1145/3234698.3234723},
abstract = {Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.},
booktitle = {Proceedings of the Fourth International Conference on Engineering &amp; MIS 2018},
articleno = {25},
numpages = {9},
keywords = {Data Warehouse, Big Data, Cloud Computing, Business Intelligence},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}