@incollection{ZEITOUNI2020159,
title = {Chapter 8 - Query Processing and Access Methods for Big Astro and Geo Databases},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {159-171},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00018-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000187},
author = {Karine Zeitouni and Mariem Brahem and Laurent Yeh and Atanas Hristov},
keywords = {Spatial databases, Spatial access methods, Query optimization, Big Data, System architecture},
abstract = {In spite of their development in different communities, either astro-informatics or geo-informatics, data management and analytics of astronomical and geospatial data share the same characteristics, and raise the same challenges when it comes to access, query, or analysis of the spatial features over Big Data. The very first challenge is to deal with the data volume, which is tremendous in many geo and astro datasets. In this chapter, we highlight their main specificity and outline the main steps of query processing in big geospatial and astronomical data servers. Through the review of the state of the art, we show the advance in the topic of Big Data management in both contexts of geospatial and sky surveying, while highlighting their similarity. This progress notwithstanding, several issues remain to deal with the variety (such as multidimensional arrays) of the data.}
}
@article{ECKARDT2020406,
title = {Opioid use disorder research and the Council for the Advancement of Nursing Science priority areas},
journal = {Nursing Outlook},
volume = {68},
number = {4},
pages = {406-416},
year = {2020},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2020.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0029655419306931},
author = {Patricia Eckardt and Donald Bailey and Holli A. DeVon and Cynthia Dougherty and Pamela Ginex and Cheryl A. Krause-Parello and Rita H. Pickler and Therese S. Richmond and Eleanor Rivera and Carol F. Roye and Nancy Redeker},
keywords = {Precision health, Big data and Data analytics, Determinants of health, Global health, Opioid use disorder research},
abstract = {Background
Chronic diseases, such as opioid use disorder (OUD) require a multifaceted scientific approach to address their evolving complexity. The Council for the Advancement of Nursing Science's (Council) four nursing science priority areas (precision health; global health, determinants of health, and big data/data analytics) were established to provide a framework to address current complex health problems.
Purpose
To examine OUD research through the nursing science priority areas and evaluate the appropriateness of the priority areas as a framework for research on complex health conditions.
Method
OUD was used as an exemplar to explore the relevance of the nursing science priorities for future research.
Findings
Research in the four priority areas is advancing knowledge in OUD identification, prevention, and treatment. Intersection of OUD research population focus and methodological approach was identified among the priority areas.
Discussion
The Council priorities provide a relevant framework for nurse scientists to address complex health problems like OUD.}
}
@article{ZHANG2020100715,
title = {Knowledge mapping of tourism demand forecasting research},
journal = {Tourism Management Perspectives},
volume = {35},
pages = {100715},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2020.100715},
url = {https://www.sciencedirect.com/science/article/pii/S2211973620300829},
author = {Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei},
keywords = {Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic},
abstract = {Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.}
}
@article{PASIDIS2019301,
title = {Congestion by accident? A two-way relationship for highways in England},
journal = {Journal of Transport Geography},
volume = {76},
pages = {301-314},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300704},
author = {Ilias Pasidis},
keywords = {Accidents, Traffic congestion, Big data, Highways, England},
abstract = {This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.}
}
@article{ZHANG2018149,
title = {Product features characterization and customers’ preferences prediction based on purchasing data},
journal = {CIRP Annals},
volume = {67},
number = {1},
pages = {149-152},
year = {2018},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0007850618300441},
author = {Jian Zhang and Alessandro Simeone and Peihua Gu and Bo Hong},
keywords = {Design, Product development, Big data},
abstract = {Big data of online product purchases is an emerging source for obtaining customers’ preferences of product features for new product development. This paper proposes a framework and associated method for product features characterization and customers’ preference prediction based on online product purchase data. Specifications and components of products are firstly analyzed and the relationships between product specifications and components are then established for features characterization. The customers preferred specifications, features and their combinations are predicted for development of new products. The features characterization and customers’ preferences prediction of toy cars were used as an example of illustrating the proposed method.}
}
@article{MOON2018304,
title = {Evaluating fidelity of lossy compression on spatiotemporal data from an IoT enabled smart farm},
journal = {Computers and Electronics in Agriculture},
volume = {154},
pages = {304-313},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918303697},
author = {Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son},
keywords = {Smart farm, Lossy compression, IoT, Signal processing, Data fidelity},
abstract = {As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.}
}
@article{XU2022105396,
title = {An overview of visualization and visual analytics applications in water resources management},
journal = {Environmental Modelling & Software},
volume = {153},
pages = {105396},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105396},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001025},
author = {Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal},
keywords = {Big Data, Hydroinformatics, Visual Analytics, Visualization, Human-Computer Interaction},
abstract = {Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for water resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of hydroinformatics applications.}
}
@article{LI2020124178,
title = {Forecasting crude oil price with multilingual search engine data},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {551},
pages = {124178},
year = {2020},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2020.124178},
url = {https://www.sciencedirect.com/science/article/pii/S037843712030025X},
author = {Jingjing Li and Ling Tang and Shouyang Wang},
keywords = {Big data, Multilingual search engine index, Crude oil price forecasting, Google Trends, Artificial intelligence},
abstract = {In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.}
}
@article{JANSSEN2020101493,
title = {Data governance: Organizing data for trustworthy Artificial Intelligence},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101493},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101493},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302719},
author = {Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski},
keywords = {Big data, Data governance, AI, Algorithmic governance, Information sharing, Artificial Intelligence, Trusted frameworks},
abstract = {The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.}
}
@incollection{MCGILVRAY2021253,
title = {Chapter 5 - Structuring Your Project},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {253-267},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000013},
author = {Danette McGilvray},
keywords = {Solution Development Life Cycle (SDLC), Agile, Scrum, sequential, waterfall, project objectives, project team, project roles, project timing, project approach},
abstract = {It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers’ choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution/system/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.}
}
@article{CAPPIELLO2022101874,
title = {Assessing and improving measurability of process performance indicators based on quality of logs},
journal = {Information Systems},
volume = {103},
pages = {101874},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101874},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000995},
author = {Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim},
keywords = {Business process, Event log, Data quality assessment, Data quality improvement},
abstract = {The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.}
}
@article{SAYAD2019130,
title = {Predictive modeling of wildfires: A new dataset and machine learning approach},
journal = {Fire Safety Journal},
volume = {104},
pages = {130-146},
year = {2019},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0379711218303941},
author = {Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}},
keywords = {Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence},
abstract = {Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.}
}
@article{GHORBANIAN2020276,
title = {Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {167},
pages = {276-288},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302008},
author = {Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou},
keywords = {Land cover classification, Sentinel, Google Earth Engine, Big data, Remote sensing, Iran},
abstract = {Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10 m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.}
}
@article{ROBERTSON2020214,
title = {An integrated environmental analytics system (IDEAS) based on a DGGS},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {162},
pages = {214-228},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300502},
author = {Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts},
keywords = {DGGS, Data model, Big data, Spatial data, Analytics, Environment},
abstract = {Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS – integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.}
}
@incollection{SEBASTIANCOLEMAN202269,
title = {Chapter 4 - The Data Challenge: The Mechanics of Meaning},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {69-92},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000043},
author = {Laura Sebastian-Coleman},
keywords = {History of data, statistics, scientific data, organizational data, relational data, characteristics of data},
abstract = {This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand “data as data.” But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.}
}
@article{MUNAPPY2022111359,
title = {Data management for production quality deep learning models: Challenges and solutions},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111359},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111359},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000905},
author = {Aiswarya Raj Munappy and Jan Bosch and Helena Holmström Olsson and Anders Arpteg and Björn Brinne},
keywords = {Deep learning, Data management, Production quality DL models, Challenges, Solutions, Validation},
abstract = {Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions.}
}
@article{GUNDLA2016460,
title = {Creating NoSQL Biological Databases with Ontologies for Query Relaxation},
journal = {Procedia Computer Science},
volume = {91},
pages = {460-469},
year = {2016},
note = {Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.120},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916313138},
author = {Naresh Kumar Gundla and Zhengxin Chen},
keywords = {NoSQL databases, Query Relaxation, Ontology, MongoDB, AllgroGraph},
abstract = {The complexity of building biological databases is well-known and ontologies play an extremely important role in biological databases. However, much of the emphasis on the role of ontologies in biological databases has been on the construction of databases. In this paper, we explore a somewhat overlooked aspect regarding ontologies in biological databases, namely, how ontologies can be used to assist better database retrieval. In particular, we show how ontologies can be used to revise user submitted queries for query relaxation. In addition, since our research is conducted at today's “big data” era, our investigation is centered on NoSQL databases which serve as a kind of “representatives” of big data. This paper contains two major parts: First we describe our methodology of building two NoSQL application databases (MongoDB and AllegroGraph) using GO ontology, and then discuss how to achieve query relaxation through GO ontology. We report our experiments and show sample queries and results. Our research on query relaxation on NoSQL databases is complementary to existing work in big data and in biological databases and deserves further exploration.}
}
@article{ALTURJMAN2020357,
title = {Intelligence and security in big 5G-oriented IoNT: An overview},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {357-368},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301074},
author = {Fadi Al-Turjman},
keywords = {IoNT, Security, Big data, Design factors},
abstract = {Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.}
}
@article{PAIS2019100194,
title = {An automated workflow for MALDI-ToF mass spectra pattern identification on large data sets: An application to detect aneuploidies from pregnancy urine},
journal = {Informatics in Medicine Unlocked},
volume = {16},
pages = {100194},
year = {2019},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2019.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819300851},
author = {Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles},
keywords = {MALDI-ToF, Pattern recognition, Quality control, Comparative intensity data, Automated processing},
abstract = {Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.}
}
@article{JEONG2019358,
title = {Cohort profile: Beyond birth cohort study – The Korean CHildren's ENvironmental health Study (Ko-CHENS)},
journal = {Environmental Research},
volume = {172},
pages = {358-366},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0013935118306388},
author = {Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha},
keywords = {Ko-CHENS, Children, Environment, Cohort profile, Birth cohort},
abstract = {The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.}
}
@article{AGANY20201704,
title = {Assessment of vector-host-pathogen relationships using data mining and machine learning},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {1704-1721},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020303202},
author = {Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba},
keywords = {Systems Bioscience, OMICs, Pathogenicity, Transmission, Adaptation, Data Mining, Big Data, Machine Learning, Association Mining, Host-Pathogen, Interaction, Infectious Disease, Vector-Borne Disease},
abstract = {Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.}
}
@article{AYVAZ2021114598,
title = {Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114598},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114598},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421000397},
author = {Serkan Ayvaz and Koray Alpay},
keywords = {Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data},
abstract = {In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.}
}
@article{FERREIRA2021757,
title = {How do data scientists and managers influence machine learning value creation?},
journal = {Procedia Computer Science},
volume = {181},
pages = {757-764},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.228},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002714},
author = {Humberto Ferreira and Pedro Ruivo and Carolina Reis},
keywords = {machine learning, business value, data scientists, managers, people factor},
abstract = {Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.}
}
@incollection{KRISHNAN202085,
title = {4 - Scientific research applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {85-97},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000041},
author = {Krish Krishnan},
keywords = {CERN, God particle, Large electron–positron collider, Large hadron collider, Quarks, Scientific research, Standard model},
abstract = {Scientific research is one area of applications and usage of big data where we can generate lots of data in a single experiment and perform complex analytics on the same in the outcome of that experiment. The most famous example that we can talk about is the usage of all infrastructure technologies in the discovery of the “God particle” or “Higgs boson particle” which is leading us to uncover more exploration around the universe.}
}
@article{POLYVYANYY2019345,
title = {A systematic approach for discovering causal dependencies between observations and incidents in the health and safety domain},
journal = {Safety Science},
volume = {118},
pages = {345-354},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316230},
author = {Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede}},
keywords = {Big data, Data mining, Process mining, Proximity of events, Causality, Health and safety, Cause of incidents},
abstract = {The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.}
}
@article{SOUIFI2022103666,
title = {Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions},
journal = {Computers in Industry},
volume = {140},
pages = {103666},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103666},
url = {https://www.sciencedirect.com/science/article/pii/S016636152200063X},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling},
abstract = {For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.}
}
@article{MORAN201942,
title = {Curious Feature Selection},
journal = {Information Sciences},
volume = {485},
pages = {42-54},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519301100},
author = {Michal Moran and Goren Gordon},
keywords = {Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Big data, Data science, Feature selection},
abstract = {In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.}
}
@article{KARIM2016214,
title = {Maintenance Analytics – The New Know in Maintenance},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {28},
pages = {214-219},
year = {2016},
note = {3rd IFAC Workshop on Advanced Maintenance Engineering, Services and Technology AMEST 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316324612},
author = {Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar},
keywords = {big data, maintenance analytics, eMaintenance, Knowledge discovery, maintenance decision support},
abstract = {Abstract:
Decision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) “Maintenance Descriptive Analytics (monitoring)”; 2) “Maintenance Diagnostic Analytics”; 3) “Maintenance Predictive Analytics”; and 4) “Maintenance Prescriptive analytics”.}
}
@incollection{BERANGER201697,
title = {2 - Ethical Development of the Medical Datasphere},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {97-166},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500026},
author = {Jérôme Béranger},
keywords = {Algorithmic ethics, Architecture, Complex data, Ethical data mining, Ethical issues, Ethical-technical guidance, Evaluation, Medical datasphere, Neo-Platonic modeling},
abstract = {Abstract:
As Lucy Suchmann observed in 2011, through Lévi-Strauss, “…we are our tools…” and our personal health data are an integral part of us. In these circumstances, it becomes necessary to question the value of Big Data in the health sphere.}
}
@article{GE20201883,
title = {Developing the Quality Model for Collaborative Open Data},
journal = {Procedia Computer Science},
volume = {176},
pages = {1883-1892},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.228},
url = {https://www.sciencedirect.com/science/article/pii/S187705092032130X},
author = {Mouzhi Ge and Włodzimierz Lewoniewski},
keywords = {Data Quality, Quality Assessment, Collaborative Open Data, Wikipedia, Quality Model},
abstract = {Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.}
}
@article{XU2021100860,
title = {Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers},
journal = {Tourism Management Perspectives},
volume = {39},
pages = {100860},
year = {2021},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2021.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2211973621000738},
author = {Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang},
keywords = {Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers},
abstract = {Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.}
}
@article{RAJ2021103107,
title = {A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0},
journal = {Journal of Network and Computer Applications},
volume = {187},
pages = {103107},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103107},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001284},
author = {Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato},
abstract = {There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.}
}
@article{AUFAURE2016100,
title = {From Business Intelligence to semantic data stream management},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {100-107},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003635},
author = {Marie-Aude Aufaure and Raja Chiky and Olivier Curé and Houda Khrouf and Gabriel Kepeklian},
keywords = {Data stream, Linked Data, Business Intelligence, Stream reasoning},
abstract = {The Semantic Web technologies are being increasingly used for exploiting relations between data. In addition, new tendencies of real-time systems, such as social networks, sensors, cameras or weather information, are continuously generating data. This implies that data and links between them are becoming extremely vast. Such huge quantity of data needs to be analyzed, processed, as well as stored if necessary. In this position paper, we will introduce recent work on Real-Time Business Intelligence combined with semantic data stream management. We will present underlying approaches such as continuous queries, data summarization and matching, and stream reasoning.}
}
@article{LU201968,
title = {Oil and Gas 4.0 era: A systematic review and outlook},
journal = {Computers in Industry},
volume = {111},
pages = {68-90},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519302064},
author = {Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang},
keywords = {Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization},
abstract = {Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.}
}
@article{SHAO2022102736,
title = {IoT data visualization for business intelligence in corporate finance},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102736},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102736},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002181},
author = {Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam},
keywords = {IoT, Data visualization, Business intelligence, Corporate finance},
abstract = {Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.}
}
@article{WANG202151,
title = {The national multi-center artificial intelligent myopia prevention and control project},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {51-55},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000085},
author = {Xun Wang and Yahan Yang and Yuxuan Wu and Wenbin Wei and Li Dong and Yang Li and Xingping Tan and Hankun Cao and Hong Zhang and Xiaodan Ma and Qin Jiang and Yunfan Zhou and Weihua Yang and Chaoyu Li and Yu Gu and Lin Ding and Yanli Qin and Qi Chen and Lili Li and Mingyue Lian and Jin Ma and Dongmei Cui and Yuanzhou Huang and Wenyan Liu and Xiao Yang and Shuiming Yu and Jingjing Chen and Dongni Wang and Zhenzhe Lin and Pisong Yan and Haotian Lin},
keywords = {Myopia prevention and control, Artificial intelligent, National multicenter project},
abstract = {In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.}
}
@article{KAYABAY2022121264,
title = {Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121264},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121264},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006983},
author = {Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit},
keywords = {Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data},
abstract = {Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.}
}
@incollection{WANG2018247,
title = {Chapter 8 - Real-Time Monitoring and Early Warning of a Train’s Running State and Operation Behavior},
editor = {Junfeng Wang},
booktitle = {Safety Theory and Control Technology of High-Speed Train Operation},
publisher = {Academic Press},
pages = {247-266},
year = {2018},
isbn = {978-0-12-813304-0},
doi = {https://doi.org/10.1016/B978-0-12-813304-0.00008-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128133040000086},
author = {Junfeng Wang},
keywords = {High-speed railway, big data, train running status, monitoring, early warning},
abstract = {In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.}
}
@article{LAU2019357,
title = {A survey of data fusion in smart city applications},
journal = {Information Fusion},
volume = {52},
pages = {357-374},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519300326},
author = {Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan},
keywords = {Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification},
abstract = {The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.}
}
@article{SMIDT20211018,
title = {The challenge of privacy and security when using technology to track people in times of COVID-19 pandemic},
journal = {Procedia Computer Science},
volume = {181},
pages = {1018-1026},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003306},
author = {Hermanus J Smidt and Osden Jokonya},
keywords = {COVID 19, tracking, society, technology, privacy},
abstract = {Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.}
}
@article{ESCOBAR2020103378,
title = {Adding value to Linked Open Data using a multidimensional model approach based on the RDF Data Cube vocabulary},
journal = {Computer Standards & Interfaces},
volume = {68},
pages = {103378},
year = {2020},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.103378},
url = {https://www.sciencedirect.com/science/article/pii/S0920548919300480},
author = {Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jesús Peral},
keywords = {Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF Data Cube vocabulary, Semantic web, Big data},
abstract = {Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.}
}
@article{LOPEZROBLES201922,
title = {30 years of intelligence models in management and business: A bibliometric review},
journal = {International Journal of Information Management},
volume = {48},
pages = {22-38},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121730244X},
author = {J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo},
keywords = {Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis},
abstract = {The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.}
}
@article{ROJO2019160,
title = {Near-ground effect of height on pollen exposure},
journal = {Environmental Research},
volume = {174},
pages = {160-169},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119302439},
author = {Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters},
keywords = {Height, Pollen, Aerobiology, Monitoring network, Big data},
abstract = {The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.}
}
@article{LARSON2016700,
title = {A review and future direction of agile, business intelligence, analytics and data science},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {700-710},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121630233X},
author = {Deanne Larson and Victor Chang},
keywords = {Agile methodologies, Business intelligence (BI), Analytics and big data, Lifecycle for BI and Big Data},
abstract = {Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.}
}
@incollection{HOVENGA2022209,
title = {Chapter 9 - Quality data, design, implementation, and governance},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {209-237},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00013-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000136},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Data quality, Data design, System implementation, Information governance, Leadership, Data supply chain},
abstract = {Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.}
}
@article{FAIEQ2017151,
title = {C2IoT: A framework for Cloud-based Context-aware Internet of Things services for smart cities},
journal = {Procedia Computer Science},
volume = {110},
pages = {151-158},
year = {2017},
note = {14th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2017) / 12th International Conference on Future Networks and Communications (FNC 2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.06.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917312486},
author = {Soufiane Faieq and Rajaa Saidi and Hamid Elghazi and Moulay Driss Rahmani},
keywords = {Internet of Things, Cloud Computing, Context-Awareness, Big Data, Service Composition, Smart City},
abstract = {The smart city vision was born by the integration of ICT in the day to day city management operations and citizens lives, owing to the need for novel and smart ways to manage the cities resources; making them more efficient, sustainable and transparent. However, the understanding of the crucial elements to this integration and how they can benefit from each other proves difficult and unclear. In this article, we investigate the intricate synergies between different technologies and paradigms involved in the smart city vision, to help design a robust framework, capable of handling the challenges impeding its successful implementation. To this end, we propose a context-aware centered approach to present a holistic view of a smart city as viewed from the different angles (Cloud, IoT, Big Data). We also propose a framework encompassing elements from the different enablers, leveraging their strengths to build and develop smart-x applications and services.}
}
@incollection{SVAB2021274,
title = {Complexity of Patient Data in Primary Care Practice},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {274-282},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11590-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383115909},
author = {Igor Švab},
keywords = {Big data, Digital health, Family medicine, Genomics, Primary care},
abstract = {The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.}
}
@article{VIDGEN2017626,
title = {Management challenges in creating value from business analytics},
journal = {European Journal of Operational Research},
volume = {261},
number = {2},
pages = {626-639},
year = {2017},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717301455},
author = {Richard Vidgen and Sarah Shaw and David B. Grant},
keywords = {Analytics, Delphi, Management challenges, Value creation, Ecosystem},
abstract = {The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.}
}
@incollection{KRISHNAN2020157,
title = {9 - Governance},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {157-174},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000090},
author = {Krish Krishnan},
keywords = {Big data application, Data management, Data-driven architecture, Governance, Machine learning, Master data, Metadata},
abstract = {Building the big data application is very interesting and can provide multiple users with multiple perspectives, data discovery to end state analytics and beyond is very much what everybody wants to achieve. Enterprises are ready to spend millions of dollars to get a share of your wallet, they want to be a part of your life and be present at every event that gets your attention. They want to leverage their partnerships and influence you, how do they make this all happen? The most successful companies will tell you their story is built on governance. The aspect of governance is very critical to the success of this journey whether internal or external. What governance are we talking about? How do we implement the same? This chapter will focus on those aspects.}
}
@article{VIAL2019113133,
title = {Reflections on quality requirements for digital trace data in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113133},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113133},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301629},
author = {Gregory Vial},
keywords = {Digital trace data, Data quality, GitHub},
abstract = {In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.}
}
@article{CORRALESGARAY201977,
title = {Knowledge areas, themes and future research on open data: A co-word analysis},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {77-87},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18303216},
author = {Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín},
keywords = {Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends},
abstract = {This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.}
}
@article{HARRISON2020102672,
title = {New and emerging data forms in transportation planning and policy: Opportunities and challenges for “Track and Trace” data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {117},
pages = {102672},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102672},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20305878},
author = {Gillian Harrison and Susan M. Grant-Muller and Frances C. Hodgson},
keywords = {Transport policy, Track and Trace, Mobile phone data, Mobility profile, Big Data},
abstract = {High quality, reliable data and robust models are central to the development and appraisal of transportation planning and policy. Although conventional data may offer good ‘content’, it is widely observed that it lacks context i.e. who and why people are travelling. Transportation modelling has developed within these boundaries, with implications for the planning, design and management of transportation systems and policy-making. This paper establishes the potential of passively collected GPS-based “Track & Trace” (T&T) datasets of individual mobility profiles towards enhancing transportation modelling and policy-making. T&T is a type of New and Emerging Data Form (NEDF), lying within the broader ‘Big Data’ paradigm, and is typically collected using mobile phone sensors and related technologies. These capture highly grained mobility content and can be linked to the phone owner/user behavioural choices and other individual context. Our meta-analysis of existing literature related to spatio-temporal mobile phone data demonstrates that NEDF’s, and in particular T&T data, have had little mention to date within an applied transportation planning and policy context. We thus establish there is an opportunity for policy-makers, transportation modellers, researchers and a wide range of stakeholders to collaborate in developing new analytic approaches, revise existing models and build the skills and related capacity needed to lever greatest value from the data, as well as to adopt new business models that could revolutionise citizen participation in policy-making. This is of particular importance due to the growing awareness in many countries for a need to develop and monitor efficient cross-sectoral policies to deliver sustainable communities.}
}
@incollection{BAI202261,
title = {Chapter Three - Data-driven approaches: Use of digitized operational data in process safety},
editor = {Faisal Khan and Hans Pasman and Ming Yang},
series = {Methods in Chemical Process Safety},
publisher = {Elsevier},
volume = {6},
pages = {61-99},
year = {2022},
booktitle = {Methods to Assess and Manage Process Safety in Digitalized Process System},
issn = {2468-6514},
doi = {https://doi.org/10.1016/bs.mcps.2022.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468651422000022},
author = {Yiming Bai and Shuaiyu Xiang and Zeheng Zhao and Borui Yang and Jinsong Zhao},
keywords = {Data-driven models, Process safety, Data preparation, Data cleaning, Statistical models, Artificial intelligence models, Process monitoring, Fault detection and diagnosis, Fault prognosis, Video monitoring},
abstract = {Process safety is playing an important role with the rapid development of industry. With the advent of the Big Data era, various and massive data from the Internet of Things can be used for process safety. In this chapter, we aim to provide the reader with a comprehensive understanding of rapidly growing data-driven process safety approaches in the chemical industry. Data-driven approaches primarily use past process data without a complex mechanism model of chemical properties or processes; hence, they have advantages in practical industrial applications. In this chapter, first, we describe the importance of data in process safety. Then, we briefly introduce the ideas and methods of data pre-processing. We follow this with a discussion on statistical-based and artificial intelligence-based data-driven approaches. Then, we elaborate on the application of data-driven methods in the field of chemical process safety. Finally, we provide a summary and outlook for advancing data-driven methods.}
}
@article{ZHANG2020104512,
title = {Blockchain-based life cycle assessment: An implementation framework and system architecture},
journal = {Resources, Conservation and Recycling},
volume = {152},
pages = {104512},
year = {2020},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0921344919304185},
author = {Abraham Zhang and Ray Y Zhong and Muhammad Farooque and Kai Kang and V G Venkatesh},
keywords = {Blockchain, Life cycle assessment, Supply chain sustainability, Environmental sustainability, Operational excellence},
abstract = {Life cycle assessment (LCA) is widely used for assessing the environmental impacts of a product or service. Collecting reliable data is a major challenge in LCA due to the complexities involved in the tracking and quantifying inputs and outputs at multiple supply chain stages. Blockchain technology offers an ideal solution to overcome the challenge in sustainable supply chain management. Its use in combination with internet-of-things (IoT) and big data analytics and visualization can help organizations achieve operational excellence in conducting LCA for improving supply chain sustainability. This research develops a framework to guide the implementation of Blockchain-based LCA. It proposes a system architecture that integrates the use of Blockchain, IoT, and big data analytics and visualization. The proposed implementation framework and system architecture were validated by practitioners who were experienced with Blockchain applications. The research also analyzes system implementation costs and discusses potential issues and solutions, as well as managerial and policy implications.}
}
@article{BUTTERWORTH2018257,
title = {The ICO and artificial intelligence: The role of fairness in the GDPR framework},
journal = {Computer Law & Security Review},
volume = {34},
number = {2},
pages = {257-268},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S026736491830044X},
author = {Michael Butterworth},
keywords = {Artificial intelligence (AI), Big data analytics, General Data Protection Regulation (GDPR), Fairness, Regulations, Collective rights, Data ethics},
abstract = {The year 2017 has seen many EU and UK legislative initiatives and proposals to consider and address the impact of artificial intelligence on society, covering questions of liability, legal personality and other ethical and legal issues, including in the context of data processing. In March 2017, the Information Commissioner's Office (UK) updated its big data guidance to address the development of artificial intelligence and machine learning, and to provide (GDPR), which will apply from 25 May 2018. This paper situates the ICO's guidance in the context of wider legal and ethical considerations and provides a critique of the position adopted by the ICO. On the ICO's analysis, the key challenge for artificial intelligence processing personal data is in establishing that such processing is fair. This shift reflects the potential for artificial intelligence to have negative social consequences (whether intended or unintended) that are not otherwise addressed by the GDPR. The question of ‘fairness’ is an important one, to address the imbalance between big data organisations and individual data subjects, with a number of ethical and social impacts that need to be evaluated.}
}
@article{QUEST2022100167,
title = {A 3D indicator for guiding AI applications in the energy sector},
journal = {Energy and AI},
volume = {9},
pages = {100167},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2022.100167},
url = {https://www.sciencedirect.com/science/article/pii/S2666546822000234},
author = {Hugo Quest and Marine Cauz and Fabian Heymann and Christian Rod and Lionel Perret and Christophe Ballif and Alessandro Virtuani and Nicolas Wyrsch},
keywords = {Artificial intelligence, Digitalisation, AI application, Big data, AI policy, Energy sector},
abstract = {The utilisation of Artificial Intelligence (AI) applications in the energy sector is gaining momentum, with increasingly intensive search for suitable, high-quality and trustworthy solutions that displayed promising results in research. The growing interest comes from decision makers of both the industry and policy domains, searching for applications to increase companies’ profitability, raise efficiency and facilitate the energy transition. This paper aims to provide a novel three-dimensional (3D) indicator for AI applications in the energy sector, based on their respective maturity level, regulatory risks and potential benefits. Case studies are used to exemplify the application of the 3D indicator, showcasing how the developed framework can be used to filter promising AI applications eligible for governmental funding or business development. In addition, the 3D indicator is used to rank AI applications considering different stakeholder preferences (risk-avoidance, profit-seeking, balanced). These results allow AI applications to be better categorised in the face of rapidly emerging national and intergovernmental AI strategies and regulations that constrain the use of AI applications in critical infrastructures.}
}
@article{ESCOBAR2021748,
title = {Quality 4.0 — Green, Black and Master Black Belt Curricula},
journal = {Procedia Manufacturing},
volume = {53},
pages = {748-759},
year = {2021},
note = {49th SME North American Manufacturing Research Conference (NAMRC 49, 2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.06.085},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001086},
author = {Carlos A. Escobar and Debejyo Chakraborty and Megan McGovern and Daniela Macias and Ruben Morales-Menendez},
keywords = {Quality 4.0, Certification, Smart manufacturing, Artificial intelligence, Big data},
abstract = {Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.}
}
@article{CHEN20151331,
title = {Towards Integrated Study of Data Management and Data Mining},
journal = {Procedia Computer Science},
volume = {55},
pages = {1331-1339},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015926},
author = {Zhengxin Chen},
keywords = {Big Data, granular computing, granularity, databases, rough set theory, database keyword search (DBKWS)},
abstract = {From very beginning, research and practice of database management systems (DBMSs) have been cantered on handling granulation and granularities at various levels, thus sharing common interests with granular computing (GrC). Although DBMS and GrC have different focuses, the advent of Big Data has brought these two research areas closer to each other, because Big Data requires integrated study of data storage and analysis. In this paper, we explore this issue. Starting with an examination of granularities from a database perspective, we discuss new challenges of Big Data. We then turn to data management issues related to GrC. As an example of possible cross-fertilization of these two fields, we examine the recent development of database keyword search (DBKWS). Even research in DBKWS is largely independent to GrC, DBKWS has to handle various issues related to granularity handling. In particular, aggregation of DBKWS results is closely related to studies in granularities and granulation, which echoes L. Zadeh's famous formula: Granulation = Summarization. We present our proposed approach, termed as extended keyword search, which illustrates that an integrated study of data management and data mining/analysis is not restricted to GrC or rough set theory}
}
@article{MONTANS2019845,
title = {Data-driven modeling and learning in science and engineering},
journal = {Comptes Rendus Mécanique},
volume = {347},
number = {11},
pages = {845-855},
year = {2019},
note = {Data-Based Engineering Science and Technology},
issn = {1631-0721},
doi = {https://doi.org/10.1016/j.crme.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1631072119301809},
author = {Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz},
keywords = {Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data},
abstract = {In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.}
}
@article{RUMSON2019598,
title = {Innovations in the use of data facilitating insurance as a resilience mechanism for coastal flood risk},
journal = {Science of The Total Environment},
volume = {661},
pages = {598-612},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.01.114},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719301317},
author = {Alexander G. Rumson and Stephen H. Hallett},
keywords = {Risk analytics, Adaptation, Remote sensing, Big Data},
abstract = {Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.}
}
@article{SINGH20191147,
title = {A Systemic Cybercrime Stakeholders Architectural Model},
journal = {Procedia Computer Science},
volume = {161},
pages = {1147-1155},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.227},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319386},
author = {Manmeet Mahinderjit Singh and Anizah Abu Bakar},
keywords = {Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory, Systemic},
abstract = {The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.}
}
@article{ANJUM2018326,
title = {Privacy preserving data by conceptualizing smart cities using MIDR-Angelization},
journal = {Sustainable Cities and Society},
volume = {40},
pages = {326-334},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717314646},
author = {Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq},
keywords = {Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy, Re-identification risk, Smart city},
abstract = {Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.}
}
@article{WU2020116388,
title = {Impact factors of the real-world fuel consumption rate of light duty vehicles in China},
journal = {Energy},
volume = {190},
pages = {116388},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116388},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219320833},
author = {Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang},
keywords = {Real-world fuel consumption rate, Energy consumption, Private passenger vehicles, Big data, China},
abstract = {Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n = 106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40 L/100 km (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.}
}
@incollection{ZOHURI202259,
title = {Chapter 3 - Data warehousing, data mining, data modeling, and data analytics},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {59-86},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000015},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Big data, Data analytics and Data predective, Data modeling, Data warehousing},
abstract = {Data warehouses are constantly evolving to support new technologies and business requirements—and remain relevant when it comes to big data and analytics. Regardless of how new or sophisticated your data warehouse is, it likely needs modernization. Data warehousing, along with data modeling, and side by side with data analytic capability gives us the upper hand with our knowledge by collecting the right information at the right time with the right data coming from all directions, whether or not these data are structured or unstructured. We should be able to have proper tools in hand to be able to take this information and knowledge to be in a position of resilience based on predictive analysis driven by data. This chapter will discuss data warehousing, data modeling, and consequently, data analytics where, in combination, they all are variables functioning within the process of predictive analytic modeling. This process allows us to have the knowledge we are looking for. Getting reliable information from data warehouses is resource-intensive; missing one step can result in wasted processing time and/or bad data.}
}
@incollection{KHAN2022,
title = {Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000590},
author = {Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam},
keywords = {NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification},
abstract = {With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.}
}
@incollection{GROOT2017127,
title = {Chapter 5 - Data Management Tools and Techniques},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {127-177},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000053},
author = {Martijn Groot},
keywords = {data management technology, databases, big data, financial analytics, IT management},
abstract = {In this chapter we look at the technology and tooling available for data management. We start with a discussion on the different components of an IT infrastructure and how to describe them. We provide a short taxonomy of tools from analytics and data distribution to data governance and end-user tools. This is followed by a discussion on data models and storage models, from traditional relational databases to different challenger technologies including NoSQL databases. We discuss data quality management and curation processes and data storage at different scales from data marts and warehouses to data lakes. We then discuss data analytics and big data technologies and what some of the use cases and implications for financial services firms are. After discussing privacy and security aspects, and the promising application areas of blockchain technology in master data, we discuss cloud storage models and what the cloud trend means for banks and asset managers. We end with a discussion on IT sourcing options, IT management, and IT maturity models before concluding with a look ahead.}
}
@article{OIEN20211334,
title = {An approach to data structuring and predictive analysis in discrete manufacturing},
journal = {Procedia CIRP},
volume = {104},
pages = {1334-1338},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.224},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011227},
author = {Christian Dalheim Øien and Sebastian Dransfeld},
keywords = {Anomaly Detection, Predictive Maintenance, Discrete Manufacturing, Big Data Analytics, Adaptive Self-learning Systems},
abstract = {In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component’s lifetime.}
}
@article{LUO2021197,
title = {Towards high quality mobile crowdsensing: Incentive mechanism design based on fine-grained ability reputation},
journal = {Computer Communications},
volume = {180},
pages = {197-209},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003637},
author = {Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo},
keywords = {Mobile crowdsensing, Incentive mechanism, Quality-aware, Reputation system},
abstract = {Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers’ fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.}
}
@article{GUO2021107627,
title = {Automated pressure transient analysis: A cloud-based approach},
journal = {Journal of Petroleum Science and Engineering},
volume = {196},
pages = {107627},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107627},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520306951},
author = {Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed},
keywords = {Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application},
abstract = {Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.}
}
@article{LEE2020101426,
title = {Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science},
journal = {The Leadership Quarterly},
pages = {101426},
year = {2020},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2020.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1048984320300539},
author = {Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene},
keywords = {Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects},
abstract = {Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.}
}
@article{WANG2018440,
title = {Research on the Theory and Method of Grid Data Asset Management},
journal = {Procedia Computer Science},
volume = {139},
pages = {440-447},
year = {2018},
note = {6th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.258},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319288},
author = {Jun Wang and Yun-si Li and Wei Song and Ai-hua Li},
keywords = {big data, grid data asset, asset management, data governance},
abstract = {In the era of Big Data, data assets have become a strategic resource which cannot be overlooked by both society and enterprises. However, data is not equal to the assets. This paper first introduces the necessary conditions of data assetization and discriminates the concepts of data governance, data management and data asset management. Then it focuses on the unique connotation and characteristics of grid data assets. With reference to the mainstream theory of data management, the framework for the grid data asset management is set up in the combination of the characteristics of data assets, business needs and the actual situation in the power supply enterprises. Finally, this paper puts forward higher system requirements and technical requirements for China’s power supply enterprises to conduct data asset management.}
}
@article{ZHANG202124,
title = {Thinking on the informatization development of China's healthcare system in the post-COVID-19 era},
journal = {Intelligent Medicine},
volume = {1},
number = {1},
pages = {24-28},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000097},
author = {Ming Zhang and Danyun Dai and Siliang Hou and Wei Liu and Feng Gao and Dong Xu and Yu Hu},
keywords = {Coronavirus disease 2019, Healthcare system, Informatization},
abstract = {With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the Coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.}
}
@article{CONDE2022101723,
title = {Applying digital twins for the management of information in turnaround event operations in commercial airports},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101723},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101723},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001811},
author = {Javier Conde and Andres Munoz-Arcentales and Mario Romero and Javier Rojo and Joaquín Salvachúa and Gabriel Huecas and Álvaro Alonso},
keywords = {Aviation, Flight turnaround events, Digital twin, Internet of Things, Data modelling, Big data},
abstract = {The aerospace sector is one of the many sectors in which large amounts of data are generated. Thanks to the evolution of technology, these data can be exploited in several ways to improve the operation and management of industrial processes. However, to achieve this goal, it is necessary to define architectures and data models that allow to manage and homogenise the heterogeneous data collected. In this paper, we present an Airport Digital Twin Reference Conceptualisation’s and data model based on FIWARE Generic Enablers and the Next Generation Service Interfaces-Linked Data standard. Concretely, we particularise the Airport Digital Twin to improve the efficiency of flight turnaround events. The architecture proposed is validated in the Aberdeen International Airport with the aim of reducing delays in commercial flights. The implementation includes an application that shows the real state of the airport, combining two-dimensional and three-dimensional virtual reality representations of the stands, and a mobile application that helps ground operators to schedule departure and arrival flights.}
}
@article{SKARPATHIOTAKI2022100274,
title = {Cross-Industry Process Standardization for Text Analytics},
journal = {Big Data Research},
volume = {27},
pages = {100274},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100274},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000915},
author = {Christina G. Skarpathiotaki and Konstantinos E. Psannis},
keywords = {Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes},
abstract = {We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.}
}
@article{ARIMURA2020100212,
title = {Changes in urban mobility in Sapporo city, Japan due to the Covid-19 emergency declarations},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {7},
pages = {100212},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220301238},
author = {Mikiharu Arimura and Tran Vinh Ha and Kota Okumura and Takumi Asada},
keywords = {Covid-19, Moving pattern, Mobile spatial statistics, Population concentration, Big data},
abstract = {At the time of writing, the world is facing the new coronavirus pandemic, which has been declared one of the most dangerous disasters of the 21st century. All nations and communities have applied many countermeasures to control the spread of the epidemic. In terms of countermeasures, lockdowns and reductions of social activities are meant to flatten the curve of infection. Nevertheless, to date, there has been no evaluation of the effectiveness of these methods. Thus, the present study aims to interpret the change in the population density of Sapporo city in the emergency's period declaration using big data obtained from mobile spatial statistics. The results indicate that, in the time of refraining from traveling, the city's residents have been more likely to stay home and less likely to travel to the center area. This has led to a decrease of up to 90% of the population density in crowded areas. The study's outcomes partly explain the statement of reducing 70%–80% of contact between people in line with the purpose of the emergency declaration. Moreover, these findings establish the primary step for further analysis of estimating the efficiency of policy in controlling the epidemic.}
}
@article{NG2017939,
title = {A Master Data Management Solution to Unlock the Value of Big Infrastructure Data for Smart, Sustainable and Resilient City Planning},
journal = {Procedia Engineering},
volume = {196},
pages = {939-947},
year = {2017},
note = {Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817331569},
author = {S. Thomas Ng and Frank J. Xu and Yifan Yang and Mengxue Lu},
keywords = {Big data, integrated infrastructure asset management, master data management, smart city, smart infrastructure},
abstract = {In recent years, many governments have launched various smart city or smart infrastructure initiatives to improve the quality of citizens’ life and help city managers / planners optimize the operation and management of urban infrastructures. By deploying internet of things (IoT) to infrastructure systems, high-volume and high-variety of data pertinent to the condition and performance of infrastructure systems along with the behaviors of citizens can be gathered, processed, integrated and analyzed through cloud-based infrastructure asset management systems, ubiquitous mobile applications and big data analytics platforms. Nonetheless, how to fully exploit the value of ‘big infrastructure data’ is still a key challenge facing most stakeholders. Unless data is shared by different infrastructure systems in an interoperable and consistent manner, it is difficult to realize the smart infrastructure concept for efficient smart city planning, not to mention about developing appropriate resilience and sustainable programs. To unlock the value of big infrastructure data for smart, sustainable and resilient city planning, a master data management (MDM) solution is proposed in this paper. MDM has been adopted in the business sector to orchestrate operational and analytical big data applications. In order to derive a suitable MDM solution for smart, sustainable and resilient city planning, commercial and open source MDM systems, smart city standards, smart city concept models, smart community infrastructure frameworks, semantic web technologies will be critically reviewed, and feedback and requirements will be gathered from experts who are responsible for developing smart, sustainable and resilient city programs. A case study which focuses on the building and transportation infrastructures of a selected community in Hong Kong will be conducted to pilot the proposed MDM solution.}
}
@article{FU2022128312,
title = {Extracting historical flood locations from news media data by the named entity recognition (NER) model to assess urban flood susceptibility},
journal = {Journal of Hydrology},
volume = {612},
pages = {128312},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.128312},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422008848},
author = {Shengnan Fu and Heng Lyu and Ze Wang and Xin Hao and Chi Zhang},
keywords = {Urban flood susceptibility, News media data, Flood locations, Named entity recognition, Data quality control},
abstract = {Flood susceptibility assessment for identifying flood-prone areas plays a significant role in flood hazard mitigation. Machine learning is an optional assessment method because of its high objectivity and computational efficiency, but how to get enough and accurate information of historical flood locations to train the machine learning models has been a key problem. In recent years, news media data from both news websites and social media accounts has emerged as a promising source for natural science studies. However, the application of news media data in urban flood susceptibility assessment is still inadequate. This study proposed an approach to fill this gap. Firstly, flood locations were extracted from news media data based on a named entity recognition (NER) model. Then, a frequency or distance-based data quality control method was employed to improve the representativeness of the extracted flooded locations. Finally, flood conditioning factors with information of historical flood locations were input into a Support Vector Machine (SVM) model for flood susceptibility assessment. We took the central city of Dalian, China as a case study. The T-test results show that there was no significant difference between the distributions of most flood conditioning factors at the flood locations from the news media data and the official planning report. In the obtained flood susceptibility map, the high flood susceptibility areas got a recall of 90% compared with the high flood hazard areas in the planning report. Performing data quality control in the frequency-based method can improve the precision of the flood susceptibility map by up to 5%, while the distance-based method is ineffective. This study provides an example and offers the value of applying new data sources and modern deep learning techniques for urban flood management.}
}
@incollection{FORTSON2021185,
title = {Chapter 10 - From Green Peas to STEVE: Citizen Science Engagement in Space Science},
editor = {Amy Paige Kaminski},
booktitle = {Space Science and Public Engagement},
publisher = {Elsevier},
pages = {185-219},
year = {2021},
isbn = {978-0-12-817390-9},
doi = {https://doi.org/10.1016/B978-0-12-817390-9.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173909000099},
author = {Lucy Fortson},
keywords = {Citizen Science, Crowdsourcing, Machine learning, Volunteer engagement},
abstract = {The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author’s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.}
}
@article{CHRISTOPOULOS201570,
title = {Extraction of the global absolute temperature for Northern Hemisphere using a set of 6190 meteorological stations from 1800 to 2013},
journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
volume = {128},
pages = {70-83},
year = {2015},
issn = {1364-6826},
doi = {https://doi.org/10.1016/j.jastp.2015.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364682615000577},
author = {Demetris T. Christopoulos},
keywords = {Absolute temperature, Northern Hemisphere, Valid station, Data quality, Seasonal bias, Extreme values distribution, Missing records, Big data analysis},
abstract = {Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.}
}
@article{LIN2018293,
title = {DTRM: A new reputation mechanism to enhance data trustworthiness for high-performance cloud computing},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {293-302},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17317247},
author = {Hui Lin and Jia Hu and Chuanfeng Xu and Jianfeng Ma and Mengyang Yu},
keywords = {Cloud computing, Reputation mechanism, Trustworthiness, Data veracity},
abstract = {Cloud computing and the mobile Internet have been the two most influential information technology revolutions, which intersect in mobile cloud computing (MCC). The burgeoning MCC enables the large-scale collection and processing of big data, which demand trusted, authentic, and accurate data to ensure an important but often overlooked aspect of big data — data veracity. Troublesome internal attacks launched by internal malicious users is one key problem that reduces data veracity and remains difficult to handle. To enhance data veracity and thus improve the performance of big data computing in MCC, this paper proposes a Data Trustworthiness enhanced Reputation Mechanism (DTRM) which can be used to defend against internal attacks. In the DTRM, the sensitivity-level based data category, Metagraph theory based user group division, and reputation transferring methods are integrated into the reputation query and evaluation process. The extensive simulation results based on real datasets show that the DTRM outperforms existing classic reputation mechanisms under bad mouthing attacks and mobile attacks.}
}
@article{PERERA2016512,
title = {Marine Engine Operating Regions under Principal Component Analysis to evaluate Ship Performance and Navigation Behavior},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {23},
pages = {512-517},
year = {2016},
note = {10th IFAC Conference on Control Applications in Marine SystemsCAMS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.487},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316320778},
author = {Lokukaluge P. Perera and Brage Mo},
keywords = {Principal Component Analysis, Big Data, Marine Engine Operations, Ship Performance Monitoring, Structured Data},
abstract = {Abstract:
Marine engine operating regions under principal component analysis (PCA) to evaluate ship performance and navigation behavior are presented in this study. A data set with ship performance and navigation information (i.e. a selected vessel) is considered to identify its hidden structure with respect to a selected operating region of the marine engine. Firstly, the data set is classified with respect to the engine operating points (i.e. operating modes), identifying three operating regions for the main engine. Secondly, one engine operating region (i.e. a data cluster) is analyzed to calculate the respective principal components (PCs). These PCs represent various relationships among ship performance and navigation parameters of the vessel and those relationships with respect to the marine engine operating region are used to evaluate ship performance and navigation behavior. Furthermore, such knowledge (i.e. PCs and parameter behavior) can also be used for sensor fault identification and data compression/expansion types of applications as a big data solution in shipping.}
}
@article{KAMAL2016191,
title = {A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset},
journal = {Computer Methods and Programs in Biomedicine},
volume = {131},
pages = {191-206},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715304119},
author = {Sarwar Kamal and Shamim Hasnat Ripon and Nilanjan Dey and Amira S. Ashour and V. Santhi},
keywords = {MapReduce, K-nearest neighbor, Big data, DNA (deoxyribonucleic acid), Computational biology, Imbalance data},
abstract = {Background
In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
Method
In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
Results
To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
Conclusions
The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.}
}
@article{GONZALEZVIDAL2022328,
title = {Intrinsic and extrinsic quality of data for open data repositories},
journal = {ICT Express},
volume = {8},
number = {3},
pages = {328-333},
year = {2022},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S240595952200090X},
author = {Aurora González-Vidal and Alfonso P. Ramallo-González and Antonio F. Skarmeta},
keywords = {Data quality, Open data, IoT, Machine learning},
abstract = {This work assesses the quality of Internet of Things data not only as an intrinsic quality on how well it represents the related phenomenon but also, on how much information it contains to educate an artificial entity. The quality metrics here proposed are tested with real datasets. Also, they are implemented on OpenCPU, so the open data repositories can use them off-the-shelf to rate their datasets without computational cost and minimum human intervention, making them more attractive to potential users and gaining visibility and impact.}
}
@article{YEBENES2019614,
title = {Towards a Data Governance Framework for Third Generation Platforms},
journal = {Procedia Computer Science},
volume = {151},
pages = {614-621},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.082},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305447},
author = {Juan Yebenes and Marta Zorrilla},
keywords = {Data governance, Industry 4.0, data bus architecture, cloud computing, IoT, big data},
abstract = {The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.}
}
@article{CHOW2017455,
title = {Internet-based computer technology on radiotherapy},
journal = {Reports of Practical Oncology & Radiotherapy},
volume = {22},
number = {6},
pages = {455-462},
year = {2017},
issn = {1507-1367},
doi = {https://doi.org/10.1016/j.rpor.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1507136716301602},
author = {James C.L. Chow},
keywords = {Radiotherapy, Computer technology, Cloud computing, Machine learning, Big data},
abstract = {Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.}
}
@article{TSAI2021105421,
title = {Sustainable supply chain management trends in world regions: A data-driven analysis},
journal = {Resources, Conservation and Recycling},
volume = {167},
pages = {105421},
year = {2021},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2021.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0921344921000288},
author = {Feng Ming Tsai and Tat-Dat Bui and Ming-Lang Tseng and Mohd Helmi Ali and Ming K. Lim and Anthony SF Chiu},
keywords = {Sustainable supply chain management, Data-driven analysis, Fuzzy Delphi method, Entropy weight method, Fuzzy decision-making trial and evaluation laboratory},
abstract = {This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.}
}
@article{GRANELL20161,
title = {Future Internet technologies for environmental applications},
journal = {Environmental Modelling & Software},
volume = {78},
pages = {1-15},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215301298},
author = {Carlos Granell and Denis Havlik and Sven Schade and Zoheir Sabeur and Conor Delaney and Jasmin Pielorz and Thomas Usländer and Paolo Mazzetti and Katharina Schleidt and Mike Kobernus and Fuada Havlik and Nils Rune Bodsberg and Arne Berre and Jose Lorenzo Mon},
keywords = {Environmental informatics, Environmental observation web, Future internet, Cloud computing, Internet of things, Big data, Environmental specific enablers, Volunteered geographic information, Crowdtasking},
abstract = {This paper investigates the usability of Future Internet technologies (aka “Generic Enablers of the Future Internet”) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the “Environmental Observation Web” as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, “the cloud” can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.}
}
@article{KATARIA2019101429,
title = {Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {33},
number = {4},
pages = {101429},
year = {2019},
note = {How to Investigate: Very Early Inflammatory Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2019.101429},
url = {https://www.sciencedirect.com/science/article/pii/S1521694219300981},
author = {Suchitra Kataria and Vinod Ravindran},
keywords = {Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health},
abstract = {Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.}
}
@article{MERHI2021121180,
title = {Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121180},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121180},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006132},
author = {Mohammad I. Merhi},
keywords = {Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP},
abstract = {This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.}
}
@article{CINNAMON2016253,
title = {Evidence and future potential of mobile phone data for disease disaster management},
journal = {Geoforum},
volume = {75},
pages = {253-264},
year = {2016},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2016.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0016718516301981},
author = {Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger},
keywords = {Mobile phone, Call detail records, SMS, Disaster, Disease, Big data},
abstract = {Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.}
}
@article{RIESENER2019304,
title = {Framework for defining information quality based on data attributes within the digital shadow using LDA},
journal = {Procedia CIRP},
volume = {83},
pages = {304-310},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119304366},
author = {Michael Riesener and Christian Dölle and Günther Schuh and Christian Tönnes},
keywords = {Digital Shadow, Information quality, Data quality, Latent dirichlet allocation (LDA)},
abstract = {The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on “information quality” and “data quality” identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.}
}
@article{NEWHART2019498,
title = {Data-driven performance analyses of wastewater treatment plants: A review},
journal = {Water Research},
volume = {157},
pages = {498-513},
year = {2019},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2019.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S0043135419302490},
author = {Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath},
keywords = {Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring},
abstract = {Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.}
}
@incollection{MOISE2015279,
title = {Chapter 12 - Terabyte-Scale Image Similarity Search},
editor = {Venu Govindaraju and Vijay V. Raghavan and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {33},
pages = {279-301},
year = {2015},
booktitle = {Big Data Analytics},
issn = {0169-7161},
doi = {https://doi.org/10.1016/B978-0-444-63492-4.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634924000125},
author = {Diana Moise and Denis Shestakov},
keywords = {Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment, SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves},
abstract = {While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.}
}
@article{MUNINGER2022140,
title = {Social media use: A review of innovation management practices},
journal = {Journal of Business Research},
volume = {143},
pages = {140-156},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322000510},
author = {Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi},
keywords = {Social media, Innovation, Systematic review, Framework and research agenda},
abstract = {The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.}
}
@article{LLAVE2018516,
title = {Data lakes in business intelligence: reporting from the trenches},
journal = {Procedia Computer Science},
volume = {138},
pages = {516-524},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918317046},
author = {Marilex Rea Llave},
keywords = {Business intelligence, big data, data lake, BI architecture},
abstract = {The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented.}
}
@article{HASSANI2021121111,
title = {The science of statistics versus data science: What is the future?},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121111},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121111},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521005448},
author = {Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag Øivind Madsen},
keywords = {Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism},
abstract = {The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.}
}
@article{MATHEUS2020101284,
title = {Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101284},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18300303},
author = {Ricardo Matheus and Marijn Janssen and Devender Maheshwari},
keywords = {Data science, Dashboards, E-government, Open government, Open data, Big data, Smart City, Design principles, Transparency, Accountability, Trust, Policy-making, Decision-making},
abstract = {Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.}
}
@incollection{PEZOULAS202019,
title = {Chapter 2 - Types and sources of medical and other related data},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {19-65},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000025},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Big data, Cohorts, Medical data acquisition, Sources of medical data, Types of medical data},
abstract = {This chapter presents the origin of medical data that comprises the core of any federated cloud platform that deals with medical data sharing and analytics. The different types and sources of medical data are extensively described along with applications and standard data acquisition protocols. Emphasis is given on the definition and the impact of the cohorts in clinical research, as well as the importance of the big data in medicine. The impact of the big data in medicine is also discussed along with emerging opportunities and challenges. The need to develop data standardization protocols across heterogeneous and dispersed data sources is finally highlighted, to enable the analysis of different types of medical big data.}
}
@article{GAHA2021216,
title = {Towards the implementation of the Digital Twin in CMM inspection process: opportunities, challenges and proposals},
journal = {Procedia Manufacturing},
volume = {54},
pages = {216-221},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001694},
author = {Raoudha Gaha and Alexandre Durupt and Benoit Eynard},
keywords = {Digital Twin, CMM, inspection, Model-based-defintion, Digital thread},
abstract = {The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.}
}
@article{AHMAD2022112128,
title = {Data-driven probabilistic machine learning in sustainable smart energy/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm},
journal = {Renewable and Sustainable Energy Reviews},
volume = {160},
pages = {112128},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112128},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122000569},
author = {Tanveer Ahmad and Rafal Madonski and Dongdong Zhang and Chao Huang and Asad Mujeeb},
keywords = {Data-driven probabilistic machine learning, Energy distribution, Discovery and design of energy materials, Big data analytics and smart grid, Strategic energy planning and smart manufacturing, Energy demand-side response},
abstract = {The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.}
}