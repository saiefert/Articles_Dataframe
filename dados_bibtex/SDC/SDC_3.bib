@article{GHASEMAGHAEI201969,
title = {Does big data enhance firm innovation competency? The mediating role of data-driven insights},
journal = {Journal of Business Research},
volume = {104},
pages = {69-84},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304138},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency},
abstract = {Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.}
}
@article{HASHEM201598,
title = {The rise of “big data” on cloud computing: Review and open research issues},
journal = {Information Systems},
volume = {47},
pages = {98-115},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001288},
author = {Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan}},
keywords = {Big data, Cloud computing, Hadoop},
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.}
}
@article{TAI2019101704,
title = {Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry},
journal = {Artificial Intelligence in Medicine},
volume = {99},
pages = {101704},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101704},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717301781},
author = {Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre},
keywords = {Big data, Machine learning, Precision medicine, AI, Mental health, Mental disease, Psychiatry, Data mining, RDoC, Research domain criteria, DSM-5. Schizophrenia, ADHD, Alzheimer, Depression, fMRI, MRI, Algorithms, IBM Watson, Neuro networking, Random forests, Decision trees, Support vector machines},
abstract = {Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.}
}
@incollection{PLOTKIN2021245,
title = {Chapter 10 - Big Data Stewardship and Data Lakes},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {245-255},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000103},
author = {David Plotkin},
keywords = {Big data, data lake, unstructured data, zone},
abstract = {Big Data Governance and big data stewardship are not so different from what we’ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of “getting it wrong” due to not only the large quantities of data and metadata, but also the speed at which the data can change.}
}
@article{BAYNE2018481,
title = {Big Data in Neonatal Health Care: Big Reach, Big Reward?},
journal = {Critical Care Nursing Clinics of North America},
volume = {30},
number = {4},
pages = {481-497},
year = {2018},
note = {Neonatal Nursing},
issn = {0899-5885},
doi = {https://doi.org/10.1016/j.cnc.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899588518309754},
author = {Lynn E. Bayne},
keywords = {Big data, Electronic health record (EHR), Neonatology, Cost-savings, Clinical decision making, Healthcare analytics}
}
@article{SAFHI201930,
title = {Assessing reliability of Big Data Knowledge Discovery process},
journal = {Procedia Computer Science},
volume = {148},
pages = {30-36},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919300055},
author = {Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi},
keywords = {Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining},
abstract = {Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.}
}
@article{KUN2019556,
title = {Application of Big Data Technology in Scientific Research Data Management of Military Enterprises},
journal = {Procedia Computer Science},
volume = {147},
pages = {556-561},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.221},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302406},
author = {Wang Kun and Liu Tong and Xie Xiaodan},
keywords = {big data technology, scientific research data, data analysis, decision},
abstract = {Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.}
}
@article{HONG2018175,
title = {Big Data in Health Care: Applications and Challenges},
journal = {Data and Information Management},
volume = {2},
number = {3},
pages = {175-197},
year = {2018},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2018-0014},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000791},
author = {Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu},
keywords = {Big Data, public health, cloud computing, medical applications},
abstract = {The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.}
}
@article{JANSSEN2017338,
title = {Factors influencing big data decision-making quality},
journal = {Journal of Business Research},
volume = {70},
pages = {338-345},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304945},
author = {Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi},
keywords = {Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality},
abstract = {Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.}
}
@article{TRUJILLO2021101911,
title = {Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue},
journal = {Data & Knowledge Engineering},
volume = {135},
pages = {101911},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101911},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000380},
author = {Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey},
keywords = {Conceptual modeling, Big Data, Machine learning, Artificial Intelligence},
abstract = {Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.}
}
@article{LV2021103298,
title = {Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data},
journal = {Cities},
volume = {116},
pages = {103298},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121001980},
author = {Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng},
keywords = {Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities},
abstract = {With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.}
}
@article{LI20191259,
title = {Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology},
journal = {Applied Energy},
volume = {242},
pages = {1259-1273},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.03.154},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919305495},
author = {Shuangqi Li and Hongwen He and Jianwei Li},
keywords = {Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning},
abstract = {As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.}
}
@article{CORTEREAL2019160,
title = {Unlocking the drivers of big data analytics value in firms},
journal = {Journal of Business Research},
volume = {97},
pages = {160-173},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318306908},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič},
keywords = {IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage},
abstract = {Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.}
}
@article{HADJSASSI2019534,
title = {A New Architecture for Cognitive Internet of Things and Big Data},
journal = {Procedia Computer Science},
volume = {159},
pages = {534-543},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313924},
author = {Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati},
keywords = {Internet of Things, Big-Data, Architecture, Cognitive, Data-flow},
abstract = {Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.}
}
@article{REN20191343,
title = {A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions},
journal = {Journal of Cleaner Production},
volume = {210},
pages = {1343-1365},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618334255},
author = {Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida},
keywords = {Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle},
abstract = {Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.}
}
@article{ALBADI2018271,
title = {Exploring Big Data Governance Frameworks},
journal = {Procedia Computer Science},
volume = {141},
pages = {271-277},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318313},
author = {Ali Al-Badi and Ali Tarhini and Asharul Islam Khan},
keywords = {Big Data, Big Data model, Big Data governance, Data management, Big Data governance framework, Big Data analytic},
abstract = {The recent explosion in ICT and digital data has led organizations, both private and public, to efficient decision-making. Nowadays organizations can store huge amounts of data, which can be accessible at any time. Big Data governance refers to the management of huge volumes of an organization’s data, exploiting it in the organization’s decision-making using different analytical tools. Big Data emergence provides great convenience, but it also brings challenges. Nevertheless, for Big Data governance, data has to be prepared in a timely manner, keeping in view the consistency and reliability of the data, and being able to trust its source and the meaningfulness of the result. Hence, a framework for Big Data governance would have many advantages. There are Big Data governance frameworks, which guide the management of Big Data. However, there are also limitations associated with these frameworks. Therefore, this study aims to explore the existing Big Data governance frameworks and their shortcomings, and propose a new framework. The proposed framework consists of eight components. As a framework validation, the proposed framework has been compared with the ISO 8000 data governance framework.}
}
@article{SIEMSANDERSON2019100071,
title = {An adaptive big data weather system for surface transportation},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {3},
pages = {100071},
year = {2019},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2019.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2590198219300703},
author = {Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt},
keywords = {Big data, Pikalert, Road weather, Surface transportation, Pavement condition, Weather forecasts},
abstract = {Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert® system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.}
}
@article{YIN2022104285,
title = {Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning},
journal = {Tunnelling and Underground Space Technology},
volume = {120},
pages = {104285},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104285},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004764},
author = {Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan},
keywords = {TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning},
abstract = {The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.}
}
@article{LIN2019197,
title = {Data source selection for information integration in big data era},
journal = {Information Sciences},
volume = {479},
pages = {197-213},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309162},
author = {Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao},
keywords = {Source selection, Data integration, Data cleaning},
abstract = {In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.}
}
@incollection{NOH2022639,
title = {20 - Big data analysis for civil infrastructure sensing},
editor = {Jerome P. Lynch and Hoon Sohn and Ming L. Wang},
booktitle = {Sensor Technologies for Civil Infrastructures (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {639-677},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-08-102706-6},
doi = {https://doi.org/10.1016/B978-0-08-102706-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081027066000076},
author = {Hae Young Noh and Jonathon Fagert},
keywords = {Big data, Civil infrastructure sensing, Damage diagnosis, Machine learning, Occupant monitoring, Smart infrastructure, Structural health monitoring},
abstract = {With the growing scale and complexity of city infrastructures, the need for data analysis and machine learning is becoming more and more prominent in the field of civil infrastructure sensing. This coupled with the explosion of available sensing data in smart cities and smart infrastructures has offered new opportunities like never before. Using big data tools at a structure level, we can understand important information about structural properties and damage states, city environmental and operational conditions, as well as an individual user or group patterns. In this chapter, we explore and provide guidance for big data analytics and its application to civil infrastructure problems. Furthermore, we discuss future directions and trends that will enable large-scale monitoring of civil infrastructure and smart cities.}
}
@article{NASHIPUDIMATH2020100033,
title = {An efficient integration and indexing method based on feature patterns and semantic analysis for big data},
journal = {Array},
volume = {7},
pages = {100033},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300187},
author = {Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain},
keywords = {Big data, Integration, Feature patterns, Indexing, Semantic analysis},
abstract = {Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.}
}
@incollection{ANGADI2022151,
title = {Chapter Seven - Role of big data analytic and machine learning in power system contingency analysis},
editor = {Rakesh Sehgal and Neeraj Gupta and Anuradha Tomar and Mukund Dutt Sharma and Vigna Kumaran},
booktitle = {Smart Electrical and Mechanical Systems},
publisher = {Academic Press},
pages = {151-184},
year = {2022},
isbn = {978-0-323-90789-7},
doi = {https://doi.org/10.1016/B978-0-323-90789-7.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390789700004X},
author = {Ravi V. Angadi and Suresh Babu Daram and P.S. Venkataramu},
keywords = {Big data, Classification, Contingency analysis, Critical, Data collection, Data mining, Data processing, Data visualization, Decision tree, Different load conditions, Energy systems, Machine learning, Non critical, Powersystem, Semi critical, Severity prediction, Structured data, Testing data set, Training data set, Voltage stability, Voltage stability index, Volume of data},
abstract = {This work discusses the use of big data and machine learning to predict the severity of a system breakdown caused by an n−1 transmission line condition. The contingency analysis is a key part of traditional energy management systems. The severity of the line is identified by computing the Line Voltage Stability Index The large amount of data handling will be involved during the contingency study. These data need to be processed and analyzed properly by data handling technique and the use of machine learning tools arrive required information in the system. The severity of transmission lines is predicted and compared using classification approaches. To create large data, MATLAB simulation results will be used and the machine learning tool, Weka is used to analyze the data and forecast transmission line. The standard IEEE 30 bus system is considered to understand the proposed methodology.}
}
@article{DECAMARGOFIORINI2018112,
title = {Management theory and big data literature: From a review to a research agenda},
journal = {International Journal of Information Management},
volume = {43},
pages = {112-129},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830553X},
author = {Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour}},
keywords = {Big data, Big data analytics, Organizational theory, Firms’ performance, Research agenda},
abstract = {The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.}
}
@article{VANDERVOORT201927,
title = {Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {27-38},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17304951},
author = {H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer},
abstract = {Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.}
}
@article{MIKALEF2020103169,
title = {Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities},
journal = {Information & Management},
volume = {57},
number = {2},
pages = {103169},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618301022},
author = {Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou},
keywords = {Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view},
abstract = {A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm’s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.}
}
@article{ANEJIONU2019456,
title = {Spatial urban data system: A cloud-enabled big data infrastructure for social and economic urban analytics},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {456-473},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.03.052},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18319046},
author = {Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole},
keywords = {Urban big data infrastructure, Urban analytics, Spatial urban indicators, Small area assessment, Spatial big data},
abstract = {The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.}
}
@article{RAJABION2019271,
title = {Healthcare big data processing mechanisms: The role of cloud computing},
journal = {International Journal of Information Management},
volume = {49},
pages = {271-289},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217304917},
author = {Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar},
keywords = {Cloud computing, Processing, Healthcare, Big data, Review},
abstract = {Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.}
}
@article{UEDA2019150,
title = {Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era},
journal = {Molecular Plant},
volume = {12},
number = {2},
pages = {150-152},
year = {2019},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1674205219300139},
author = {Yoshiaki Ueda and Shuichi Yanagisawa}
}
@article{HUGHES2020120300,
title = {Sowing the seeds of value? Persuasive practices and the embedding of big data analytics},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120300},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120300},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311264},
author = {Jeffrey Hughes and Kirstie Ball},
keywords = {Big data analytics, Persuasion, Practice, Capabilities, Value},
abstract = {This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.}
}
@article{BRADLOW201779,
title = {The Role of Big Data and Predictive Analytics in Retailing},
journal = {Journal of Retailing},
volume = {93},
number = {1},
pages = {79-95},
year = {2017},
note = {The Future of Retailing},
issn = {0022-4359},
doi = {https://doi.org/10.1016/j.jretai.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022435916300835},
author = {Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti},
keywords = {Big data, Predictive analytics, Retailing, Pricing},
abstract = {The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.}
}
@article{HAJLI2020135,
title = {Understanding market agility for new product success with big data analytics},
journal = {Industrial Marketing Management},
volume = {86},
pages = {135-143},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118304735},
author = {Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem},
keywords = {Big data analytics, Customer agility, Effective use of data, New product success},
abstract = {The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.}
}
@article{GALETSI2019112533,
title = {Values, challenges and future directions of big data analytics in healthcare: A systematic review},
journal = {Social Science & Medicine},
volume = {241},
pages = {112533},
year = {2019},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2019.112533},
url = {https://www.sciencedirect.com/science/article/pii/S0277953619305271},
author = {P. Galetsi and K. Katsaliaki and S. Kumar},
keywords = {Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses},
abstract = {The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.}
}
@incollection{LYTRAS2021xvii,
title = {Preface: artificial intelligence and big data analytics for smart healthcare: a digital transformation of healthcare primer},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {xvii-xxvii},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000188},
author = {Miltiadis D. Lytras and Anna Visvizi and Akila Sarirete and Kwok Tai Chui}
}
@article{URREHMAN2019247,
title = {The role of big data analytics in industrial Internet of Things},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {247-259},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18313645},
author = {Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera},
keywords = {Internet of Things, Cyber-physical systems, Cloud computing, Analytics, Big data},
abstract = {Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.}
}
@incollection{DAS2020127,
title = {Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare},
editor = {Amit Banerjee and Basabi Chakraborty and Hiroshi Inokawa and Jitendra {Nath Roy}},
booktitle = {Terahertz Biomedical and Healthcare Technologies},
publisher = {Elsevier},
pages = {127-143},
year = {2020},
isbn = {978-0-12-818556-8},
doi = {https://doi.org/10.1016/B978-0-12-818556-8.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185568000070},
author = {Debashis Das and Chinmay Chakraborty and Sourav Banerjee},
keywords = {3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images},
abstract = {This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, “big data” is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary “V's” of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.}
}
@article{CABALLERO2022102058,
title = {BR4DQ: A methodology for grouping business rules for data quality evaluation},
journal = {Information Systems},
volume = {109},
pages = {102058},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2022.102058},
url = {https://www.sciencedirect.com/science/article/pii/S0306437922000485},
author = {Ismael Caballero and Fernando Gualo and Moisés Rodríguez and Mario Piattini},
keywords = {Business rules, Data quality, Data quality evaluation, Data quality measurement, Data quality characteristics, Data quality properties, ISO/IEC 25012, ISO/IEC 25024},
abstract = {Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.}
}
@article{HOLMLUND2020356,
title = {Customer experience management in the age of big data analytics: A strategic framework},
journal = {Journal of Business Research},
volume = {116},
pages = {356-365},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320300345},
author = {Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki},
keywords = {Customer experience, Customer experience management, Customer experience insight, Big data analytics},
abstract = {Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.}
}
@incollection{CYCHOSZ20221,
title = {Chapter One - Using big data from long-form recordings to study development and optimize societal impact},
editor = {Rick O. Gilmore and Jeffrey J. Lockman},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {62},
pages = {1-36},
year = {2022},
booktitle = {New Methods and Approaches for Studying Child Development},
issn = {0065-2407},
doi = {https://doi.org/10.1016/bs.acdb.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065240721000434},
author = {Margaret Cychosz and Alejandrina Cristia},
keywords = {Big data, Wearable technology, Algorithm bias, Automatic measurement, Language, Audio recording, Children},
abstract = {Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings—clinical treatment, large-scale interventions, and language documentation—where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.}
}
@article{GOLDSTEIN2022100215,
title = {Visual Acuity – Assessment of Data Quality and Usability in an Electronic Health Record System},
journal = {Ophthalmology Science},
pages = {100215},
year = {2022},
issn = {2666-9145},
doi = {https://doi.org/10.1016/j.xops.2022.100215},
url = {https://www.sciencedirect.com/science/article/pii/S266691452200104X},
author = {Judith E. Goldstein and Xinxing Guo and Michael V. Boland and Kerry E. Smith},
abstract = {ABSTRACT
Objective
To examine the data quality and usability of visual acuity (VA) data extracted from an electronic health record (EHR) system during ophthalmology encounters and provide recommendations for consideration of relevant VA endpoints in retrospective analyses.
Design
Retrospective, EHR data analysis
Participants
All patients with eyecare office encounters at any one of the nine locations of a large academic medical center between August 1st 2013 and December 31st 2015.
Methods
Data from 13 of the 21 VA fields (accounting for 93% VA data) in EHR encounters were extracted, categorized, recoded, and assessed for conformance and plausibility using an internal data dictionary, a 38-item listing of VA line measurements and observations including 28 line-measurements (e.g., 20/30, 20/400) and 10 observations (e.g., NLP [no light perception]). Entries were classified into usable and unusable data. Usable data were further categorized based on conformance to internal data dictionary: (1) exact match; (2) conditional conformance, letter count (e.g., 20/30+2-3); (3) convertible conformance (e.g., 5/200 to 20/800); (4) plausible but cannot be conformed (e.g., 5/400). Data were deemed unusable when they were not plausible.
Main Outcome Measures
Proportions of usable and unusable VA entries at the overall and subspecialty levels.
Results
All VA data from 513,036 encounters representing 166,212 patients were included. Of the 1,573,643 VA entries, 1,438,661 (91.4%) contained usable data. There were 1,196,720 (76.1%) exact match (category 1), 185,692 (11.8%) conditional conformance (category 2), 40,270 (2.6%) convertible conformance (category 3), and 15,979 (1.1%) plausible but not conformed entries. VA entries during visits with providers from retina (17.5%), glaucoma (14.0%), neuro-ophthalmology (8.9%), and low vision (8.8%) had the highest rates of unusable data. Documented VA entries with providers from comprehensive eye care (86.7%), oculoplastics (81.5%), and pediatrics/strabismus (78.6%) yielded the highest proportions of exact match with the data dictionary.
Conclusions
EHR VA data quality and usability vary across documented VA measures, observations, and eyecare subspecialty. We proposed a checklist of considerations and recommendations for planning, extracting, analyzing, and reporting retrospective study outcomes using EHR VA data. These are important first steps to standardize analyses enabling comparative research.}
}
@article{BIESIALSKA2021106448,
title = {Big Data analytics in Agile software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {132},
pages = {106448},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106448},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301981},
author = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
keywords = {Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review},
abstract = {Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.}
}
@article{BENASSULI20197,
title = {Bringing big data analytics closer to practice: A methodological explanation and demonstration of classification algorithms},
journal = {Health Policy and Technology},
volume = {8},
number = {1},
pages = {7-13},
year = {2019},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211883718301631},
author = {Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner},
keywords = {Congestive heart failure, Machine learning, Logistic regression, Boosted decision tree, Support vector machine, Neural network},
abstract = {Background
Big data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.
Objectives
To methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.
Methods
We demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.
Results
The results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.
Conclusions
The results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.}
}
@article{RAUT201910,
title = {Linking big data analytics and operational sustainability practices for sustainable business management},
journal = {Journal of Cleaner Production},
volume = {224},
pages = {10-24},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.03.181},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619308753},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede},
keywords = {Big-data analytics, Ecological-economic-social sustainability, Green practices, SustainableOperations management, Structural equation modelling-artificial neural network, Emerging economies},
abstract = {Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.}
}
@incollection{LOSHIN201339,
title = {Chapter 5 - Data Governance for Big Data Analytics: Considerations for Data Policies and Processes},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {39-48},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000053},
author = {David Loshin},
keywords = {Data governance, accuracy, completeness, consistency, currency, data requirements, consumer expectations, data quality dimensions, metadata, reference data, repurposing, enrichment, enhancement},
abstract = {In this chapter we look at the need for oversight and governance for the data, especially when those developing big data applications often bypass traditional IT and data management channels. Some of the key issues involve the fact that for big data applications that consume massive amounts of data streamed from external sources, there is little or no control that can be exerted to ensure data quality and usability. We consider five key concepts, namely managing data consumer expectations, defining critical data quality dimensions, monitoring consistency of metadata, data repurposing and reinterpretation, and data enrichment when possible.}
}
@article{BOLDOSOVA2020122,
title = {Telling stories that sell: The role of storytelling and big data analytics in smart service sales},
journal = {Industrial Marketing Management},
volume = {86},
pages = {122-134},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118303353},
author = {Valeriia Boldosova},
keywords = {Storytelling, Big data analytics, Smart service, Customer reference, Customer-supplier relationships},
abstract = {The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.}
}
@article{FENG2021103636,
title = {Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning},
journal = {Tunnelling and Underground Space Technology},
volume = {110},
pages = {103636},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103636},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820305903},
author = {Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing},
keywords = {TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction},
abstract = {This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days’ continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.}
}
@article{LV2020103,
title = {Analysis of healthcare big data},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {103-110},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20304829},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Health care, Privacy security risk, Privacy measures},
abstract = {In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China’s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.}
}
@article{ELAGGOUNE2020465,
title = {A fuzzy agent approach for smart data extraction in big data environments},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {4},
pages = {465-478},
year = {2020},
note = {Emerging Software Systems},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819302010},
author = {Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough},
keywords = {Big data, Multi-agent systems, Wireless sensor network, Fuzzy logic, Smart data},
abstract = {The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.}
}
@incollection{HULSEN202169,
title = {Chapter 4 - Challenges and solutions for big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {69-94},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000167},
author = {Tim Hulsen},
keywords = {Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics},
abstract = {“Big data” is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to “long data,” “wide data,” and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.}
}
@article{GILL202051,
title = {Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model},
journal = {Clinics in Laboratory Medicine},
volume = {40},
number = {1},
pages = {51-59},
year = {2020},
note = {Direct-to-Consumer Testing: The Role of Laboratory Medicine},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300927},
author = {Emily L. Gill and Stephen R. Master},
keywords = {Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization}
}
@article{VIEIRA2020125,
title = {Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio},
journal = {Procedia Manufacturing},
volume = {42},
pages = {125-131},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.093},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306582},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Industry 4.0},
abstract = {The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders’ interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.}
}
@article{DAISSAOUI2020161,
title = {IoT and Big Data Analytics for Smart Buildings: A Survey},
journal = {Procedia Computer Science},
volume = {170},
pages = {161-168},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304506},
author = {Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath},
keywords = {Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing},
abstract = {The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.}
}
@article{BELHADI2019106099,
title = {Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies},
journal = {Computers & Industrial Engineering},
volume = {137},
pages = {106099},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.106099},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219305686},
author = {Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi}},
keywords = {Big Data Analytics, Manufacturing process, Big Data Analytics capabilities, Business intelligence, Literature review, Multiple case study},
abstract = {Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.}
}
@article{HAMALAINEN2019100105,
title = {Industrial applications of big data in disruptive innovations supporting environmental reporting},
journal = {Journal of Industrial Information Integration},
volume = {16},
pages = {100105},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.100105},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300044},
author = {Esa Hämäläinen and Tommi Inkinen},
keywords = {Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography},
abstract = {Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.}
}
@article{ZHANG2019814,
title = {Big data driven decision-making for batch-based production systems},
journal = {Procedia CIRP},
volume = {83},
pages = {814-818},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119310169},
author = {Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li},
keywords = {Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan},
abstract = {The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).}
}
@article{TU2020101428,
title = {Portraying the spatial dynamics of urban vibrancy using multisource urban big data},
journal = {Computers, Environment and Urban Systems},
volume = {80},
pages = {101428},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2019.101428},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519302674},
author = {Wei Tu and Tingting Zhu and Jizhe Xia and Yulun Zhou and Yani Lai and Jincheng Jiang and Qingquan Li},
keywords = {Urban vibrancy, Geographically weighted regression, mobile phone data, Social media, Points-of-interest, Big data},
abstract = {Understanding urban vibrancy aids policy-making to foster urban space and therefore has long been a goal of urban studies. Recently, the emerging urban big data and urban analytic methods have enabled us to portray citywide vibrancy. From the social sensing perspective, this study presents a comprehensive and comparative framework to cross-validate urban vibrancy and uncover associated spatial effects. Spatial patterns of urban vibrancy indicated by multisource urban sensing data (points-of-interest, social media check-ins, and mobile phone records) were investigated. A comprehensive urban vibrancy metric was formed by adaptively weighting these metrics. The association between urban vibrancy and demographic, economic, and built environmental factors was revealed with global regression models and local regression models. An empirical experiment was conducted in Shenzhen. The results demonstrate that four urban vibrancy metrics are all higher in the special economic zone (SEZ) and lower in non-SEZs but with different degrees of spatial aggregation. The influences of employment and road density on all vibrancy metrics are significant and positive. However, the effects of metro stations, land use mix, building footprints, and distance to district center depend on the vibrancy indicator and location. These findings unravel the commonalities and differences in urban vibrancy metrics derived from multisource urban big data and the hidden spatial dynamics of the influences of associated factors. They further suggest that urban policies should be proposed to foster vibrancy in Shenzhen therefore benefit social wellbeing and urban development in the long term. They also provide valuable insights into the reliability of urban big data-driven urban studies.}
}
@article{WANG202110,
title = {An interview with Shouyang Wang: research frontier of big data-driven economic and financial forecasting},
journal = {Data Science and Management},
volume = {1},
number = {1},
pages = {10-12},
year = {2021},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666764921000011},
author = {Shouyang Wang},
keywords = {Big data, Economic forecasting, Data mining, Spatio-temporal},
abstract = {The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.}
}
@article{GHOLIZADEH2020120640,
title = {A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data},
journal = {Journal of Cleaner Production},
volume = {258},
pages = {120640},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.120640},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620306879},
author = {Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh},
keywords = {logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, ε-Constraint},
abstract = {Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.}
}
@article{JHA2020113382,
title = {A note on big data analytics capability development in supply chain},
journal = {Decision Support Systems},
volume = {138},
pages = {113382},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113382},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301378},
author = {Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai},
keywords = {Big data, Analytics, Capability development, Qualitative study, Supply chain},
abstract = {Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.}
}
@article{URBAN2020117792,
title = {Application of big data analysis technique on high-velocity airblast atomization: Searching for optimum probability density function},
journal = {Fuel},
volume = {273},
pages = {117792},
year = {2020},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2020.117792},
url = {https://www.sciencedirect.com/science/article/pii/S0016236120307870},
author = {András Urbán and Axel Groniewsky and Milan Malý and Viktor Józsa and Jan Jedelský},
keywords = {Big data, Airblast, Rapeseed oil, PDA, Probability density function, Likelihood},
abstract = {In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4 bar and 25 to 100 °C, respectively. Approximately 400 million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.}
}
@article{ROOS202218,
title = {Record linkage and big data—enhancing information and improving design},
journal = {Journal of Clinical Epidemiology},
volume = {150},
pages = {18-24},
year = {2022},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2022.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0895435622001524},
author = {Leslie L. Roos and Elizabeth Wall-Wieler and Charles Burchill and Naomi C. Hamm and Amani F. Hamad and Lisa M. Lix},
keywords = {Research possibilities, Covariates, Family data, Registries, Observational studies, Design},
abstract = {Background and Objectives
To highlight the potential of multiple file record linkage. Linkage increases the value of existing information by supplying missing data or correcting errors in existing data, through generating important covariates, and by using family information to control for unmeasured variables and expand research opportunities.
Methods
Recent Manitoba papers highlight the use of linkage to produce better studies. Specific ways in which linkage helps deal with different substantive issues are described.
Results
Wide data files—files containing considerable amounts of information on each individual—generated by linkage improve research by facilitating better design. Nonexperimental work in particular benefits from such linkages. Population registries are especially valuable in supplying family data to facilitate work across different substantive fields.
Conclusion
Several examples show how record linkage magnifies the value of information from individual projects. The results of observational studies become more defensible through the better designs facilitated by such linkage.}
}
@article{RANA2019807,
title = {How Big Data Science Can Improve Linkage and Retention in Care},
journal = {Infectious Disease Clinics of North America},
volume = {33},
number = {3},
pages = {807-815},
year = {2019},
note = {HIV},
issn = {0891-5520},
doi = {https://doi.org/10.1016/j.idc.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0891552019300455},
author = {Aadia I. Rana and Michael J. Mugavero},
keywords = {HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance}
}
@incollection{BACHHETY202145,
title = {2 - Big Data Analytics for healthcare: theory and applications},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {45-67},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000084},
author = {Shivam Bachhety and Shivani Kapania and Rachna Jain},
keywords = {Big Data, healthcare, Big Data Analytics, Hadoop},
abstract = {In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.}
}
@article{PEDRO20193,
title = {Capabilities and Readiness for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {164},
pages = {3-10},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.147},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321866},
author = {Jenifer Pedro and Irwin Brown and Mike Hart},
keywords = {Big data analytics, organisational readiness, organisational capabilities, frameworks, business analytics, thematic analysis},
abstract = {Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.}
}
@article{OPREA2021106902,
title = {Insights into demand-side management with big data analytics in electricity consumers’ behaviour},
journal = {Computers & Electrical Engineering},
volume = {89},
pages = {106902},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106902},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620307540},
author = {Simona-Vasilica Oprea and Adela Bâra and Bogdan George Tudorică and Maria Irène Călinoiu and Mihai Alexandru Botezatu},
keywords = {Big data, Machine learning, Smart meters, Electricity consumption, Clustering, Questionnaire analytics},
abstract = {The consumption data from smart meters and complex questionnaires reveals the electricity consumers’ willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30 min intervals, as well as data collected from complex questionnaires.}
}
@article{AITHAMMOU2020102122,
title = {Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics},
journal = {Information Processing & Management},
volume = {57},
number = {1},
pages = {102122},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102122},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319305163},
author = {Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline},
keywords = {Big data, FastText, Recurrent neural networks, LSTM, BiLSTM, GRU, Natural language processing, Sentiment analysis, Social big data analytics},
abstract = {Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users’ behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.}
}
@article{WANG2020119299,
title = {Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits},
journal = {Journal of Cleaner Production},
volume = {248},
pages = {119299},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119299},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619341691},
author = {Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu},
keywords = {multi-life-cycle remanufacturing, Sustainable products, Big data, CPS-Digital-twin(CPSDT), IoT-cloud, Reconfiguration},
abstract = {Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.}
}
@article{HUANG2021144535,
title = {An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms},
journal = {Science of The Total Environment},
volume = {769},
pages = {144535},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.144535},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720380669},
author = {Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan},
keywords = {Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation},
abstract = {An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km × 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23–55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5–17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.}
}
@article{GE2018601,
title = {Big Data for Internet of Things: A Survey},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {601-614},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316953},
author = {Mouzhi Ge and Hind Bangui and Barbora Buhnova},
keywords = {Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation, Building automation, Smart Cities},
abstract = {With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.}
}
@article{RAHUL2020364,
title = {Data Life Cycle Management in Big Data Analytics},
journal = {Procedia Computer Science},
volume = {173},
pages = {364-371},
year = {2020},
note = {International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920315465},
author = {Kumar Rahul and Rohitash Kumar Banyal},
keywords = {Data life cycle, Data creation, Data usability, Healthcare, Big data},
abstract = {Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data’s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.}
}
@article{MIKALEF2019261,
title = {Big data analytics and firm performance: Findings from a mixed-method approach},
journal = {Journal of Business Research},
volume = {98},
pages = {261-276},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S014829631930061X},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty},
abstract = {Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.}
}
@article{MATHIS2020582,
title = {Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {34},
number = {3},
pages = {582-585},
year = {2020},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053077019311590},
author = {Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren}
}
@article{ARIYALURANHABEEB2019289,
title = {Real-time big data processing for anomaly detection: A Survey},
journal = {International Journal of Information Management},
volume = {45},
pages = {289-307},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218301658},
author = {Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran},
keywords = {Real-time, Big data processing, Anomaly detection and machine learning algorithms},
abstract = {The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.}
}
@article{SHAW2021108953,
title = {Marine big data analysis of ships for the energy efficiency changes of the hull and maintenance evaluation based on the ISO 19030 standard},
journal = {Ocean Engineering},
volume = {232},
pages = {108953},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108953},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821003887},
author = {Heiu-Jou Shaw and Cheng-Kuan Lin},
keywords = {Energy efficiency management, ISO 19030, Hull and propeller maintenance},
abstract = {This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.}
}
@article{JONES20193,
title = {What we talk about when we talk about (big) data},
journal = {The Journal of Strategic Information Systems},
volume = {28},
number = {1},
pages = {3-16},
year = {2019},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0963868718302622},
author = {Matthew Jones},
abstract = {In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between “data in principle” as they are recorded, and the “data in practice” as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.}
}
@article{LIN2022120320,
title = {Enhanced commercial cooking inventories from the city scale through normalized emission factor dataset and big data},
journal = {Environmental Pollution},
pages = {120320},
year = {2022},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2022.120320},
url = {https://www.sciencedirect.com/science/article/pii/S0269749122015342},
author = {Pengchuan Lin and Jian Gao and Yisheng Xu and James J. Schauer and Jiaqi Wang and Wanqing He and Lei Nie},
keywords = {Point of interest data, Emission inventory, Cooking emission factors, Dataset, Beijing},
abstract = {Cooking emission inventories always have poor spatial resolutions when applying with traditional methods, making their impacts on ambient air and human health remain obscure. In this study, we created a systematic dataset of cooking emission factors (CEFs) and applied it with a new data source, cooking-related point of interest (POI) data, to build up highly spatial resolved cooking emission inventories from the city scale. Averaged CEFs of six particulate and gaseous species (PM, OC, EC, NMHC, OVOCs, VOCs) were 5.92 ± 6.28, 4.10 ± 5.50, 0.05 ± 0.05, 22.54 ± 20.48, 1.56 ± 1.44, and 7.94 ± 6.27 g/h normalized in every cook stove, respectively. A three-field CEF index containing activity and emission factor species was created to identify and further build a connection with cooking-related POI data. A total of 95,034 cooking point sources were extracted from Beijing, as a study city. In downtown areas, four POI types were overlapped in the central part of the city and radiated into eight distinct directions from south to north. Estimated PM/VOC emissions caused by cooking activities in Beijing were 4.81/9.85 t per day. A 3D emission map showed an extremely unbalanced emission density in the Beijing region. Emission hotspots were seen in Central Business District (CBD), Sanlitun, and Wangjing in Chaoyang District and Willow and Zhongguancun in Haidian District. PM/VOC emissions could be as high as 16.6/42.0 kg/d in the searching radius of 2 km. For PM, the total emissions were 417.4, 389.0, 466.9, and 443.0 t between Q1 and Q4 2019 in Beijing, respectively. The proposed methodology is transferrable to other Chinese cities for deriving enhanced commercial cooking inventories and potentially highlighting the further importance of cooking emissions on air quality and human health.}
}
@incollection{ILMUDEEN202133,
title = {Chapter 3 - Big data-based frameworks for healthcare systems},
editor = {Pradeep N and Sandeep Kautish and Sheng-Lung Peng},
booktitle = {Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics},
publisher = {Academic Press},
pages = {33-56},
year = {2021},
isbn = {978-0-12-821633-0},
doi = {https://doi.org/10.1016/B978-0-12-821633-0.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128216330000039},
author = {Aboobucker Ilmudeen},
keywords = {Big data, Frameworks, Healthcare, Healthcare systems},
abstract = {Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.}
}
@article{ZHANG2020483,
title = {Linking big data analytical intelligence to customer relationship management performance},
journal = {Industrial Marketing Management},
volume = {91},
pages = {483-494},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308762},
author = {Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han},
keywords = {Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance},
abstract = {This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.}
}
@article{HAUSLADEN2020101721,
title = {Towards a maturity model for big data analytics in airline network planning},
journal = {Journal of Air Transport Management},
volume = {82},
pages = {101721},
year = {2020},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2019.101721},
url = {https://www.sciencedirect.com/science/article/pii/S0969699718304988},
author = {Iris Hausladen and Maximilian Schosser},
keywords = {Maturity model, Network planning, Big data analytics, Airlines, Case study},
abstract = {The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{SILVA2020107828,
title = {Can big data explain yield variability and water productivity in intensive cropping systems?},
journal = {Field Crops Research},
volume = {255},
pages = {107828},
year = {2020},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2020.107828},
url = {https://www.sciencedirect.com/science/article/pii/S0378429019318039},
author = {João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma},
keywords = {Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands},
abstract = {Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.}
}
@article{LI2019991,
title = {Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?},
journal = {Applied Energy},
volume = {239},
pages = {991-1002},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919302922},
author = {Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan},
keywords = {Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data},
abstract = {Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.}
}
@article{GRIGG2022100619,
title = {DUG Insight: A software package for big-data analysis and visualisation, and its demonstration for passive radar space situational awareness using radio telescopes},
journal = {Astronomy and Computing},
volume = {40},
pages = {100619},
year = {2022},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2022.100619},
url = {https://www.sciencedirect.com/science/article/pii/S2213133722000452},
author = {D. Grigg and S.J. Tingay and M. Sokolowski and R.B. Wayth},
keywords = {Space situational awareness, , Radio astronomy, Interactive workflow creation, High performance computing},
abstract = {As the demand for software to support the processing and analysis of massive radio astronomy datasets increases in the era of the SKA, we demonstrate the interactive workflow building, data mining, processing, and visualisation capabilities of DUG Insight. We test the performance and flexibility of DUG Insight by processing almost 68,000 full sky radio images produced from the Engineering Development Array (EDA2) over the course of a three day period. The goal of the processing was to passively detect and identify known Resident Space Objects (RSOs: satellites and debris in orbit) and investigate how radio interferometry could be used to passively monitor aircraft traffic. These signals are observable due to both terrestrial FM radio signals reflected back to Earth and out-of-band transmission from RSOs. This surveillance of the low Earth orbit and airspace environment is useful as a contribution to space situational awareness and aircraft tracking technology. From the observations, we made 40 detections of 19 unique RSOs within a range of 1,500 km from the EDA2. This is a significant improvement on a previously published study of the same dataset and showcases the flexible features of DUG Insight that allow the processing of complex datasets at scale. Future enhancements of our DUG Insight workflow will aim to realise real-time acquisition, detect unknown RSOs, and continue to process data from SKA-relevant facilities.}
}
@article{MAJEED2021102026,
title = {A big data-driven framework for sustainable and smart additive manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {102026},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102026},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302374},
author = {Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin},
keywords = {Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization},
abstract = {From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.}
}
@article{LI202018,
title = {Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning},
journal = {Data and Information Management},
volume = {4},
number = {1},
pages = {18-43},
year = {2020},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2020-0003},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000572},
author = {Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox},
keywords = {information system education, computer science education, problem-based learning, natural language processing, NLP, big data text analytics, machine learning, deep learning},
abstract = {Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.}
}
@article{XIANG2020106538,
title = {Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence},
journal = {Computers & Industrial Engineering},
volume = {145},
pages = {106538},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106538},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220302722},
author = {Zehua Xiang and Minli Xu},
keywords = {Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence},
abstract = {In the “Internet+” era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP’s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a “win–win” situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer’s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP’s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP’s overconfidence and cost-sharing strategies may damage the supplier’s profit, the total profit of the CLSC increases.}
}
@article{CONNELLY20161,
title = {The role of administrative data in the big data revolution in social science research},
journal = {Social Science Research},
volume = {59},
pages = {1-12},
year = {2016},
note = {Special issue on Big Data in the Social Sciences},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2016.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X1630206X},
author = {Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben},
keywords = {Big data, Administrative data, Data management, Data quality, Data access},
abstract = {The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.}
}
@article{BARJAMARTINEZ2021111459,
title = {Artificial intelligence techniques for enabling Big Data services in distribution networks: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {150},
pages = {111459},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111459},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121007413},
author = {Sara Barja-Martinez and Mònica Aragüés-Peñalba and Íngrid Munné-Collado and Pau Lloret-Gallego and Eduard Bullich-Massagué and Roberto Villafafila-Robles},
keywords = {Machine learning, Deep learning, Smart grid, Distribution grid, Smart energy service},
abstract = {Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.}
}
@article{AMANULLAH2020495,
title = {Deep learning and big data technologies for IoT security},
journal = {Computer Communications},
volume = {151},
pages = {495-517},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419315361},
author = {Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran},
keywords = {Deep learning, Big data, IoT security},
abstract = {Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.}
}
@incollection{RISTEVSKI202185,
title = {4 - Healthcare and medical Big Data analytics},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {85-112},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000059},
author = {Blagoj Ristevski and Snezana Savoska},
keywords = {Big Data, medical and healthcare Big Data, Big Data Analytics, databases, healthcare information systems},
abstract = {In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient’s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.}
}
@article{MCCOY201774,
title = {Geospatial Big Data and archaeology: Prospects and problems too great to ignore},
journal = {Journal of Archaeological Science},
volume = {84},
pages = {74-94},
year = {2017},
note = {Archaeological GIS Today: Persistent Challenges, Pushing Old Boundaries, and Exploring New Horizons},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2017.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0305440317300821},
author = {Mark D. McCoy},
keywords = {Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science},
abstract = {As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.}
}
@article{MARIANI2020338,
title = {Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies},
journal = {Journal of Business Research},
volume = {121},
pages = {338-352},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320305956},
author = {Marcello M. Mariani and Samuel {Fosso Wamba}},
keywords = {Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data},
abstract = {The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.}
}
@article{CAISSIE2020e773,
title = {Radiotherapy (RT) Patterns Of Practice Variability Identified As A Challenge To Real-World Big Data: Recommendations From The Learning From Analysis Of Multicenter Big Data Aggregation (LAMBDA) Consortium},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {108},
number = {3, Supplement },
pages = {e773-e774},
year = {2020},
note = {Proceedings of the American Society for Radiation Oncology},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2020.07.224},
url = {https://www.sciencedirect.com/science/article/pii/S0360301620316436},
author = {A.L. Caissie and M.L. Mierzwa and C.D. Fuller and M. Rajaraman and A. Lin and A.M. McDonald and R.A. Popple and Y. Xiao and L. {van Dijk} and P. Balter and H. Fong and H. Ping and M. Kovoor and J. Lee and A. Rao and M.K. Martel and R.F. Thompson and B. Merz and J. Yao and C. Mayo}
}
@article{LIOUTAS2019100297,
title = {Key questions on the use of big data in farming: An activity theory approach},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100297},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1573521418302197},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}},
keywords = {Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory},
abstract = {Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.}
}
@article{KUMARI2018169,
title = {Multimedia big data computing and Internet of Things applications: A taxonomy and process model},
journal = {Journal of Network and Computer Applications},
volume = {124},
pages = {169-195},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303011},
author = {Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo},
keywords = {Multimedia big data, Data acquisition, Data representation, Data reduction, Data analysis, Data security and privacy, System intelligence, Distributed system, Social media},
abstract = {With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data – MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.}
}
@article{LI2019393,
title = {Functional Neuroimaging in the New Era of Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {393-401},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301603},
author = {Xiang Li and Ning Guo and Quanzheng Li},
keywords = {Big data, Neuroimaging, Machine learning, Health informatics, fMRI},
abstract = {The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.}
}
@article{YANG2022108322,
title = {Data quality assessment and analysis for pest identification in smart agriculture},
journal = {Computers and Electrical Engineering},
volume = {103},
pages = {108322},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108322},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622005444},
author = {Jiachen Yang and Guipeng Lan and Yang Li and Yicheng Gong and Zhuo Zhang and Sezai Ercisli},
keywords = {Smart agriculture, Pest identification, Information entropy, Data-centric},
abstract = {Deep learning has played a crucial role in the field of smart agriculture and been widely used in various applications. However, the deep learning models are constrained by data quality, which means poor data quality and unreliable data annotation will seriously restrict the performance of smart applications. In this paper, we proposed two methods to assess data quality, named Bound-DE and Multi-Branch. In experiments, the IP06 dataset and the ResNet-18 backbone network were adopted. The results show that the redundancy of the used public dataset is so large that about 50% of the data can achieve the similar test accuracy. Furthermore, we also analyzed the high contributive samples and summarized the rules of those selected informative samples, which is significant for the design of high-efficiency datasets. In summary, this study guides and promotes the following data-centric research in the field of smart agriculture.}
}
@article{ZHAO2020132,
title = {Privacy-preserving clustering for big data in cyber-physical-social systems: Survey and perspectives},
journal = {Information Sciences},
volume = {515},
pages = {132-155},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309764},
author = {Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang},
keywords = {CPSS, Big data, Cloud computing, Privacy preserving, Clustering},
abstract = {Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company’s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.}
}
@article{VIEIRA2020132,
title = {Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context},
journal = {Procedia Manufacturing},
volume = {42},
pages = {132-139},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305825},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Data issues, Industry 4.0},
abstract = {Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.}
}
@article{LUNDBERG2021100244,
title = {Editorial to the Special Issue on Big Data in Industrial and Commercial Applications},
journal = {Big Data Research},
volume = {26},
pages = {100244},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100244},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000617},
author = {Lars Lundberg and Håkan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab}
}
@article{LOPEZROBLES2019729,
title = {The last five years of Big Data Research in Economics, Econometrics and Finance: Identification and conceptual analysis},
journal = {Procedia Computer Science},
volume = {162},
pages = {729-736},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320551},
author = {José Ricardo López-Robles and Marisela Rodríguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jesús Cobo},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).}
}