@article{ZHANG2018146,
title = {A survey on deep learning for big data},
journal = {Information Fusion},
volume = {42},
pages = {146-157},
year = {2018},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517305328},
author = {Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li},
keywords = {Deep learning, Big data, Stacked auto-encoders, Deep belief networks, Convolutional neural networks, Recurrent neural networks},
abstract = {Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.}
}
@article{GUPTA201878,
title = {Big data with cognitive computing: A review for the future},
journal = {International Journal of Information Management},
volume = {42},
pages = {78-89},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218304110},
author = {Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter},
keywords = {Big data, Cognitive computing, Literature review, Resource based View, Institutional theory},
abstract = {Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V’s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.}
}
@article{ADDOTENKORANG2016528,
title = {Big data applications in operations/supply-chain management: A literature review},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {528-543},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S0360835216303631},
author = {Richard Addo-Tenkorang and Petri T. Helo},
keywords = {Big data – applications and analysis, Internet of Things (IoT), Cloud computing, Master database management, Operations/supply-chain management},
abstract = {Purpose
Big data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of “big data.” Therefore, this paper attempts to thoroughly investigate “big data,” its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of “big data” and its extension into “big data II”/IoT–value-adding perspectives by proposing a value-adding framework.
Methodology/research approach
The research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of “big data” applications between 2010 and 2015.
Findings/results
The four main attributes or factors identified with “big data” include – big data development sources (Variety – V1), big data acquisition (Velocity – V2), big data storage (Volume – V3), and finally big data analysis (Veracity – V4). However, the study of “big data” has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding – V5) – “Big Data cloud computing perspective/Internet of Things (IoT)”. Hence, the four Vs of “big data” is now expanded into five Vs.
Originality/value of research
This paper presents original literature review research discussing “big data” issues, trends and perspectives in operations/supply-chain management in order to propose “Big data II” (IoT – Value-adding) framework. This proposed framework is supposed or assumed to be an extension of “big data” in a value-adding perspective, thus proposing that “big data” be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.}
}
@article{SHAHA2016725,
title = {Big Data, Big Problems: Incorporating Mission, Values, and Culture in Provider Affiliations},
journal = {Orthopedic Clinics of North America},
volume = {47},
number = {4},
pages = {725-732},
year = {2016},
note = {Sports-Related Injuries},
issn = {0030-5898},
doi = {https://doi.org/10.1016/j.ocl.2016.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0030589816300554},
author = {Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh},
keywords = {Big data, Comparative effectiveness, Orthopedics, Total joint arthroplasty, Administrative database, Clinical database}
}
@article{COX2018111,
title = {Big data: Some statistical issues},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {111-115},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300609},
author = {D.R. Cox and Christiana Kartsonaki and Ruth H. Keogh},
keywords = {Big data, Electronic health records, Epidemiology, Metrology, Precision},
abstract = {A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.}
}
@article{SUN201827,
title = {Lossless Pruned Naive Bayes for Big Data Classifications},
journal = {Big Data Research},
volume = {14},
pages = {27-36},
year = {2018},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214579616301320},
author = {Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu},
keywords = {Big data, Classification, Naive Bayes, Large-scale taxonomy, Lossless, Pruned},
abstract = {In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.}
}
@article{SAIF2018118,
title = {Performance Analysis of Big Data and Cloud Computing Techniques: A Survey},
journal = {Procedia Computer Science},
volume = {132},
pages = {118-127},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918309062},
author = {Subia Saif and Samar Wazir},
keywords = {Big Data, Big Data Analytics (BDA), Cloud Computing, Cloud based Big Data Enterprise Solutions, Big Data Storage, Big Data Warehouse, Streaming Data, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, Microsoft Azure},
abstract = {A cloud framework refers to the aggregation of components like development tools, middleware and database services, needed for cloud computing, which aids in developing, deploying and managing cloud based applications strenuously, consequently making it an efficacious paradigm for massive scaling of dynamically allocated resources and their complex computing. Big Data Analytics (BDA) delivers data management solutions in the cloud architecture for storing, analysing and processing a huge volume of data. This paper presents a survey for performance based comparative analysis of cloud-based big data frameworks from leading enterprises like Amazon, Google, IBM, and Microsoft, which will assist researcher, IT analysts, reader and business user in picking the framework best suited for their work ensuring success in terms of favourable outcomes.}
}
@incollection{SHARMA2019189,
title = {Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey for the Medical Industry},
editor = {Valentina E. Balas and Le Hoang Son and Sudan Jha and Manju Khari and Raghvendra Kumar},
booktitle = {Internet of Things in Biomedical Engineering},
publisher = {Academic Press},
pages = {189-212},
year = {2019},
isbn = {978-0-12-817356-5},
doi = {https://doi.org/10.1016/B978-0-12-817356-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173565000103},
author = {Neha Sharma and Malini M. Patil and Madhavi Shamkuwar},
keywords = {Big data, Medical big data, Healthcare data, Medical big data analytics, Healthcare data analytics, Data analytics, Pharmacology data analytics},
abstract = {The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.}
}
@article{SAGGI2018758,
title = {A survey towards an integration of big data analytics to big insights for value-creation},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {758-790},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316307178},
author = {Mandeep Kaur Saggi and Sushma Jain},
keywords = {Big data, Data analytics, Machine learning, Big data visualization, Decision-making, Smart agriculture, Smart city application, Value-creation, Value-discover, Value-realization},
abstract = {Big Data Analytics (BDA) is increasingly becoming a trending practice that generates an enormous amount of data and provides a new opportunity that is helpful in relevant decision-making. The developments in Big Data Analytics provide a new paradigm and solutions for big data sources, storage, and advanced analytics. The BDA provide a nuanced view of big data development, and insights on how it can truly create value for firm and customer. This article presents a comprehensive, well-informed examination, and realistic analysis of deploying big data analytics successfully in companies. It provides an overview of the architecture of BDA including six components, namely: (i) data generation, (ii) data acquisition, (iii) data storage, (iv) advanced data analytics, (v) data visualization, and (vi) decision-making for value-creation. In this paper, seven V's characteristics of BDA namely Volume, Velocity, Variety, Valence, Veracity, Variability, and Value are explored. The various big data analytics tools, techniques and technologies have been described. Furthermore, it presents a methodical analysis for the usage of Big Data Analytics in various applications such as agriculture, healthcare, cyber security, and smart city. This paper also highlights the previous research, challenges, current status, and future directions of big data analytics for various application platforms. This overview highlights three issues, namely (i) concepts, characteristics and processing paradigms of Big Data Analytics; (ii) the state-of-the-art framework for decision-making in BDA for companies to insight value-creation; and (iii) the current challenges of Big Data Analytics as well as possible future directions.}
}
@article{DEFREITASVISCONDI201954,
title = {A Systematic Literature Review on big data for solar photovoltaic electricity generation forecasting},
journal = {Sustainable Energy Technologies and Assessments},
volume = {31},
pages = {54-63},
year = {2019},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S2213138818301036},
author = {Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza},
keywords = {Systematic Literature Review, Solar energy forecasting, Machine learning, Data mining},
abstract = {Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.}
}
@article{BIKAKIS2019100123,
title = {Big Data Exploration, Visualization and Analytics},
journal = {Big Data Research},
volume = {18},
pages = {100123},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2214579619302254},
author = {Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil}
}
@article{STOREY201750,
title = {Big data technologies and Management: What conceptual modeling can do},
journal = {Data & Knowledge Engineering},
volume = {108},
pages = {50-67},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300277},
author = {Veda C. Storey and Il-Yeol Song},
abstract = {The era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision-making and knowledge discovery activities. In this paper, the five Vs of big data, volume, velocity, variety, veracity, and value, are reviewed, as well as new technologies, including NoSQL databases that have emerged to accommodate the needs of big data initiatives. The role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data.}
}
@article{RIVERA20191,
title = {Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {24},
pages = {1-6},
year = {2019},
note = {5th IFAC Symposium on Telematics Applications TA 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.364},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319322645},
author = {Domingo Llorente Rivera and Markus R. Scholz and Christoph Bühl and Markus Krauss and Klaus Schilling},
keywords = {industrial analytics, anomaly detection, predictive maintenance, hydraulic pump, stochastic modeling},
abstract = {In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.}
}
@article{BALACHANDRAN20171112,
title = {Challenges and Benefits of Deploying Big Data Analytics in the Cloud for Business Intelligence},
journal = {Procedia Computer Science},
volume = {112},
pages = {1112-1122},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917314953},
author = {Bala M. Balachandran and Shivika Prasad},
keywords = {Cloud Computing, Big Data Analytics, Cloud Analytics, Security, Privacy, Business Intelligence, MapReduce, AaaS, CLaaS},
abstract = {Cloud computing and big data analytics are, without a doubt, two of the most important technologies to enter the mainstream IT industry in recent years. Surprisingly, the two technologies are coming together to deliver powerful results and benefits for businesses. Cloud computing is already changing the way IT services are provided by so called cloud companies and how businesses and users interact with IT resources. Big Data is a data analysis methodology enables by recent advances in information and communications technology. However, big data analysis requires a huge amount of computing resources making adoption costs of big data technology is not affordable for many small to medium enterprises. In this paper, we outline the the benefits and challenges involved in deploying big data analytics through cloud computing. We argue that cloud computing can support the storage and computing requirements of big data analytics. We discuss how the consolidation of these two dominant technologies can enhance the process of big data mining enabling businesses to improve decision-making processes. We also highlight the issues and risks that should be addressed when using a so called CLaaS, cloud-based service model.}
}
@article{YANG20141563,
title = {A spatiotemporal compression based approach for efficient big data processing on Cloud},
journal = {Journal of Computer and System Sciences},
volume = {80},
number = {8},
pages = {1563-1583},
year = {2014},
note = {Special Issue on Theory and Applications in Parallel and Distributed Computing Systems},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2014.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S002200001400066X},
author = {Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen},
keywords = {Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling},
abstract = {It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.}
}
@article{GUNTHER2017191,
title = {Debating big data: A literature review on realizing value from big data},
journal = {The Journal of Strategic Information Systems},
volume = {26},
number = {3},
pages = {191-209},
year = {2017},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717302615},
author = {Wendy Arianne Günther and Mohammad H. {Rezazade Mehrizi} and Marleen Huysman and Frans Feldberg},
keywords = {Big data, Analytics, Literature review, Value realization, Portability, Interconnectivity},
abstract = {Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.}
}
@incollection{QING2021181,
title = {Chapter 9 - Global Practice of AI and Big Data in Oil and Gas Industry},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {181-210},
year = {2021},
isbn = {978-0-12-820714-7},
doi = {https://doi.org/10.1016/B978-0-12-820714-7.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207147000091},
author = {Wu Qing},
keywords = {artificial intelligence, big data, digital core, digital rock physics, multiphase flow physics information, oil recovery, molecule advanced planning and scheduling, system (MAPS), crude oil selection, crude oil property prediction, process optimization, CCR unit, FCC unit, ethylene cracking unit, correlation analysis, the anomaly detection, parameters optimization analysis, prediction analysis, equipment preventive maintenance, distributed equipment health monitoring system, time series, early warning system for equipment fault monitoring, residual life prediction for equipment},
abstract = {This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics—DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.}
}
@article{TIWARI2018319,
title = {Big data analytics in supply chain management between 2010 and 2016: Insights to industries},
journal = {Computers & Industrial Engineering},
volume = {115},
pages = {319-330},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217305508},
author = {Sunil Tiwari and H.M. Wee and Yosef Daryanto},
keywords = {Big data analytics, Supply chain management, Big data application},
abstract = {This paper investigates big data analytics research and application in supply chain management between 2010 and 2016 and provides insights to industries. In recent years, the amount of data produced from end-to-end supply chain management practices has increased exponentially. Moreover, in current competitive environment supply chain professionals are struggling in handling the huge data. They are surveying new techniques to investigate how data are produced, captured, organized and analyzed to give valuable insights to industries. Big Data analytics is one of the best techniques which can help them in overcoming their problem. Realizing the promising benefits of big data analytics in the supply chain has motivated us to write a review on the importance/impact of big data analytics and its application in supply chain management. First, we discuss big data analytics individually, and then we discuss the role of big data analytics in supply chain management (supply chain analytics). Current research and application are also explored. Finally, we outline the insights to industries. Observations and insights from this paper could provide the guideline for academia and practitioners in implementing big data analytics in different aspects of supply chain management.}
}
@article{WANG20183,
title = {Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations},
journal = {Technological Forecasting and Social Change},
volume = {126},
pages = {3-13},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516000500},
author = {Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd},
keywords = {Big data analytics, Big data analytics architecture, Big data analytics capabilities, Business value of information technology (IT), Health care},
abstract = {To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.}
}
@article{COBB2018640,
title = {Big data: More than big data sets},
journal = {Surgery},
volume = {164},
number = {4},
pages = {640-642},
year = {2018},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2018.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0039606018303660},
author = {Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo},
abstract = {The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.}
}
@article{ELRAGAL2014242,
title = {ERP and Big Data: The Inept Couple},
journal = {Procedia Technology},
volume = {16},
pages = {242-249},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.089},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314003168},
author = {Ahmed Elragal},
keywords = {ERP, Big Data, Research Agenda},
abstract = {The world is witnessing an unprecedented interest in big data. Big data is data that is big in size (volume), big in variety (structured; semi-structured; unstructured), and big in speed of change (velocity). It was reported that almost 90% of the data worldwide was just created in the past 2 years. Therefore, this paper is an attempt to align ERP systems with big data. The objective is to suggest a future research agenda to bring together big data and ERP. While almost everyone is talking about big data at the product or tool level, relationship with social media, relationship with Internet of things, etc. no one has tried to integrate big data and ERP. A research agenda is discussed and introduced in this paper.}
}
@article{BONNEL20181,
title = {Transport survey methods - in the era of big data facing new and old challenges},
journal = {Transportation Research Procedia},
volume = {32},
pages = {1-15},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301522},
author = {Patrick Bonnel and Marcela A. Munizaga},
keywords = {ISCTSC, transport, survey methodology, big data},
abstract = {This document presents an introduction to the ISCTSC Special Issue of Transport Research Procedia. It synthesizes the discussions held at the 11th International Conference on Transport Survey Methods, and describes the contents of the selected contributions. This conference has been held in different countries from all over the world, involving an increasing group of enthusiastic and generous specialists, willing to share their knowledge. This 11th conference was an opportunity to discuss the state of the art on transport survey methods, but also to question the way transport surveys are conducted in the era of big data. We took the opportunity to identify the main challenges, and the most important questions.}
}
@article{XIA2018191,
title = {Using spatiotemporal patterns to optimize Earth Observation Big Data access: Novel approaches of indexing, service modeling and cloud computing},
journal = {Computers, Environment and Urban Systems},
volume = {72},
pages = {191-203},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300401},
author = {Jizhe Xia and Chaowei Yang and Qingquan Li},
keywords = {Spatiotemporal optimization, Big Data, Cloud computing, Global operation, GEOSS},
abstract = {Based on our GEOSS Clearinghouse operating experience, we summarized the three Earth Observation (EO) Big Data access challenges, namely, fast access, accurate service estimation and global access, and two essential research questions: are there any spatiotemporal patterns when end users access EO data, and how can these spatiotemporal patterns be utilized to better facilitate EO Big Data access? To tackle these two research questions, we conducted a two-year pattern analysis with 2+ million user access records. The spatial pattern, temporal pattern and spatiotemporal pattern of user-data interactions were explored. For the second research question, we developed three spatiotemporal optimization strategies to respond to the three access challenges: a) spatiotemporal indexing to accelerate data access, b) spatiotemporal service modeling to improve data access accuracy and c) spatiotemporal cloud computing to enhance global access. This research is a pioneering framework for spatiotemporal optimization of EO Big Data access and valuable for other multidisciplinary geographic data and information research.}
}
@article{SIDDIQA2016151,
title = {A survey of big data management: Taxonomy and state-of-the-art},
journal = {Journal of Network and Computer Applications},
volume = {71},
pages = {151-166},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516300583},
author = {Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin},
keywords = {Big data management, Storage, Big data, Processing, Security},
abstract = {The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.}
}
@incollection{SHARMA202137,
title = {Chapter 2 - Deep learning in big data and data mining},
editor = {Vincenzo Piuri and Sandeep Raj and Angelo Genovese and Rajshree Srivastava},
booktitle = {Trends in Deep Learning Methodologies},
publisher = {Academic Press},
pages = {37-61},
year = {2021},
series = {Hybrid Computational Intelligence for Pattern Analysis},
isbn = {978-0-12-822226-3},
doi = {https://doi.org/10.1016/B978-0-12-822226-3.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222263000027},
author = {Deepak Kumar Sharma and Bhanu Tokas and Leo Adlakha},
keywords = {Aspect extraction, Big data, CRISP-DM, Customer relations, Data mining, Data visualization, Deep learning, Distributed computing, LSTM, Machine learning},
abstract = {The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.}
}
@article{REY201837,
title = {Causes of deaths data, linkages and big data perspectives},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {37-40},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X16301652},
author = {Grégoire Rey and Karim Bounebache and Claire Rondet},
keywords = {Causes of death data, Data linkages, Big data},
abstract = {The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.}
}
@article{SHIN2016837,
title = {Demystifying big data: Anatomy of big data developmental process},
journal = {Telecommunications Policy},
volume = {40},
number = {9},
pages = {837-854},
year = {2016},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0308596115000567},
author = {Dong-Hee Shin},
keywords = {Big data, Data ecosystem, Normalization, Normalization process theory, Big data user, Big data user experience},
abstract = {This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users׳ adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.}
}
@article{AHMAD2016439,
title = {An efficient divide-and-conquer approach for big data analytics in machine-to-machine communication},
journal = {Neurocomputing},
volume = {174},
pages = {439-453},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.04.109},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215012369},
author = {Awais Ahmad and Anand Paul and M. Mazhar Rathore},
keywords = {M2M, Big Data, Divide-and-conquer, Data fusion domain},
abstract = {Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core™i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.}
}
@article{SILVA2020893,
title = {Identification of Patterns of Fatal Injuries in Humans through Big Data},
journal = {Procedia Computer Science},
volume = {170},
pages = {893-898},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305524},
author = {Jesus Silva and Jack Zilberman and Ligia Romero and Omar Bonerge Pineda and Yaneth Herazo-Beltran},
keywords = {Recognition of automated standards, mining, decision trees},
abstract = {External cause injuries are defined as intentionally or unintentionally harm or injury to a person, which may be caused by trauma, poisoning, assault, accidents, etc., being fatal (fatal injury) or not leading to death (non-fatal injury). External injuries have been considered a global health problem for two decades. This work aims to determine criminal patterns using data mining techniques to a sample of patients from Mumbai city in India.}
}
@incollection{HAYMOND202137,
title = {Chapter 3 - Machine learning and big data in pediatric laboratory medicine},
editor = {Dennis Dietzen and Michael Bennett and Edward Wong and Shannon Haymond},
booktitle = {Biochemical and Molecular Basis of Pediatric Disease (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
pages = {37-70},
year = {2021},
isbn = {978-0-12-817962-8},
doi = {https://doi.org/10.1016/B978-0-12-817962-8.00018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179628000184},
author = {Shannon Haymond and Randall K. Julian and Emily L. Gill and Stephen R. Master},
keywords = {Artificial intelligence, Big data, Laboratory medicine, Machine learning, Pediatrics, Regulation},
abstract = {Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.}
}
@article{ZARRINPAR2020599,
title = {What Can We Learn About Drug Safety and Other Effects in the Era of Electronic Health Records and Big Data That We Would Not Be Able to Learn From Classic Epidemiology?},
journal = {Journal of Surgical Research},
volume = {246},
pages = {599-604},
year = {2020},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0022480419306985},
author = {Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo},
keywords = {Electronic health record, Big data, Drug safety, Health care database, Cancer risk},
abstract = {As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.}
}
@article{SANTOS2017750,
title = {A Big Data system supporting Bosch Braga Industry 4.0 strategy},
journal = {International Journal of Information Management},
volume = {37},
number = {6},
pages = {750-760},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217306023},
author = {Maribel Yasmina Santos and Jorge {Oliveira e Sá} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and João Galvão},
keywords = {Big Data, Industry 4.0, Big Data analytics, Big Data architecture, Bosch},
abstract = {People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia – Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).}
}
@article{KARIM2020942,
title = {Big data management in participatory sensing: Issues, trends and future directions},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {942-955},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311627},
author = {Ahmad Karim and Aisha Siddiqa and Zanab Safdar and Maham Razzaq and Syeda Anum Gillani and Huma Tahir and Sana Kiran and Ejaz Ahmed and Muhammad Imran},
keywords = {Participatory sensing, Big data, Big data management, Big data analytics, Mobile cloud computing},
abstract = {Participatory sensing has become an emerging technology of this era owing to its low cost in big sensor data collection. Prior to participatory sensing, large-scale deployment complexities were found in wireless sensor networks when collecting data from widespread resources. Participatory sensing systems employ handheld devices as sensors to collect data from communities and transmit to the cloud, where data are further analyzed by expert systems. The processes involved in participatory sensing, such as data collection, transmission, analysis, and visualization, exhibit certain management issues. This study aims to identify big data management issues that must be addressed at the cloud side during data processing and storing and at the participant side during data collection and visualization. It then proposes a framework for big data management in participatory sensing to resolve the contemporary big data management issues on the basis of suggested principles. Moreover, this work presents case studies to elaborate the existence of the highlighted issues. Finally, the limitations, recommendations, and future research directions for academia and industry in the domain of participatory sensing are discussed.}
}
@article{LI2018301,
title = {Big data in tourism research: A literature review},
journal = {Tourism Management},
volume = {68},
pages = {301-323},
year = {2018},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0261517718300591},
author = {Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li},
keywords = {Tourism research, Big data, Literature review, Tourism management, Tourist behavior},
abstract = {Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.}
}
@article{SANCHEZMARTINEZ2016236,
title = {Workshop 5 report: Harnessing big data},
journal = {Research in Transportation Economics},
volume = {59},
pages = {236-241},
year = {2016},
note = {Competition and Ownership in Land Passenger Transport (selected papers from the Thredbo 14 conference)},
issn = {0739-8859},
doi = {https://doi.org/10.1016/j.retrec.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0739885916301494},
author = {Gabriel E. Sánchez-Martínez and Marcela Munizaga},
keywords = {Big data, Measurement, Implementation challenges, Analysis tools, Transit best practices},
abstract = {A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.}
}
@article{VANDENBROEK2018330,
title = {Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation},
journal = {Technological Forecasting and Social Change},
volume = {129},
pages = {330-338},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314695},
author = {Tijs {van den Broek} and Anne Fleur {van Veenstra}},
keywords = {Disruptive innovation, Data protection regulation, Big data, Governance, Inter-organizational collaboration},
abstract = {Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.}
}
@article{PALANISAMY2019415,
title = {Implications of big data analytics in developing healthcare frameworks – A review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {31},
number = {4},
pages = {415-425},
year = {2019},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817302938},
author = {Venketesh Palanisamy and Ramkumar Thirunavukarasu},
keywords = {Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns, Tools},
abstract = {The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.}
}
@article{LIANG2019290,
title = {A survey on big data-driven digital phenotyping of mental health},
journal = {Information Fusion},
volume = {52},
pages = {290-307},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253518305244},
author = {Yunji Liang and Xiaolong Zheng and Daniel D. Zeng},
keywords = {Digital phenotyping, Big data, Mental health, Data mining, Information fusion},
abstract = {The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.}
}
@article{ARUNACHALAM2018416,
title = {Understanding big data analytics capabilities in supply chain management: Unravelling the issues, challenges and implications for practice},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {416-436},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1366554516303799},
author = {Deepak Arunachalam and Niraj Kumar and John Paul Kawalek},
keywords = {Supply chain management, Big data analytics, Capabilities, Maturity model},
abstract = {In the era of Big Data, many organisations have successfully leveraged Big Data Analytics (BDA) capabilities to improve their performance. However, past literature on BDA have put limited focus on understanding the capabilities required to extract value from big data. In this context, this paper aims to provide a systematic literature review of BDA capabilities in supply chain and develop the capabilities maturity model. The paper presents the bibliometric and thematic analysis of research papers from 2008 to 2016. This paper contributes in theorizing BDA capabilities in context of supply chain, and provides future direction of research in this field.}
}
@article{HUANCHUN2020102024,
title = {Analyzing the Influencing Factors of Urban Thermal Field Intensity Using Big-Data-Based GIS},
journal = {Sustainable Cities and Society},
volume = {55},
pages = {102024},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102024},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720300111},
author = {Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng},
keywords = {land-surface temperature, thermal field pattern, POI data, GIS, air temperature},
abstract = {The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.}
}
@article{CUQUET201874,
title = {The societal impact of big data: A research roadmap for Europe},
journal = {Technology in Society},
volume = {54},
pages = {74-86},
year = {2018},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X17300131},
author = {Martí Cuquet and Anna Fensel},
keywords = {Big data, Research roadmap, Societal externalities, Skills development, Standardisation},
abstract = {With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.}
}
@article{YAQOOB20161231,
title = {Big data: From beginning to future},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part B},
pages = {1231-1247},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216304753},
author = {Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos},
keywords = {Big data, Parallel and distributed computing, Cloud computing, Internet of things, Social media, Analytics},
abstract = {Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.}
}
@incollection{MISHRA20201,
title = {Chapter 1 - Analysis of the role and scope of big data analytics with IoT in health care domain},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar and Manju Khari},
booktitle = {Handbook of Data Science Approaches for Biomedical Engineering},
publisher = {Academic Press},
pages = {1-23},
year = {2020},
isbn = {978-0-12-818318-2},
doi = {https://doi.org/10.1016/B978-0-12-818318-2.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183182000015},
author = {Sushruta Mishra and Brojo Kishore Mishra and Hrudaya Kumar Tripathy and Arijit Dutta},
keywords = {Big data analytics, Cloud, Health care domain, Internet of Things (IoT), Sensors},
abstract = {Data analytics play an active role in medical applications to extract relevant information from heaps of data samples. Internet of things (IoT) technology has slowly captured the market and is entering the health care sector too. With the help of big data analytics, various IoT-based devices can auto-monitor the health conditions of patients and can send the status to concerned physicians and family members. Thus, the integration of big data analytics with IoT technology forms a favorable combination in the health care domain. In this chapter, we discuss the two latest trends that include big data analytics and IoT with respect to its relevance in medical fields. We also analyze a health care monitoring system, which is an IoT-based model integrated with big data analytics. The system integrates patient specific information over the cloud. In the more developed model, the implementation was made to monitor the health status of the patients. The developed model was found to be faster and thus it can be easily implemented into a real time patient health monitoring and status management system. Medical experts can take advantage of this system model, thereby providing appropriate information to appropriate patient and doctors at appropriate time.}
}
@article{NIMMAGADDA2018143,
title = {On big data-guided upstream business research and its knowledge management},
journal = {Journal of Business Research},
volume = {89},
pages = {143-158},
year = {2018},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318302054},
author = {Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood},
keywords = {Upstream business, Heterogeneous and multidimensional data, Data warehousing and mining, Big Data paradigm, Spatial-temporal dimensions},
abstract = {The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.}
}
@article{RIGGINS201723,
title = {Data governance case at KrauseMcMahon LLP in an era of self-service BI and Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {23-36},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301208},
author = {Frederick J. Riggins and Bonnie K. Klamm},
keywords = {Big Data, Data governance, Self-service business intelligence},
abstract = {This case increases your understanding of data governance in an era of sophisticated analytics and Big Data where corporate data integrity and data quality may be at risk. KrauseMcMahon, a large certified public accounting and business consulting firm, faces a tradeoff of increasing control of the company’s data assets versus unleashing end user innovation due to the proliferation of self-service business intelligence tools. You are required to analyze the issues in the case from organizational, financial, and technical perspectives to propose alternatives the organization should consider and make specific recommendations on how the company should proceed. By completing this case, you will demonstrate cross-disciplinary abilities related to foundational business, accounting, and broad management competencies. By addressing such competencies, the case requires your use of accounting, MIS, and upper-level business skills.}
}
@article{MCKINNEY201763,
title = {The need for ‘skeptical’ accountants in the era of Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {63-80},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301051},
author = {Earl McKinney and Charles J. Yoos and Ken Snead},
keywords = {Big Data, Analysis, Informed skepticism, Questioning},
abstract = {Big Data is now readily available for analysis, and analysts trained to conduct effective analysis in this area are in high demand. However, there is a dearth of discussion in the literature related to identifying the important cognitive skills required for accountants to conduct effective Big Data analysis. Here we argue that accountants need to approach Big Data analysis as informed skeptics, being ever ready to challenge the analysis by asking good questions in appropriate topical areas. These areas include understanding the limits of measurement and representation, the subjectiveness of insight, the challenges of statistics and integrating data sets, and the effects of underdetermination and inductive reasoning. Accordingly, we develop a framework and an illustrative example to facilitate the training of accounting students to become informed skeptics in the era of Big Data by explaining the conceptual relevance of each of the topical areas to Big Data analysis. In addition, example questions are identified that accountants conducting Big Data analysis should be asking regarding each topic. Further, for each topic, references to additional resources are provided that students can access to learn more about effectively conducting Big Data analysis.}
}
@article{WANG2017287,
title = {Exploring the path to big data analytics success in healthcare},
journal = {Journal of Business Research},
volume = {70},
pages = {287-299},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304891},
author = {Yichuan Wang and Nick Hajli},
keywords = {Big data analytics, Business value, Capability building view, Resource-based theory, Information technology source management, Health care industries},
abstract = {Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.}
}
@article{BROEDERS2017309,
title = {Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {309-323},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300675},
author = {Dennis Broeders and Erik Schrijvers and Bart {van der Sloot} and Rosamunde {van Brakel} and Josta {de Hoog} and Ernst {Hirsch Ballin}},
keywords = {Big Data, Security, Data protection, Privacy, Regulation, Fraud, Policing, Surveillance, Algorithmic accountability, the Netherlands},
abstract = {Big Data analytics in national security, law enforcement and the fight against fraud have the potential to reap great benefits for states, citizens and society but require extra safeguards to protect citizens' fundamental rights. This involves a crucial shift in emphasis from regulating Big Data collection to regulating the phases of analysis and use. In order to benefit from the use of Big Data analytics in the field of security, a framework has to be developed that adds new layers of protection for fundamental rights and safeguards against erroneous and malicious use. Additional regulation is needed at the levels of analysis and use, and the oversight regime is in need of strengthening. At the level of analysis – the algorithmic heart of Big Data processes – a duty of care should be introduced that is part of an internal audit and external review procedure. Big Data projects should also be subject to a sunset clause. At the level of use, profiles and (semi-) automated decision-making should be regulated more tightly. Moreover, the responsibility of the data processing party for accuracy of analysis – and decisions taken on its basis – should be anchored in legislation. The general and security-specific oversight functions should be strengthened in terms of technological expertise, access and resources. The possibilities for judicial review should be expanded to stimulate the development of case law.}
}
@article{SMIRNOVA201825,
title = {A practical guide to big data},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {25-29},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300592},
author = {Ekaterina Smirnova and Andrada Ivanescu and Jiawei Bai and Ciprian M. Crainiceanu},
keywords = {Big data, Wearable and implantable computing, Accelerometer},
abstract = {Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.}
}
@article{WESTRA2016286,
title = {Big Data and Perioperative Nursing},
journal = {AORN Journal},
volume = {104},
number = {4},
pages = {286-292},
year = {2016},
note = {Special Focus Issue: Technology},
issn = {0001-2092},
doi = {https://doi.org/10.1016/j.aorn.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0001209216304410},
author = {Bonnie L. Westra and Jessica J. Peterson},
keywords = {big data, perioperative nursing, quality care, nursing knowledge, nursing informatics},
abstract = {Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.}
}
@article{LAREYRE2020e575,
title = {Artificial Intelligence in Vascular Surgery: Moving from Big Data to Smart Data},
journal = {Annals of Vascular Surgery},
volume = {67},
pages = {e575-e576},
year = {2020},
issn = {0890-5096},
doi = {https://doi.org/10.1016/j.avsg.2020.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0890509620303411},
author = {Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort}
}
@article{GHANI2019417,
title = {Social media big data analytics: A survey},
journal = {Computers in Human Behavior},
volume = {101},
pages = {417-428},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.08.039},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830414X},
author = {Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed},
keywords = {Big data, Social media, Machine learning, Analytics},
abstract = {Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.}
}
@article{MOGHADAM2018401,
title = {Information security governance in big data environments: A systematic mapping},
journal = {Procedia Computer Science},
volume = {138},
pages = {401-408},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316909},
author = {Reza Saneei Moghadam and Ricardo Colomo-Palacios},
keywords = {information security governance, big data, framework, systematic mapping},
abstract = {Information security governance is an important aspects for all organizations. Given the crucial importance of IT systems and the increasing range of threats these systems are facing, there is an increasing interest on the topic. On the other hand, Big Data environments are also beginning to be more pervasive as IT is increasing its importance for organizations worldwide. In order to better know which aspects are the most important for the intersection of Big Data and information security governance, authors present in this paper a systematic mapping on this topic. Authors illustrate challenges and gaps concerning the topic and clarify these challenges by means of a classification of the environments they take place, the security risk spectrums they concern, and the security governance measures they take to mitigate them; by providing solutions as in a framework, model, software or tool, wherever possible. Results are expected to be useful for IT security professionals and information systems practitioners as a whole.}
}
@article{ELOUAZZANI201852,
title = {A new technique ensuring privacy in big data: K-anonymity without prior value of the threshold k},
journal = {Procedia Computer Science},
volume = {127},
pages = {52-59},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830108X},
author = {Zakariae {El Ouazzani} and Hanan {El Bakkali}},
keywords = {k-anonymity, quasi identifier attributes, big data, anonymization, privacy},
abstract = {Big data has become omnipresent and crucial for many application domains. Big data makes reference to the explosive quantity of data generated in today’s society that might contain personally identifiable information (PII). That’s why the challenge from the point of view of data privacy is one of the major hurdles for the application of big data. In that situation, several techniques were exposed in order to ensure privacy in big data including generalization, randomization and cryptographic techniques as well. It is well known that there exist two main types of attributes in the literature, quasi identifier and sensitive attributes. In this paper, we are going to focus on quasi identifier attributes. Over the years, k-anonymity has been treated with great interest as an anonymization technique ensuring privacy in big data when we are dealing with quasi identifier attributes. Despite the fact that many algorithms of k-anonymity have been proposed, most of them admit that the threshold k of k-anonymity has to be known before anonymizing the data set. Here, a novel way in applying k-anonymity for quasi identifier attributes is presented. It’s a new algorithm called “k-anonymity without prior value of the threshold k”. Our proposed algorithm was experimentally evaluated using a test table of quasi identifier attributes. Furthermore, we highlight all the steps of our proposed algorithm with detailed comments.}
}
@article{BEN2019403,
title = {A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources},
journal = {Environmental Pollution},
volume = {253},
pages = {403-411},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930795X},
author = {YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong},
keywords = {Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai},
abstract = {An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.}
}
@article{YASSINE2019563,
title = {IoT big data analytics for smart homes with fog and cloud computing},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {563-573},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18311099},
author = {Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad},
keywords = {Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics, Energy management, Smart homes},
abstract = {Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.}
}
@incollection{HULSEN202169,
title = {Chapter 4 - Challenges and solutions for big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {69-94},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000167},
author = {Tim Hulsen},
keywords = {Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics},
abstract = {“Big data” is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to “long data,” “wide data,” and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.}
}
@article{LEE2017293,
title = {Big data: Dimensions, evolution, impacts, and challenges},
journal = {Business Horizons},
volume = {60},
number = {3},
pages = {293-303},
year = {2017},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317300046},
author = {In Lee},
keywords = {Big data, Internet of things, Data analytics, Sentiment analysis, Social network analysis, Web analytics},
abstract = {Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.}
}
@article{EREVELLES2016897,
title = {Big Data consumer analytics and the transformation of marketing},
journal = {Journal of Business Research},
volume = {69},
number = {2},
pages = {897-904},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315002842},
author = {Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne},
keywords = {Big Data, Consumer analytics, Consumer insights, Resource-based theory, Induction, Ignorance},
abstract = {Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources—physical, human, and organizational capital—moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.}
}
@article{WANG201864,
title = {An integrated big data analytics-enabled transformation model: Application to health care},
journal = {Information & Management},
volume = {55},
number = {1},
pages = {64-79},
year = {2018},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617303129},
author = {Yichuan Wang and LeeAnn Kung and William Yu Chung Wang and Casey G. Cegielski},
keywords = {Big data analytics, IT-enabled transformation, Practice-based view, Business value of IT, Healthcare, Content analysis},
abstract = {A big data analytics-enabled transformation model based on practice-based view is developed, which reveals the causal relationships among big data analytics capabilities, IT-enabled transformation practices, benefit dimensions, and business values. This model was then tested in healthcare setting. By analyzing big data implementation cases, we sought to understand how big data analytics capabilities transform organizational practices, thereby generating potential benefits. In addition to conceptually defining four big data analytics capabilities, the model offers a strategic view of big data analytics. Three significant path-to-value chains were identified for healthcare organizations by applying the model, which provides practical insights for managers.}
}
@article{NADAL201775,
title = {A software reference architecture for semantic-aware Big Data systems},
journal = {Information and Software Technology},
volume = {90},
pages = {75-92},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304287},
author = {Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio},
keywords = {Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis},
abstract = {Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.}
}
@article{AHMED2017459,
title = {The role of big data analytics in Internet of Things},
journal = {Computer Networks},
volume = {129},
pages = {459-471},
year = {2017},
note = {Special Issue on 5G Wireless Networks for IoT and Body Sensors},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617302591},
author = {Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos},
keywords = {Internet of Things, Big data, Analytics, Distributed computing, Smart city},
abstract = {The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.}
}
@article{KHAN2017923,
title = {A survey on scholarly data: From big data perspective},
journal = {Information Processing & Management},
volume = {53},
number = {4},
pages = {923-944},
year = {2017},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316305994},
author = {Samiya Khan and Xiufeng Liu and Kashish A. Shakil and Mansaf Alam},
keywords = {Scholarly data, Big data, Cloud-based big data analytics, Big scholarly data, Big data platform, Scholarly applications},
abstract = {Recently, there has been a shifting focus of organizations and governments towards digitization of academic and technical documents, adding a new facet to the concept of digital libraries. The volume, variety and velocity of this generated data, satisfies the big data definition, as a result of which, this scholarly reserve is popularly referred to as big scholarly data. In order to facilitate data analytics for big scholarly data, architectures and services for the same need to be developed. The evolving nature of research problems has made them essentially interdisciplinary. As a result, there is a growing demand for scholarly applications like collaborator discovery, expert finding and research recommendation systems, in addition to several others. This research paper investigates the current trends and identifies the existing challenges in development of a big scholarly data platform, with specific focus on directions for future research and maps them to the different phases of the big data lifecycle.}
}
@incollection{MCGILVRAY20217,
title = {Chapter 1 - Data Quality and the Data-Dependent World},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {7-14},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000219},
author = {Danette McGilvray},
keywords = {Data-dependent world, data-driven, assets, technology, legal and regulatory tsunami, Internet of Things (IoT), big data, artificial intelligence (AI), machine learning (ML), The Leader’s Data Manifesto, data literacy, change, COVID-19},
abstract = {Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader’s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.}
}
@article{SHOUMY2020102447,
title = {Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals},
journal = {Journal of Network and Computer Applications},
volume = {149},
pages = {102447},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102447},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303078},
author = {Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia},
keywords = {Affective computing, Multimodal fusion, Sentiment databases, Sentiment analysis, Affective applications},
abstract = {Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.}
}
@article{PERAKIS2020107035,
title = {CYBELE – Fostering precision agriculture & livestock farming through secure access to large-scale HPC enabled virtual industrial experimentation environments fostering scalable big data analytics},
journal = {Computer Networks},
volume = {168},
pages = {107035},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.107035},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619305353},
author = {Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne},
keywords = {Precision agriculture, Precision livestock farming, High performance computing, Big data analytics},
abstract = {According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.}
}
@article{LI2016119,
title = {Geospatial big data handling theory and methods: A review and research challenges},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {119-133},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002439},
author = {Songnian Li and Suzana Dragicevic and Francesc Antón Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng},
keywords = {Big data, Geospatial, Data handling, Analytics, Spatial modeling, Review},
abstract = {Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.}
}
@article{SYED2019136,
title = {Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {136-151},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18321071},
author = {Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi},
keywords = {Ambient Assisted Living (AAL), Big data analytics, Internet of Medical Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors},
abstract = {In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.}
}
@incollection{KABALCI2019309,
title = {Chapter 8 - Big data, privacy and security in smart grids},
editor = {Ersan Kabalci and Yasin Kabalci},
booktitle = {From Smart Grid to Internet of Energy},
publisher = {Academic Press},
pages = {309-333},
year = {2019},
isbn = {978-0-12-819710-3},
doi = {https://doi.org/10.1016/B978-0-12-819710-3.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197103000089},
author = {Ersan Kabalci and Yasin Kabalci},
keywords = {Internet of things (IoT), Machine-to-machine communication (M2M), Human to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining, Security, Privacy},
abstract = {The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.}
}
@article{KOSELEVA2017544,
title = {Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges},
journal = {Procedia Engineering},
volume = {172},
pages = {544-549},
year = {2017},
note = {Modern Building Materials, Structures and Techniques},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.02.064},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817305702},
author = {Natalija Koseleva and Guoda Ropaite},
keywords = {Energy Efficiency, Big Data, Characteristics of Big Data, Big Data analysis, Construction, Web of Science},
abstract = {Data generation has increased drastically over the past few years. Data management has also grown in importance because extracting the significant value out of a huge pile of raw data is of prime important thing to make different decisions. One of the important sectors nowadays is construction sector, especially building energy efficiency field. Collecting big amount of data, using different kinds of big data analysis can help to improve construction process from the energy efficiency perspective. This article reviews the understanding of Big Data, methods used for Big Data analysis and the main problems with Big Data in the field of energy.}
}
@article{ALWAN2022101951,
title = {Data quality challenges in large-scale cyber-physical systems: A systematic review},
journal = {Information Systems},
volume = {105},
pages = {101951},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101951},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001484},
author = {Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin},
keywords = {Cyber-physical systems (CPS), Wireless Sensor Networks(WSN), Data quality management, Data quality dimensions, Smart cities, Quality of observations},
abstract = {Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.}
}
@article{KAMBATLA20142561,
title = {Trends in big data analytics},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2561-2573},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514000057},
author = {Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama},
keywords = {Big-data, Analytics, Data centers, Distributed systems},
abstract = {One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications’ considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.}
}
@article{ANDREOLINI201567,
title = {Adaptive, scalable and reliable monitoring of big data on clouds},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {67-79},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S074373151400149X},
author = {Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi},
keywords = {Adaptivity, Monitoring, Cloud computing, Big data, Scalability},
abstract = {Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.}
}
@article{SHARPLES2018105,
title = {The role of statistics in the era of big data: Electronic health records for healthcare research},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {105-110},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300890},
author = {Linda D. Sharples},
keywords = {Electronic healthcare records},
abstract = {The transferring of medical records into huge electronic databases has opened up opportunities for research but requires attention to data quality, study design and issues of bias and confounding.}
}
@article{WORDSWORTH20181048,
title = {Using “Big Data” in the Cost-Effectiveness Analysis of Next-Generation Sequencing Technologies: Challenges and Potential Solutions},
journal = {Value in Health},
volume = {21},
number = {9},
pages = {1048-1053},
year = {2018},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2018.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1098301518322654},
author = {Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier},
keywords = {Big data, cost-effectiveness, next generation sequencing},
abstract = {Next-generation sequencing (NGS) is considered to be a prominent example of “big data” because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.}
}
@article{VERMA2018791,
title = {An extension of the technology acceptance model in the big data analytics system implementation environment},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {791-806},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300043},
author = {Surabhi Verma and Som Sekhar Bhattacharyya and Saurav Kumar},
keywords = {Technology acceptance model, Big data analytics system, System quality, Information quality},
abstract = {Research on the adoption of systems for big data analytics has drawn enormous attention in Information Systems research. This study extends big data analytics adoption research by examining the effects of system characteristics on the attitude of managers towards the usage of big data analytics systems. A research model has been proposed in this study based on an extensive review of literature pertaining to the Technology Acceptance Model, with further validation by a survey of 150 big data analytics users. Results of this survey confirm that characteristics of the big data analytics system have significant direct and indirect effects on belief in the benefits of big data analytics systems and perceived usefulness, attitude and adoption. Moreover, there are mediation effects that exist among the system characteristics, benefits of big data analytics systems, perceived usefulness and the attitude towards using big data analytics system. This study expands the existing body of knowledge on the adoption of big data analytics systems, and benefits big data analytics providers and vendors while helping in the formulation of their business models.}
}
@article{KAUR20181049,
title = {Big Data and Machine Learning Based Secure Healthcare Framework},
journal = {Procedia Computer Science},
volume = {132},
pages = {1049-1059},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830752X},
author = {Prableen Kaur and Manik Sharma and Mamta Mittal},
keywords = {Big Data, Healthcare, Big data analytics, disease diagnosis, predictive analysis, security, privacy},
abstract = {The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.}
}
@article{OUYANG201860,
title = {Methodologies, principles and prospects of applying big data in safety science research},
journal = {Safety Science},
volume = {101},
pages = {60-71},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517304320},
author = {Qiumei Ouyang and Chao Wu and Lang Huang},
keywords = {Safety big data (SBD), Safety small data (SSD), Safety science, Big data application, Method, Principle, Prospect and challenge},
abstract = {It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.}
}
@article{CHATFIELD2018336,
title = {Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services},
journal = {Government Information Quarterly},
volume = {35},
number = {2},
pages = {336-347},
year = {2018},
note = {Agile Government and Adaptive Governance in the Public Sector},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17300394},
author = {Akemi Takeoka Chatfield and Christopher G. Reddick},
keywords = {On-demand services, Customer agility, Systemic use, Big data, Big data analytics, IT assimilation, Process-level strategic alignment, Digital infrastructures, 311 services, Government},
abstract = {A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.}
}
@article{OUSSOUS2018431,
title = {Big Data technologies: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {30},
number = {4},
pages = {431-448},
year = {2018},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817300034},
author = {Ahmed Oussous and Fatima-Zahra Benjelloun and Ayoub {Ait Lahcen} and Samir Belfkih},
keywords = {Big Data, Hadoop, Big Data distributions, Big Data analytics, NoSQL, Machine learning},
abstract = {Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.}
}
@article{DREMEL2020103121,
title = {Actualizing big data analytics affordances: A revelatory case study},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103121},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308522},
author = {Christian Dremel and Matthias M. Herterich and Jochen Wulf and Jan {vom Brocke}},
keywords = {Big data analytics, Affordance theory, Socio-technical approach, Organizational transformation, Organizational benefits, Affordance actualization},
abstract = {Drawing on a revelatory case study, we identify four big data analytics (BDA) actualization mechanisms: (1) enhancing, (2) constructing, (3) coordinating, and (4) integrating, which manifest in actions on three socio-technical system levels, i.e., the structure, actor, and technology levels. We investigate the actualization of four BDA affordances at an automotive manufacturing company, i.e., establishing customer-centric marketing, provisioning vehicle-data-driven services, data-driven vehicle developing, and optimizing production processes. This study introduces a theoretical perspective to BDA research that explains how organizational actions contribute to actualizing BDA affordances. We further provide practical implications that can help guide practitioners in BDA adoption.}
}
@article{BACHECHI2022100292,
title = {Big Data Analytics and Visualization in Traffic Monitoring},
journal = {Big Data Research},
volume = {27},
pages = {100292},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100292},
url = {https://www.sciencedirect.com/science/article/pii/S221457962100109X},
author = {Chiara Bachechi and Laura Po and Federica Rollo},
keywords = {Traffic, Time series, Air quality maps, Real-time data, Spatio-temporal data, Smart cities},
abstract = {This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.}
}
@article{LIM201886,
title = {Smart cities with big data: Reference models, challenges, and considerations},
journal = {Cities},
volume = {82},
pages = {86-99},
year = {2018},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2018.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0264275117308545},
author = {Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio},
keywords = {Smart city, Big data, Reference model, Challenge, Consideration},
abstract = {Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.}
}
@article{LI2021103928,
title = {Search query of English translation text based on embedded system and big data},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103928},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103928},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121001071},
author = {Zhihong Li},
keywords = {Cross-language information retrieval, Optical character recognition, Embedded applications},
abstract = {Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.}
}
@article{LI2019168,
title = {Lithium-ion battery modeling based on Big Data},
journal = {Energy Procedia},
volume = {159},
pages = {168-173},
year = {2019},
note = {Renewable Energy Integration with Mini/Microgrid},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2018.12.046},
url = {https://www.sciencedirect.com/science/article/pii/S1876610218313419},
author = {Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang},
keywords = {electric vehicle, lithium-ion power battery, modeling, battery management, bigdata, deeplearning},
abstract = {Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.}
}
@article{MEHTA201857,
title = {Concurrence of big data analytics and healthcare: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {114},
pages = {57-65},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618302466},
author = {Nishita Mehta and Anil Pandit},
keywords = {Big data, Analytics, Healthcare, Predictive analytics, Evidence-based medicine},
abstract = {Background
The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care.
Purpose
This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges.
Data sources
A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered.
Study selection
Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected.
Data extraction
Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare.
Results
A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare.
Conclusion
This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.}
}
@article{SHENG201797,
title = {A multidisciplinary perspective of big data in management research},
journal = {International Journal of Production Economics},
volume = {191},
pages = {97-112},
year = {2017},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S092552731730169X},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Big data, Management research, Literature review},
abstract = {In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.}
}
@article{AKOKA2017105,
title = {Research on Big Data – A systematic mapping study},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {105-115},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917300211},
author = {Jacky Akoka and Isabelle Comyn-Wattiau and Nabil Laoufi},
keywords = {Big Data, Systematic mapping study, Framework, Artefact, Usage, Analytics},
abstract = {Big Data has emerged as a significant area of study for both practitioners and researchers. Big Data is a term for massive data sets with large structure. In 2012, Big Data passed the top of the Gartner Hype Cycle, attesting the maturity level of this technology and its applications. The aim of this paper is to examine how do researchers grasp the big data concept? We will answer the following questions: How many research papers are produced? What is the annual trend of publications? What are the hot topics in big data research? What are the most investigated big data topics? Why the research is performed? What are the most frequently obtained research artefacts? What does big data research produces? Who are the active authors? Which journals include papers on Big Data? What are the active disciplines? For this purpose, we provide a framework identifying existing and emerging research areas of Big Data. This framework is based on eight dimensions, including the SMACIT (Social Mobile Analytics Cloud Internet of Things) perspective. Current and past research in Big Data are analyzed using a systematic mapping study of publications based on more than a decade of related academic publications. The results have shown that significant contributions have been made by the research community, attested by a continuous increase in the number of scientific publications that address Big Data. We found that researchers are increasingly involved in research combining Big Data and Analytics, Cloud, Internet of things, mobility or social media. As for quality objectives, besides an interest in performance, other topics as scalability is emerging. Moreover, security and quality aspects become important. Researchers on Big Data provide more algorithms, frameworks, and architectures than other artifacts. Finally, application domains such as earth, energy, medicine, ecology, marketing, and health attract more attention from researchers on big data. A complementary content analysis on a subset of papers sheds some light on the evolving field of big data research.}
}
@article{LI2019103149,
title = {Experience and reflection from China’s Xiangya medical big data project},
journal = {Journal of Biomedical Informatics},
volume = {93},
pages = {103149},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103149},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930067X},
author = {Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan},
keywords = {Medical big data, Data sharing, Information security, Cooperation mechanism, Medical data acquisition, Medical data centre},
abstract = {The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.}
}
@article{CANITO20181,
title = {Unfolding the relations between companies and technologies under the Big Data umbrella},
journal = {Computers in Industry},
volume = {99},
pages = {1-8},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S016636151830040X},
author = {João Canito and Pedro Ramos and Sérgio Moro and Paulo Rita},
keywords = {Big data companies, Big data technologies, Online news, Gartner magic quadrant},
abstract = {Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.}
}
@article{TCHAGNAKOUANOU201868,
title = {An optimal big data workflow for biomedical image analysis},
journal = {Informatics in Medicine Unlocked},
volume = {11},
pages = {68-74},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352914818300844},
author = {Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and René Tchinda},
keywords = {Biomedical images, Big data, Artificial intelligence, Machine learning, Hadoop/spark},
abstract = {Background and objective
In the medical field, data volume is increasingly growing, and traditional methods cannot manage it efficiently. In biomedical computation, the continuous challenges are: management, analysis, and storage of the biomedical data. Nowadays, big data technology plays a significant role in the management, organization, and analysis of data, using machine learning and artificial intelligence techniques. It also allows a quick access to data using the NoSQL database. Thus, big data technologies include new frameworks to process medical data in a manner similar to biomedical images. It becomes very important to develop methods and/or architectures based on big data technologies, for a complete processing of biomedical image data.
Method
This paper describes big data analytics for biomedical images, shows examples reported in the literature, briefly discusses new methods used in processing, and offers conclusions. We argue for adapting and extending related work methods in the field of big data software, using Hadoop and Spark frameworks. These provide an optimal and efficient architecture for biomedical image analysis. This paper thus gives a broad overview of big data analytics to automate biomedical image diagnosis. A workflow with optimal methods and algorithm for each step is proposed.
Results
Two architectures for image classification are suggested. We use the Hadoop framework to design the first, and the Spark framework for the second. The proposed Spark architecture allows us to develop appropriate and efficient methods to leverage a large number of images for classification, which can be customized with respect to each other.
Conclusions
The proposed architectures are more complete, easier, and are adaptable in all of the steps from conception. The obtained Spark architecture is the most complete, because it facilitates the implementation of algorithms with its embedded libraries.}
}
@article{MCISAAC2020510,
title = {Real-world evaluation of enhanced recovery after surgery: big data under the microscope},
journal = {British Journal of Anaesthesia},
volume = {124},
number = {5},
pages = {510-512},
year = {2020},
issn = {0007-0912},
doi = {https://doi.org/10.1016/j.bja.2020.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0007091220300593},
author = {Daniel I. McIsaac},
keywords = {big data, enhanced recovery, epidemiology, orthopedic surgery, postoperative outcome, study design}
}
@article{YAN2018457,
title = {Energy-efficient shipping: An application of big data analysis for optimizing engine speed of inland ships considering multiple environmental factors},
journal = {Ocean Engineering},
volume = {169},
pages = {457-468},
year = {2018},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2018.08.050},
url = {https://www.sciencedirect.com/science/article/pii/S0029801818316421},
author = {Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn},
keywords = {Ship energy efficiency, Speed optimization, Big data analysis, Parallel k-means algorithm, Hadoop},
abstract = {Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.}
}
@article{SCOTT201820,
title = {The role of Statistics in the era of big data: Crucial, critical and under-valued},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {20-24},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300956},
author = {E. Marian Scott},
keywords = {Data, Sampling, Variability, Inference, Uncertainty},
abstract = {What is the role of Statistics in the era of big data, or is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of ‘bigness’ of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{WIBISONO201929,
title = {Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm for big data prediction},
journal = {Knowledge-Based Systems},
volume = {176},
pages = {29-39},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119301443},
author = {Ari Wibisono and Devvi Sarwinda},
keywords = {ARDEV, Big data prediction, FIMT-DD, Tree regression},
abstract = {Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.}
}
@article{AKHAVANHEJAZI201891,
title = {Power systems big data analytics: An assessment of paradigm shift barriers and prospects},
journal = {Energy Reports},
volume = {4},
pages = {91-100},
year = {2018},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352484717300616},
author = {Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad},
keywords = {Energy, Big data analytics, Internet of energy, Smart grid},
abstract = {Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today’s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.}
}
@article{QIAN2021645,
title = {Visual recognition processing of power monitoring data based on big data computing},
journal = {Energy Reports},
volume = {7},
pages = {645-657},
year = {2021},
note = {2021 International Conference on Energy Engineering and Power Systems},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S235248472101009X},
author = {Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi},
keywords = {Power control data, Monitoring, Visual identification, Iterative screening, CARIMA},
abstract = {The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2–7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.}
}
@article{ULLAH201981,
title = {Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review},
journal = {Journal of Systems and Software},
volume = {151},
pages = {81-118},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.051},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300172},
author = {Faheem Ullah and Muhammad {Ali Babar}},
keywords = {Big data, Cybersecurity, Quality attribute, Architectural tactic},
abstract = {Context
Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
Objective
We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
Method
We used Systematic Literature Review (SLR) method for reviewing 74 papers.
Result
Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
Conclusion
Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.}
}
@article{LI2020106143,
title = {Ensemble-based deep learning for estimating PM2.5 over California with multisource big data including wildfire smoke},
journal = {Environment International},
volume = {145},
pages = {106143},
year = {2020},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2020.106143},
url = {https://www.sciencedirect.com/science/article/pii/S0160412020320985},
author = {Lianfa Li and Mariam Girguis and Frederick Lurmann and Nathan Pavlovic and Crystal McClure and Meredith Franklin and Jun Wu and Luke D. Oman and Carrie Breton and Frank Gilliland and Rima Habre},
keywords = {PM, Machine learning, Air pollution exposure, Wildfires, Remote sensing, California, High spatiotemporal resolution},
abstract = {Introduction
Estimating PM2.5 concentrations and their prediction uncertainties at a high spatiotemporal resolution is important for air pollution health effect studies. This is particularly challenging for California, which has high variability in natural (e.g, wildfires, dust) and anthropogenic emissions, meteorology, topography (e.g. desert surfaces, mountains, snow cover) and land use.
Methods
Using ensemble-based deep learning with big data fused from multiple sources we developed a PM2.5 prediction model with uncertainty estimates at a high spatial (1 km × 1 km) and temporal (weekly) resolution for a 10-year time span (2008–2017). We leveraged autoencoder-based full residual deep networks to model complex nonlinear interrelationships among PM2.5 emission, transport and dispersion factors and other influential features. These included remote sensing data (MAIAC aerosol optical depth (AOD), normalized difference vegetation index, impervious surface), MERRA-2 GMI Replay Simulation (M2GMI) output, wildfire smoke plume dispersion, meteorology, land cover, traffic, elevation, and spatiotemporal trends (geo-coordinates, temporal basis functions, time index). As one of the primary predictors of interest with substantial missing data in California related to bright surfaces, cloud cover and other known interferences, missing MAIAC AOD observations were imputed and adjusted for relative humidity and vertical distribution. Wildfire smoke contribution to PM2.5 was also calculated through HYSPLIT dispersion modeling of smoke emissions derived from MODIS fire radiative power using the Fire Energetics and Emissions Research version 1.0 model.
Results
Ensemble deep learning to predict PM2.5 achieved an overall mean training RMSE of 1.54 μg/m3 (R2: 0.94) and test RMSE of 2.29 μg/m3 (R2: 0.87). The top predictors included M2GMI carbon monoxide mixing ratio in the bottom layer, temporal basis functions, spatial location, air temperature, MAIAC AOD, and PM2.5 sea salt mass concentration. In an independent test using three long-term AQS sites and one short-term non-AQS site, our model achieved a high correlation (>0.8) and a low RMSE (<3 μg/m3). Statewide predictions indicated that our model can capture the spatial distribution and temporal peaks in wildfire-related PM2.5. The coefficient of variation indicated highest uncertainty over deciduous and mixed forests and open water land covers.
Conclusion
Our method can be generalized to other regions, including those having a mix of major urban areas, deserts, intensive smoke events, snow cover and complex terrains, where PM2.5 has previously been challenging to predict. Prediction uncertainty estimates can also inform further model development and measurement error evaluations in exposure and health studies.}
}