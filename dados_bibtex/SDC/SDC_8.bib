@article{PAVEL20224837,
title = {The potential of a data centred approach & knowledge graph data representation in chemical safety and drug design},
journal = {Computational and Structural Biotechnology Journal},
volume = {20},
pages = {4837-4849},
year = {2022},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2022.08.061},
url = {https://www.sciencedirect.com/science/article/pii/S2001037022003956},
author = {Alisa Pavel and Laura A. Saarimäki and Lena Möbus and Antonio Federico and Angela Serra and Dario Greco},
keywords = {Knowledge graph, Big data, Data integration, Toxicology, Drug design, Chemical safety},
abstract = {Big Data pervades nearly all areas of life sciences, yet the analysis of large integrated data sets remains a major challenge. Moreover, the field of life sciences is highly fragmented and, consequently, so is its data, knowledge, and standards. This, in turn, makes integrated data analysis and knowledge gathering across sub-fields a demanding task. At the same time, the integration of various research angles and data types is crucial for modelling the complexity of organisms and biological processes in a holistic manner. This is especially valid in the context of drug development and chemical safety assessment where computational methods can provide solutions for the urgent need of fast, effective, and sustainable approaches. At the same time, such computational methods require the development of methodologies suitable for an integrated and data centred Big Data view. Here we discuss Knowledge Graphs (KG) as a solution to a data centred analysis approach for drug and chemical development and safety assessment. KGs are knowledge bases, data analysis engines, and knowledge discovery systems all in one, allowing them to be used from simple data retrieval, over meta-analysis to complex predictive and knowledge discovery systems. Therefore, KGs have immense potential to advance the data centred approach, the re-usability, and informativity of data. Furthermore, they can improve the power of analysis, and the complexity of modelled processes, all while providing knowledge in a natively human understandable network data model.}
}
@incollection{DOGUC202295,
title = {Chapter 9 - Recent applications of data mining in medical diagnosis and prediction},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {95-109},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000066},
author = {Ozge Doguc and Zehra Nur Canbolat and Gokhan Silahtaroglu},
keywords = {Big data, Data mining, Health sector, Intelligent medical diagnosis systems},
abstract = {Big data has been used in the health sector to improve the quality of life, predict epidemics, cure diseases, and avoid preventable deaths, beyond increasing profits or reducing the burden of excess labor. Data sources in healthcare have become quite diversified and accessible to individuals, such as wearable and implantable devices, smartphones, and real-time sensors. When combined with existing health data, daily (even instantaneous) data from these devices can be used to predict future health conditions of individuals and to identify necessary intervention points. This chapter discusses a number of recent studies that introduces methods for using big data to create intelligent systems for patient diagnosis, triage, predicting lab results, and even detecting tumors. These studies open ways for researchers in the healthcare sector to improve the quality of services provided to the patients as well as reducing costs for the healthcare institutions.}
}
@article{SHAH2019562,
title = {An Internet-of-things Enabled Smart Manufacturing Testbed},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {1},
pages = {562-567},
year = {2019},
note = {12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.06.122},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319302083},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-things, smart manufacturing, big data, data analytics, statistical analysis, vibration, soft sensor, process monitoring},
abstract = {The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.}
}
@article{PRODHAN2022105327,
title = {A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions},
journal = {Environmental Modelling & Software},
volume = {149},
pages = {105327},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105327},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222000330},
author = {Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana},
keywords = {Machine learning, Deep learning, Forecasting, Drought, Big data},
abstract = {Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.}
}
@incollection{BERMAN20181,
title = {1 - Introduction},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {1-13},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000017},
author = {Jules J. Berman},
keywords = {Big data definition, Small data, Data filtering, Data reduction},
abstract = {Big Data is not synonymous with lots and lots of data. Useful Big Data resources adhere to a set of data management principles that are fundamentally different from the traditional practices followed for small data projects. The areas of difference include: data collection; data annotation (including metadata and identifiers); location and distribution of stored data; classification of data; data access rules; data curation; data immutability; data permanence; verification and validity methods for the contained data; analytic methods; costs; and incumbent legal, social, and ethical issues. Skilled professionals who are adept in the design and management of small data resources may be unprepared for the unique challenges posed by Big Data. This chapter is an introduction to topics that will be fully explained in later chapters.}
}
@article{PUTHAL201722,
title = {A dynamic prime number based efficient security mechanism for big sensing data streams},
journal = {Journal of Computer and System Sciences},
volume = {83},
number = {1},
pages = {22-42},
year = {2017},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022000016000209},
author = {Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen},
keywords = {Security, Sensor networks, Big data stream, Key exchange, Security verification},
abstract = {Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.}
}
@article{D2021,
title = {A study on artificial intelligence for monitoring smart environments},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321043911},
author = {Karthika D.},
keywords = {Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies},
abstract = {Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.}
}
@article{DETRE2018541,
title = {Handling veracity in multi-criteria decision-making: A multi-dimensional approach},
journal = {Information Sciences},
volume = {460-461},
pages = {541-554},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517309337},
author = {Guy {De Tré} and Robin {De Mol} and Antoon Bronselaer},
keywords = {Multi-criteria decision-making, Veracity in ‘big’ data, Data quality assessment, Data quality handling},
abstract = {Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.}
}
@incollection{DEMCHENKO201721,
title = {Chapter 2 - Cloud Computing Infrastructure for Data Intensive Applications},
editor = {Hui-Huang Hsu and Chuan-Yu Chang and Ching-Hsien Hsu},
booktitle = {Big Data Analytics for Sensor-Network Collected Intelligence},
publisher = {Academic Press},
pages = {21-62},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-809393-1},
doi = {https://doi.org/10.1016/B978-0-12-809393-1.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093931000027},
author = {Yuri Demchenko and Fatih Turkmen and Cees {de Laat} and Ching-Hsien Hsu and Christophe Blanchet and Charles Loomis},
keywords = {Big Data, Big Data Architecture Framework (BDAF), Big data infrastructure, NIST Big Data Architecture (BDRA), Cloud computing, Intercloud Architecture Framework (ICAF), Cloud powered design, SlipStream cloud automation platform},
abstract = {This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis of emerging Big Data and data intensive technologies and provides the general definition of the Big Data Architecture Framework (BDAF) that includes the following components: Big Data definition, Data Management including data lifecycle and data structures, generically cloud based BDI, Data Analytics technologies and platforms, and Big Data security, compliance, and privacy. The chapter refers to NIST Big Data Reference Architecture (BDRA) and summarizes general requirements to Big Data systems described in NIST documents. The proposed BDI and its cloud-based components are defined in accordance with the NIST BDRA and BDAF. This chapter provides detailed analysis of the two bioinformatics use cases as typical example of the Big Data applications that have being developed by the authors in the framework of the CYCLONE project. The effective use of cloud for bioinformatics applications requires maximum automation of the applications deployment and management that may include resources from multiple clouds and providers. The proposed CYCLONE platform for multicloud multiprovider applications deployment and management is based on the SlipStream cloud automation platform and includes all necessary components to build and operate complex scientific applications. The chapter discusses existing platforms for cloud powered applications development and deployment automation, in particularly referring to the SlipStream cloud automation platform, which allows multicloud applications deployment and management. The chapter also includes a short overview of the existing Big Data platforms and services provided by the major cloud services providers which can be used for fast deployment of customer Big Data applications using the benefits of cloud technologies and global cloud infrastructure.}
}
@article{KARAFILI201852,
title = {An argumentation reasoning approach for data processing},
journal = {Computers in Industry},
volume = {94},
pages = {52-61},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S016636151730338X},
author = {Erisa Karafili and Konstantina Spanaki and Emil C. Lupu},
keywords = {Data processing, Data quality, Usage control, Argumentation reasoning, Data manufacturing, Case scenarios},
abstract = {Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.}
}
@article{OYEDELE2021100158,
title = {Machine learning predictions for lost time injuries in power transmission and distribution projects},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100158},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100158},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000797},
author = {Ahmed O. Oyedele and Anuoluwapo O. Ajayi and Lukumon O. Oyedele},
keywords = {Hazard assessment, Deep learning, Big data predictive analytics, Power infrastructure, Zero count data},
abstract = {Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts’ perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.}
}
@article{REIS2020232,
title = {Assessing the drivers of machine learning business value},
journal = {Journal of Business Research},
volume = {117},
pages = {232-243},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320303581},
author = {Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro},
keywords = {Machine learning, Business value, Competitive advantage, Dynamic capabilities theory},
abstract = {Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.}
}
@article{LI202156,
title = {Construction of an artificial intelligence system in dermatology: effectiveness and consideration of Chinese Skin Image Database (CSID)},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {56-60},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S266710262100005X},
author = {Chengxu Li and Wenmin Fei and Yang Han and Xiaoli Ning and Ziyi Wang and Keke Li and Ke Xue and Jingkai Xu and Ruixing Yu and Rusong Meng and Feng Xu and Weimin Ma and Yong Cui},
keywords = {Artificial intelligence, Dermatology, Skin image, Chinese Skin Image Database},
abstract = {After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.}
}
@article{MAHDAVINEJAD2018161,
title = {Machine learning for internet of things data analysis: a survey},
journal = {Digital Communications and Networks},
volume = {4},
number = {3},
pages = {161-175},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S235286481730247X},
author = {Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth},
keywords = {Machine learning, Internet of Things, Smart data, Smart City},
abstract = {Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.}
}
@article{CHO2022102477,
title = {What's driving the diffusion of next-generation digital technologies?},
journal = {Technovation},
pages = {102477},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102477},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000244},
author = {Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik},
abstract = {The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.}
}
@article{ZHOU2021103342,
title = {Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation},
journal = {Journal of Energy Storage},
volume = {44},
pages = {103342},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103342},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21010331},
author = {Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng},
keywords = {Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction},
abstract = {This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.}
}
@incollection{PARMAR2022401,
title = {18 - 5G-enabled deep learning-based framework for healthcare mining: State of the art and challenges},
editor = {Sudeep Tanwar},
booktitle = {Blockchain Applications for Healthcare Informatics},
publisher = {Academic Press},
pages = {401-420},
year = {2022},
isbn = {978-0-323-90615-9},
doi = {https://doi.org/10.1016/B978-0-323-90615-9.00016-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323906159000165},
author = {Rahil Parmar and Dhruval Patel and Naitik Panchal and Uttam Chauhan and Jitendra Bhatia},
keywords = {5G healthcare, Deep learning, Big data, Human disease prediction},
abstract = {The importance of healthcare technologies has been made clear in the current pandemic. Healthcare informatics plays an important role in facilitating healthcare and providing healthcare services in real time. Healthcare informatics has developed from Healthcare 1.0 to Healthcare 4.0 in the last few decades. The data generated from the various sources are stored as electronic health record. These data are collected in different forms and formats. The inconsistent data could be handled using various techniques of big data. The information obtained from big data analytics can be used for the prediction of diseases or conditions using artificial intelligence, machine learning, and deep learning techniques. 5G plays an important role in healthcare informatics by enabling real-time remote monitoring and improving augmented reality, virtual reality, and spatial computing. With 5G technologies, a large number of devices can be connected using high-performance computing over large distances to provide healthcare services. Blockchain is applied in healthcare for health record management, insurance claims, drug tracking, authentication, and ensuring the integrity of medical data. Deep learning techniques can be applied to ever-changing data for the detection and prevention of disease. For the classification challenge, deep convolutional neural networks using pictures of diseased regions are often utilized. In many research techniques, AlexNet and GoogLeNet have been utilized to identify plant diseases. This chapter discusses the state of the art for detecting human sickness as well as the associated 5G healthcare framework for improving it.}
}
@article{XIE202272,
title = {Real-World Data for Healthcare Research in China: Call for Actions},
journal = {Value in Health Regional Issues},
volume = {27},
pages = {72-81},
year = {2022},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212109921000765},
author = {Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu},
keywords = {administrative claims, data access, electronic health records, real-world data},
abstract = {Objectives
This study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.
Methods
The study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.
Results
Secondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.
Conclusions
Although healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed—data access/sharing and data quality.}
}
@article{SHET2021311,
title = {Examining the determinants of successful adoption of data analytics in human resource management – A framework for implications},
journal = {Journal of Business Research},
volume = {131},
pages = {311-326},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.03.054},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002174},
author = {Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi},
keywords = {Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis},
abstract = {Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.}
}
@article{JOSEPH2022115,
title = {A Predictive Maintenance Application for A Robot Cell using LSTM Model},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {115-120},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.193},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014082},
author = {Doyel Joseph and Tilani Gallege and Ebru Turanoglu Bekar and Catarina Dudas and Anders Skoogh},
keywords = {Smart Maintenance, Predictive Maintenance, Machine Learning, Long Short-Term Memory (LSTM), CRISP-DM, Industrial Robots, Manufacturing},
abstract = {Maintaining equipment is critical for increasing production capacity and decreasing production time. With the advent of digitalization, industries are able to access massive amounts of data that can be used to ensure their long-term viability and competitive advantage by implementing predictive maintenance. Therefore, this study aims to demonstrate a predictive maintenance application for a robot cell using real-world manufacturing big data coming from a company in the automotive industry. A hyperparameter tuned Long Short-Term Memory (LSTM) model is developed, and the results show that this model is capable of predicting the day of failure with good accuracy. The difficulties inherent in conducting real-world industrial initiatives are analyzed, and recommendations for improvement are presented.}
}
@article{AMEEN2021106761,
title = {Consumer interaction with cutting-edge technologies: Implications for future research},
journal = {Computers in Human Behavior},
volume = {120},
pages = {106761},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106761},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221000832},
author = {Nisreen Ameen and Sameer Hosany and Ali Tarhini},
keywords = {Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics},
abstract = {This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.}
}
@article{FRONZETTICOLLADON2019113075,
title = {Using social network and semantic analysis to analyze online travel forums and forecast tourism demand},
journal = {Decision Support Systems},
volume = {123},
pages = {113075},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113075},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301046},
author = {Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella},
keywords = {Tourism forecasting, Social network analysis, Semantic analysis, Online community, Text mining, Big data},
abstract = {Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.}
}
@article{JUNG2020112,
title = {Ten-year patient journey of stage III non-small cell lung cancer patients: A single-center, observational, retrospective study in Korea (Realtime autOmatically updated data warehOuse in healTh care; UNIVERSE-ROOT study)},
journal = {Lung Cancer},
volume = {146},
pages = {112-119},
year = {2020},
issn = {0169-5002},
doi = {https://doi.org/10.1016/j.lungcan.2020.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0169500220304670},
author = {Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park},
keywords = {Real-time updated system, Big data, Real-world data, NSCLC, Treatment},
abstract = {Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.}
}
@article{WANG2021189,
title = {Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework},
journal = {Process Safety and Environmental Protection},
volume = {148},
pages = {189-199},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2020.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0957582020318000},
author = {Bing Wang},
keywords = {Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making},
abstract = {In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.}
}
@article{WANG2022106310,
title = {Are the official national data credible? Empirical evidence from statistics quality evaluation of China's coal and its downstream industries},
journal = {Energy Economics},
pages = {106310},
year = {2022},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2022.106310},
url = {https://www.sciencedirect.com/science/article/pii/S014098832200439X},
author = {Delu Wang and Fan Chen and Jingqi Mao and Nannan Liu and Fangyu Rong},
keywords = {Industrial statistics, Data quality, Comprehensive evaluation, Coal-related industry},
abstract = {The authenticity and quality of industrial statistical data directly affects all types of systematic research based on it. Considering the limitations of extant data quality evaluation literature on research objects and evaluation methods, we constructed a new data quality comprehensive inspection and evaluation model based on Benford's Law (BL) and the technique for order of preference by similarity to ideal solution (TOPSIS), selected coal-related industries as the research object, and conducted an empirical test along the research path of “Industry→Province→Indicator”. The results showed that, at industry level, the quality of statistical data for China's coal-related industries from 2001 to 2016 was generally poor. Among the eight sample industries selected, the data quality for five industries (including coal, electricity, and steel) was assessed as poor or slightly poor. Furthermore, at the provincial level, there is significant spatial heterogeneity in the quality of statistical data for various industries affected by factors such as economic structure, marketization level, and industrial diversity. Compared with other types of statistical indicators, industry financial indicators are more prone to data quality problems at the indicator level, and the suspicious indicators of different industries show certain common characteristics and some industry differences. To improve the quality of industrial statistical data and reduce the possible adverse impacts of data quality problems, based on the research findings, we propose targeted countermeasures and suggestions on how to prevent data fraud and effectively identify and rationally use suspicious data.}
}
@incollection{KOUL20223,
title = {Chapter 1 - Influence and implementation of Industry 4.0 in health care},
editor = {Aboul Ella Hassanien and Jyotir Moy Chatterjee and Vishal Jain},
booktitle = {Artificial Intelligence and Industry 4.0},
publisher = {Academic Press},
pages = {3-21},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-88468-6},
doi = {https://doi.org/10.1016/B978-0-323-88468-6.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323884686000024},
author = {Sumit Koul},
keywords = {Industrial revolution, Healthcare 4.0, Blockchain technology, Internet of Things (IoT), Big data},
abstract = {With the advancement of technology, the world is changing and automating at a rapid pace. Digitization plays a major role in the automation of technology. In this context, Industry 4.0 shows how industrial production is developing along with the latest technology. In Industry 4.0, much manual work has been replaced by automated machines that can be controlled with developing technologies such as artificial intelligence (AI), big data, cloud computing, and so on. Various technologies have been introduced in the healthcare sector due to Industry 4.0. These include AI, three-dimensional printing, machine learning, cognitive systems, autonomous robots, autonomous vehicles, augmented reality, big data, Internet of Things (IoT), blockchain technology, and more. This chapter discusses the transformation of the healthcare industry in the context of Industry 4.0. It presents a detailed study on big data, IoT, and blockchain technology with different applications that can enhance what is known as Healthcare 4.0. The chapter includes three case studies that illustrate the use of innovation in Healthcare 4.0 to detect and diagnose disease using portable medical devices connected to the IoT.}
}
@article{KEBISEK202011168,
title = {Artificial Intelligence Platform Proposal for Paint Structure Quality Prediction within the Industry 4.0 Concept},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11168-11174},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.299},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320305796},
author = {M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec},
keywords = {artificial intelligence, automotive, big data analytics, industry 4.0, knowledge discovery, neural networks, prediction, principal component analysis},
abstract = {This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.}
}
@article{NEETHIRAJAN2021100408,
title = {Digital Livestock Farming},
journal = {Sensing and Bio-Sensing Research},
volume = {32},
pages = {100408},
year = {2021},
issn = {2214-1804},
doi = {https://doi.org/10.1016/j.sbsr.2021.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2214180421000131},
author = {Suresh Neethirajan and Bas Kemp},
keywords = {Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture},
abstract = {As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal’s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.}
}
@incollection{FAULKNER202081,
title = {4 - Data Fundamentals},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {81-92},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000176},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Data ecosystems, Data context, Data architectures, Data Integrity, Data Quality},
abstract = {Data is one component in a system. It has value. The economics of increasingly data-centric systems is explored. There is a growing reliance on data to create systems that are larger scale, have wider scope and are more complex. As reliance on the data component increases, a self-reinforcing problem of implementing checks and balances necessary to enforce appropriate levels of risk reduction arises. This chapter introduces data architecture elements of container and content. It places this architecture within an appropriate data context. It introduces the concepts of data quality and data integrity. Data integrity is placed within the Safety Management and Safety Assurance processes. Big data and machine learning are considered in this context.}
}
@article{HARRIGAN2021102246,
title = {Identifying influencers on social media},
journal = {International Journal of Information Management},
volume = {56},
pages = {102246},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220314456},
author = {Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers},
keywords = {Influencers, Market mavens, Big data, Social media, Twitter},
abstract = {The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.}
}
@article{KIM201818,
title = {Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability},
journal = {Computers in Human Behavior},
volume = {86},
pages = {18-33},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830178X},
author = {Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis},
keywords = {Semantic web, IT professionals' perspective technology adoption, Technology-organization-environment framework, Innovation diffusion theory},
abstract = {The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology–organization–environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.}
}
@article{TRIVEDI2022,
title = {A Practical Guide to Use of Publicly Available Data Sets for Observational Research in Interventional Radiology},
journal = {Journal of Vascular and Interventional Radiology},
year = {2022},
issn = {1051-0443},
doi = {https://doi.org/10.1016/j.jvir.2022.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1051044322011241},
author = {Premal S. Trivedi and Vincent M. Timpone and Rustain L. Morgan and Alexandria M. Jensen and Margaret Reid and P. Michael Ho and Osman Ahmed},
abstract = {Observational data research studying access, utilization, cost, and outcomes of image-guided interventions using publicly available “big data” sets is growing in the interventional radiology (IR) literature. Publicly available data sets offer insight into real-world care and represent an important pillar of IR research moving forward. They offer insights into how IR procedures are being used nationally and whether they are working as intended. On the other hand, large data sources are aggregated using complex sampling frames, and their strengths and weaknesses only become apparent after extensive use. Unintentional misuse of large data sets can result in misleading or sometimes erroneous conclusions. This review introduces the most commonly used databases relevant to IR research, highlights their strengths and limitations, and provides recommendations for use. In addition, it summarizes methodologic best practices pertinent to all data sets for planning and executing scientifically rigorous and clinically relevant observational research.}
}
@article{FAN2021123651,
title = {The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {123651},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123651},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620336969},
author = {Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo},
keywords = {Internet of things in agriculture, Big data, High-throughput phenotype, Data mining},
abstract = {With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype–phenotype–envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.}
}
@article{AHMAD2021125834,
title = {Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities},
journal = {Journal of Cleaner Production},
volume = {289},
pages = {125834},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125834},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621000548},
author = {Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen},
keywords = {Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization},
abstract = {The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.}
}
@incollection{GUDIVADA201731,
title = {Chapter 2 - Data Analytics: Fundamentals},
editor = {Mashrur Chowdhury and Amy Apon and Kakan Dey},
booktitle = {Data Analytics for Intelligent Transportation Systems},
publisher = {Elsevier},
pages = {31-67},
year = {2017},
isbn = {978-0-12-809715-1},
doi = {https://doi.org/10.1016/B978-0-12-809715-1.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809715100002X},
author = {Venkat N. Gudivada},
keywords = {Data analytics, data science, data mining, clustering, classification, model building},
abstract = {This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, big data analytics, to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open source tools and resources for developing data analytics systems are listed. Future directions in data analytics are indicated. The chapter concludes by providing a summary. To reinforce and enhance the reader’s data analytics knowledge and tools, questions and exercise problems are provided at the end of the chapter.}
}
@article{LI20189,
title = {Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China},
journal = {Computers, Environment and Urban Systems},
volume = {70},
pages = {9-23},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517301916},
author = {Fa Li and Zhipeng Gui and Huayi Wu and Jianya Gong and Yuan Wang and Siyu Tian and Jiawen Zhang},
keywords = {Geocoding, Missing values imputation, High Performance Computing, Industrial spatial distribution, Urban spatial structure, Short text classification},
abstract = {Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.}
}
@article{LIU201990,
title = {Constructing Large Scale Cohort for Clinical Study on Heart Failure with Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies in Data Reuse},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {2},
pages = {90-102},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003579},
url = {https://www.sciencedirect.com/science/article/pii/S1001929419300318},
author = {Daowen Liu and Liqi Lei and Tong Ruan and Ping He},
keywords = {electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study},
abstract = {Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.}
}
@article{CHENG2022134380,
title = {Dirty skies lower subjective well-being},
journal = {Journal of Cleaner Production},
pages = {134380},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134380},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203952X},
author = {Lu Cheng and Zhifu Mi and Yi-Ming Wei and Shidong Wang and Klaus Hubacek},
keywords = {Subjective well-being, Air pollution, Big data, Sentiment analysis},
abstract = {Self-reported life satisfaction of China's population has not improved as much as expected during the economic boom, which was accompanied by a significant decline in environmental performance. Is environmental pollution the culprit for the lagging subjective well-being? To explore this issue, this paper adopts the sentiment analysis method to construct a real-time daily subjective well-being metric at the city level based on the big data of online search traces. Using daily data from 13 Chinese cities centred on Beijing between August 2014 and December 2019, we look at the corelation between subjective well-being and air pollution and the heterogeneity in this relationship based on two separate identification strategies. We find that air pollutants are negatively correlated with subjective well-being, and well-being tends to decline more from pollution during hot seasons. In addition, residents in wealthier regions tend to be more sensitive to air pollution. This result may be explained by the differences in the subjective perception of air pollution and personal preferences at different levels of income. These findings provide information about the concerns of the public to the central government, thereby helping it take appropriate actions to respond to the dynamics of subjective well-being.}
}
@article{ZHANG2021101336,
title = {A framework of energy-consumption driven discrete manufacturing system},
journal = {Sustainable Energy Technologies and Assessments},
volume = {47},
pages = {101336},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101336},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821003465},
author = {Tao Zhang and Weixi Ji and Yongtao Qiu},
keywords = {Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining},
abstract = {Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.}
}
@incollection{WANG2021295,
title = {Chapter 13 - Artificial Intelligence for Flood Observation},
editor = {Guy J-P. Schumann},
booktitle = {Earth Observation for Flood Applications},
publisher = {Elsevier},
pages = {295-304},
year = {2021},
series = {Earth Observation},
isbn = {978-0-12-819412-6},
doi = {https://doi.org/10.1016/B978-0-12-819412-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194126000134},
author = {Ruo-Qian Wang},
keywords = {artificial intelligence, natural language processing, computer vision, machine learning, Big Data, Internet of Things, crowdsourcing, citizen science, surveillance video},
abstract = {Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical  declination in the Internet of Things (IoT) costs. Flood research and applications will progress with this emerging technology, as AI is creating new flood data sources, enhancing our capability to analyze the data, and improving our accuracy of flood predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of “review of the reviews” with example highlight the emerging AI applications in the field of flood hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.}
}
@article{CHENG2021102938,
title = {Construction of a service quality scale for the online food delivery industry},
journal = {International Journal of Hospitality Management},
volume = {95},
pages = {102938},
year = {2021},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2021.102938},
url = {https://www.sciencedirect.com/science/article/pii/S0278431921000815},
author = {Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen},
keywords = {Online food delivery, Service quality, Big data analytic, OFD service quality scale},
abstract = {The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.}
}
@article{SUN2022,
title = {A blockchain-based audit approach for encrypted data in federated learning},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000979},
author = {Zhe Sun and Junping Wan and Lihua Yin and Zhiqiang Cao and Tianjie Luo and Bin Wang},
keywords = {Audit, Data quality, Blockchain, Secure aggregation, Federated learning},
abstract = {The development of data-driven artificial intelligence technology has given birth to a variety of big data applications. Data has become an essential factor to improve these applications. Federated learning, a privacy-preserving machine learning method, is proposed to leverage data from different data owners. It is typically used in conjunction with cryptographic methods, in which data owners train the global model by sharing encrypted model updates. However, data encryption makes it difficult to identify the quality of these model updates. Malicious data owners may launch attacks such as data poisoning and free-riding. To defend against such attacks, it is necessary to find an approach to audit encrypted model updates. In this paper, we propose a blockchain-based audit approach for encrypted gradients. It uses a behavior chain to record the encrypted gradients from data owners, and an audit chain to evaluate the gradients’ quality. Specifically, we propose a privacy-preserving homomorphic noise mechanism in which the noise of each gradient sums to zero after aggregation, ensuring the availability of aggregated gradient. In addition, we design a joint audit algorithm that can locate malicious data owners without decrypting individual gradients. Through security analysis and experimental evaluation, we demonstrate that our approach can defend against malicious gradient attacks in federated learning.}
}
@article{JAGATHEESAPERUMAL2022107691,
title = {A holistic survey on the use of emerging technologies to provision secure healthcare solutions},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107691},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107691},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000131},
author = {Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan},
keywords = {Healthcare, Security, Internet of Things, Artificial intelligence, Machine learning, Deep learning, 5G networks},
abstract = {Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.}
}
@article{R2020235,
title = {Weibull Cumulative Distribution based real-time response and performance capacity modeling of Cyber–Physical Systems through software defined networking},
journal = {Computer Communications},
volume = {150},
pages = {235-244},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419311673},
author = {Gifty R. and Bharathi R.},
keywords = {Cyber–Physical Systems (CPS), Weibull Cumulative Distribution, Big data, Response time},
abstract = {Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber–Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber–physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.}
}
@article{LI20161,
title = {A snail shell process model for knowledge discovery via data analytics},
journal = {Decision Support Systems},
volume = {91},
pages = {1-12},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616301233},
author = {Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson},
keywords = {Knowledge discovery via data analytics, Snail shell process model, KDDA, Big data analytics, Data-driven decision making},
abstract = {The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.}
}
@incollection{FARRE2022197,
title = {Chapter 7 - Data-driven policy evaluation},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {197-225},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000026},
author = {Marçal Farré and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farré},
keywords = {Survey data, Administrative data, Big data, Policy evaluation, Impact evaluation, Process evaluation},
abstract = {Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.}
}
@article{THOMAS2021101994,
title = {Advances in monitoring and evaluation in low- and middle-income countries},
journal = {Evaluation and Program Planning},
volume = {89},
pages = {101994},
year = {2021},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2021.101994},
url = {https://www.sciencedirect.com/science/article/pii/S0149718921000896},
author = {James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar},
keywords = {Monitoring and evaluation, Health information systems, Developing countries},
abstract = {Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected “big data” from electronic health records and social media.}
}
@article{HANSEN20222461,
title = {Finding the FAIRness in perovskite photovoltaics research},
journal = {Matter},
volume = {5},
number = {8},
pages = {2461-2464},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S259023852200296X},
author = {Kameron R. Hansen and Luisa Whittaker-Brooks},
abstract = {With thousands of publications per year, the volume of data published on perovskite solar cells since the spark of the “perovskite fever” in 2013 is enormous and far exceeds the amount that any individual researcher could digest. To tackle this issue, Jacobsson et al.1 have created The Perovskite Database, which is part of a larger trend to harness the power of big data and artificial intelligence to accelerate the commercialization of perovskite solar cells.}
}
@article{VALENCA2021100008,
title = {Main challenges and opportunities to dynamic road space allocation: From static to dynamic urban designs},
journal = {Journal of Urban Mobility},
volume = {1},
pages = {100008},
year = {2021},
issn = {2667-0917},
doi = {https://doi.org/10.1016/j.urbmob.2021.100008},
url = {https://www.sciencedirect.com/science/article/pii/S266709172100008X},
author = {Gabriel Valença and Filipe Moura and Ana {Morais de Sá}},
keywords = {Dynamic road space allocation, Big data, Smart cities, Intelligent transportation systems, Information and communication technology, Urban planning},
abstract = {Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from façade to façade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.}
}
@article{SHENG2019321,
title = {Technology in the 21st century: New challenges and opportunities},
journal = {Technological Forecasting and Social Change},
volume = {143},
pages = {321-335},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311319},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making},
abstract = {Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.}
}
@article{APPELBAUM201729,
title = {Impact of business analytics and enterprise systems on managerial accounting},
journal = {International Journal of Accounting Information Systems},
volume = {25},
pages = {29-44},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1467089517300490},
author = {Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan},
keywords = {Managerial accounting, Business analytics, Big data, Enterprise systems, Business intelligence},
abstract = {The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.}
}
@article{KHALOUFI2018294,
title = {Security model for Big Healthcare Data Lifecycle},
journal = {Procedia Computer Science},
volume = {141},
pages = {294-301},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318520},
author = {Hayat Khaloufi and Karim Abouelmehdi and Abderrahim Beni-hssane and Mostafa Saadi},
keywords = {Big data Security, Big data in healthcare, Big data lifecycle, Security threat model},
abstract = {Big data is a concept that aimed at collecting, storing, processing and transforming large amount of data into value using new combination of strategies and technologies. Big data is characterized by data that have a large volume, massive velocity, numerous variety, useful value, and veracity. Big Data Analytics offers tremendous insights to different organizations especially in healthcare. Currently, Big healthcare data has the highest potential for improving patient outcomes, gaining valuable insights, predicting outbreaks of epidemics, avoiding preventable diseases and effectively minimizing the cost of healthcare delivery. However, the dynamic nature of health data presents various conceptual, technical, legal and ethical challenges associated with the data processing and analysis activities. The big data security and privacy concepts are some of the most pertinent issues and have become increasingly significant associated with big healthcare data in the modern world. In this paper, we give an overview of big data characteristics and challenges in healthcare and present big healthcare data lifecycle integrated with security threats and attacks to provide encompass policies and mechanisms that aim at solving the various security challenges in each step of big data lifecycle. The focus is also placed on the description of the recently proposed techniques related to authentication, encryption, anonymization, access control, and privacy. We finally propose an approach to secure threat model for big healthcare data lifecycle as a main contribution of this paper.}
}
@article{KREGEL2021107083,
title = {Process Mining for Six Sigma: Utilising Digital Traces},
journal = {Computers & Industrial Engineering},
volume = {153},
pages = {107083},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.107083},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220307531},
author = {I. Kregel and D. Stemann and J. Koch and A. Coners},
keywords = {process mining, six sigma, DMAIC, big data analytics, data science, project management},
abstract = {Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma’s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.}
}
@article{SILVA2015289,
title = {Workshop Synthesis: Respondent/Survey Interaction in a World of Web and Smartphone Apps},
journal = {Transportation Research Procedia},
volume = {11},
pages = {289-296},
year = {2015},
note = {Transport Survey Methods: Embracing Behavioural and Technological Changes Selected contributions from the 10th International Conference on Transport Survey Methods 16-21 November 2014, Leura, Australia},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2015.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S2352146515003166},
author = {João de Abreu e Silva and Mark Davis},
keywords = {web based surveys, smartphone surveys, respondent interaction, respondent burden},
abstract = {Web and smartphone surveys are increasingly being used to collect travel information. This workshop explored respondent interaction with these tools, covering a range of research concerns. While smartphone surveys facilitate real-time passive collection of continuous data, thereby reducing respondent burden, their use raises many issues common with those present in web surveys. These include survey design, sample representativeness, privacy, respondent burden, data quality and validation. Workshop participants considered possible areas for future research on these issues and others such as provision of feedback to respondents, linking with big data and focusing on attitudinal and behavioural motivations.}
}
@article{RAGUSEO2021103451,
title = {Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness},
journal = {Information & Management},
volume = {58},
number = {4},
pages = {103451},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103451},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000252},
author = {Elisabetta Raguseo and Federico Pigni and Claudio Vitari},
keywords = {Streams of big data, Process efficiency, Product effectiveness, Competitive advantage},
abstract = {Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.}
}
@article{OZKAN2019208,
title = {Criminology in the age of data explosion: New directions},
journal = {The Social Science Journal},
volume = {56},
number = {2},
pages = {208-219},
year = {2019},
issn = {0362-3319},
doi = {https://doi.org/10.1016/j.soscij.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0362331918301514},
author = {Turgut Ozkan},
keywords = {Social science, Big data, Crime, Social media, Data-driven social science},
abstract = {This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.}
}
@article{LIONO2019196,
title = {QDaS: Quality driven data summarisation for effective storage management in Internet of Things},
journal = {Journal of Parallel and Distributed Computing},
volume = {127},
pages = {196-208},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S074373151830220X},
author = {Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim},
keywords = {Quality of data, Storage management, Internet of Things (IoT), Cloud computing, Quality of service, Data summarisation},
abstract = {The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.}
}
@article{FARROKHI2020257,
title = {Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence},
journal = {Industrial Marketing Management},
volume = {91},
pages = {257-273},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308464},
author = {Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi},
keywords = {Big data, Artificial intelligence, Machine learning, Data mining, Sentiment analytics},
abstract = {Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.}
}
@article{REIS2015238,
title = {Integrating modelling and smart sensors for environmental and human health},
journal = {Environmental Modelling & Software},
volume = {74},
pages = {238-246},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S136481521500167X},
author = {Stefan Reis and Edmund Seto and Amanda Northcross and Nigel W.T. Quinn and Matteo Convertino and Rod L. Jones and Holger R. Maier and Uwe Schlink and Susanne Steinle and Massimo Vieno and Michael C. Wimberly},
keywords = {Integrated modelling, Environmental sensors, Population health, Environmental health, Big data},
abstract = {Sensors are becoming ubiquitous in everyday life, generating data at an unprecedented rate and scale. However, models that assess impacts of human activities on environmental and human health, have typically been developed in contexts where data scarcity is the norm. Models are essential tools to understand processes, identify relationships, associations and causality, formalize stakeholder mental models, and to quantify the effects of prevention and interventions. They can help to explain data, as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results. We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing ‘Big Data’ and, more importantly, make the vital step from ‘Big Data’ to ‘Big Information’. In this paper, we illustrate current developments and identify key research needs using human and environmental health challenges as an example.}
}
@article{ENCINAS2022109904,
title = {Downhole data correction for data-driven rate of penetration prediction modeling},
journal = {Journal of Petroleum Science and Engineering},
volume = {210},
pages = {109904},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109904},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521015217},
author = {Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui},
keywords = {Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks},
abstract = {In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.}
}
@article{KUMAR202185,
title = {Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: an ISM Approach},
journal = {Procedia CIRP},
volume = {98},
pages = {85-90},
year = {2021},
note = {The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121000330},
author = {Pramod Kumar and Jaiprakash Bhamu and Kuldip Singh Sangwan},
keywords = {Industry 4.0, interpretive structural modeling, digital manufacturing, barriers, MICMAC analysis},
abstract = {Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.}
}
@article{SCHAEFER2021156,
title = {Framework of Data Analytics and Integrating Knowledge Management},
journal = {International Journal of Intelligent Networks},
volume = {2},
pages = {156-165},
year = {2021},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666603021000208},
author = {Camilla Schaefer and Ana Makatsaria},
keywords = {Data analytics, Knowledge management, Big data, Business intelligence, Data discovery},
abstract = {Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users’ process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.}
}
@article{XIONG2021386,
title = {Anti-collusion data auction mechanism based on smart contract},
journal = {Information Sciences},
volume = {555},
pages = {386-409},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.10.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520310458},
author = {Wei Xiong and Li Xiong},
keywords = {Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum},
abstract = {Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.}
}
@article{DIVAIO2022121201,
title = {Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121201},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121201},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100634X},
author = {Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine},
keywords = {Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance},
abstract = {This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.}
}
@incollection{TAT2021395,
title = {Chapter 17 - Ethical and legal challenges},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {395-410},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739000178},
author = {Emily Tat and Mark Rabbat},
keywords = {Artificial intelligence, Autonomy, Big data, Black box, Ethics, Informed consent, Liability, Privacy, Safety},
abstract = {As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal “black box” algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.}
}
@article{HARRISON2022103795,
title = {At the limit? Using operational data to estimate train driver human reliability},
journal = {Applied Ergonomics},
volume = {104},
pages = {103795},
year = {2022},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2022.103795},
url = {https://www.sciencedirect.com/science/article/pii/S0003687022001181},
author = {Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw Gibson and Alice Monk},
abstract = {Human reliability analysis plays an important role in the safety assessment and management of rail operations. This paper discusses how the increasing availability of operational data can be used to develop an understanding of train driver reliability. The paper derives human reliability data for two driving tasks, stopping at red signals and controlling speed on approach to buffer stops. In the first of these cases, a tool has been developed that can estimate the number of times a signal is approached at red by trains on the Great Britain (GB) rail network. The tool has been developed using big data techniques and ideas, recording and analysing millions of pieces of data from live operational feeds to update and summarise statistics from thousands of signal locations in GB on a daily basis. The resulting driver reliability data are compared to similar analyses of other train driving tasks. This shows human reliability approaching the currently accepted limits of human performance. It also shows higher error rates amongst freight train drivers than passenger train drivers for these tasks. The paper highlights the importance of understanding the task specific performance limits if further improvements in human reliability are sought. It also provides a practical example of how big data could play an increasingly important role in system error management, whether from the perspective of understanding normal performance and the limits of performance for specific tasks or as the basis for dynamic safety indicators which, if not leading, could at least become closer to real time.}
}
@article{SHARMA2022100383,
title = {Digital Twins: State of the art theory and practice, challenges, and open research questions},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100383},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000516},
author = {Angira Sharma and Edward Kosasih and Jie Zhang and Alexandra Brintrup and Anisoara Calinescu},
keywords = {Digital Twin, Internet of Things, Autonomous systems, Big data, Machine learning},
abstract = {Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation, optimisation and accurate forecasting. However, the theoretical framework and practical implementations of digital twin (DT) are yet to fully achieve this vision at scale. Although an increasing number of successful implementations exist in research and industrial works, sufficient implementation details are not publicly available, making it difficult to fully assess their components and effectiveness, to draw comparisons, identify successful solutions, share lessons, and thus to jointly advance and benefit from the DT methodology. This work first presents a review of relevant DT research and industrial works, focusing on the key DT features, current approaches in different domains, and successful DT implementations, to infer the key DT components and properties, and to identify current limitations and reasons behind the delay in the widespread implementation and adoption of digital twin. This work identifies that the major reasons for this delay are: the fact the DT is still a fast evolving concept; the lack of a universal DT reference framework, e.g. DT standards are scarce and still evolving; problem- and domain-dependence; security concerns over shared data; lack of DT performance metrics; and reliance of digital twin on other fast-evolving technologies. Advancements in machine learning, Internet of Things (IoT) and big data have led to significant improvements in DT features such as real-time monitoring and accurate forecasting. Despite this progress and individual company-based efforts, certain research and implementation gaps exist in the field, which have so far prevented the widespread adoption of the DT concept and technology; these gaps are also discussed in this work. Based on reviews of past work and the identified gaps, this work then defines a conceptualisation of DT which includes its components and properties; these also validate the uniqueness of DT as a concept, when compared to similar concepts such as simulation, autonomous systems and optimisation. Real-life case studies are used to showcase the application of the conceptualisation. This work discusses the state-of-the-art in DT, addresses relevant and timely DT questions, and identifies novel research questions, thus contributing to a better understanding of the DT paradigm and advancing the theory and practice of DT and its allied technologies.}
}
@article{AKTER201985,
title = {Analytics-based decision-making for service systems: A qualitative study and agenda for future research},
journal = {International Journal of Information Management},
volume = {48},
pages = {85-95},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218312696},
author = {Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos},
keywords = {Big data analytics, Decision-making, Service systems},
abstract = {While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.}
}
@article{YAISH20142168,
title = {Multi-tenant Elastic Extension Tables Data Management},
journal = {Procedia Computer Science},
volume = {29},
pages = {2168-2181},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914003792},
author = {Haitham Yaish and Madhu Goyal and George Feuerlicht},
keywords = {Cloud Computing, Software as a Service, Big Data, Elastic Extension Tables, Multi-tenant Database, Relational Tables, Virtual Relational Tables.},
abstract = {Multi-tenant database is a new database solution which is significant for Software as a service (SaaS) and Big Data applications in the context of cloud computing paradigm. This multi-tenant database has significant design challenges to develop a solution that ensures a high level of data quality, accessibility, and manageability for the tenants using this database. In this paper, we propose a multi-tenant data management service called Elastic Extension Tables Schema Handler Service (EETSHS), which is based on a multi-tenant database schema called Elastic Extension Tables (EET). This data management service satisfies tenants’ different business requirements, by creating, managing, organizing, and administratinglarge volumes of structured, semi-structured, and unstructured data. Furthermore, it combines traditional relational data with virtual relational data in a single database schema and allows tenants to manage this data by calling functions from this service. We present algorithms for frequently used functions of this service, and perform several experiments to measure the feasibility and effectiveness of managing multi-tenant data using these functions. We report experimental results of query execution timesfor managing tenants’ virtual and traditional relational data showing that EET schema is a good candidate for the management of multi-tenant data for SaaS and Big Data applications.}
}
@article{ANG20161,
title = {Big Sensor Data Applications in Urban Environments},
journal = {Big Data Research},
volume = {4},
pages = {1-12},
year = {2016},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615300241},
author = {Li-Minn Ang and Kah Phooi Seng},
keywords = {Big data, Sensor-based systems, Survey, Application, Challenges},
abstract = {The emergence of new technologies such as Internet/Web/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.}
}
@article{LIU2021589,
title = {Toward intelligent wireless communications: Deep learning - based physical layer technologies},
journal = {Digital Communications and Networks},
volume = {7},
number = {4},
pages = {589-597},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000742},
author = {Siqi Liu and Tianyu Wang and Shaowei Wang},
keywords = {Data-driven, Deep learning, Physical layer, Wireless communications},
abstract = {Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.}
}
@article{XIA2021100055,
title = {Aiding pro-environmental behavior measurement by Internet of Things},
journal = {Current Research in Behavioral Sciences},
volume = {2},
pages = {100055},
year = {2021},
issn = {2666-5182},
doi = {https://doi.org/10.1016/j.crbeha.2021.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2666518221000425},
author = {Ziqian Xia and Yurong Liu},
keywords = {Pro-environmental behavior, Internet of Things, Measurement, Big data, Environmental psychology},
abstract = {Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.}
}
@article{WANG201714,
title = {GSA: Genome Sequence Archive*},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {15},
number = {1},
pages = {14-18},
year = {2017},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022917300025},
author = {Yanqing Wang and Fuhai Song and Junwei Zhu and Sisi Zhang and Yadong Yang and Tingting Chen and Bixia Tang and Lili Dong and Nan Ding and Qian Zhang and Zhouxian Bai and Xunong Dong and Huanxin Chen and Mingyuan Sun and Shuang Zhai and Yubin Sun and Lei Yu and Li Lan and Jingfa Xiao and Xiangdong Fang and Hongxing Lei and Zhang Zhang and Wenming Zhao},
keywords = {Genome Sequence Archive, GSA, Big data, Raw sequence data, INSDC},
abstract = {With the rapid development of sequencing technologies towards higher throughput and lower cost, sequence data are generated at an unprecedentedly explosive rate. To provide an efficient and easy-to-use platform for managing huge sequence data, here we present Genome Sequence Archive (GSA; http://bigd.big.ac.cn/gsa or http://gsa.big.ac.cn), a data repository for archiving raw sequence data. In compliance with data standards and structures of the International Nucleotide Sequence Database Collaboration (INSDC), GSA adopts four data objects (BioProject, BioSample, Experiment, and Run) for data organization, accepts raw sequence reads produced by a variety of sequencing platforms, stores both sequence reads and metadata submitted from all over the world, and makes all these data publicly available to worldwide scientific communities. In the era of big data, GSA is not only an important complement to existing INSDC members by alleviating the increasing burdens of handling sequence data deluge, but also takes the significant responsibility for global big data archive and provides free unrestricted access to all publicly available data in support of research activities throughout the world.}
}
@article{DEMOULIN2020103120,
title = {Acceptance of text-mining systems: The signaling role of information quality},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103120},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308765},
author = {Nathalie T.M. Demoulin and Kristof Coussement},
keywords = {Technology acceptance model (TAM), Text mining, Big data, Information quality, Top management support},
abstract = {The popularity of the big data domain has boosted corporate interest in collecting and storing tremendous amounts of consumers’ textual information. However, decision makers are often overwhelmed by the abundance of information, and the usage of text mining (TM) tools is still at its infancy. This study validates an extended technology acceptance model integrating information quality (IQ) and top management support. Results confirm that IQ influences behavioral intentions and TM tools usage, through perceptions of external control, perceived ease of use, and perceived usefulness; top management support also has a key role in determining the usage of TM tools.}
}
@article{NASCIMENTO202097,
title = {Estimating record linkage costs in distributed environments},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {97-106},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302756},
author = {Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre},
keywords = {Record linkage, Theoretical model, Data quality, Cloud computing},
abstract = {Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.}
}
@incollection{TONG20221,
title = {Chapter One - Introduction of medical genomics and clinical informatics integration for p-Health care},
editor = {David B. Teplow},
series = {Progress in Molecular Biology and Translational Science},
publisher = {Academic Press},
volume = {190},
number = {1},
pages = {1-37},
year = {2022},
booktitle = {Precision Medicine},
issn = {1877-1173},
doi = {https://doi.org/10.1016/bs.pmbts.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S187711732200062X},
author = {Li Tong and Hang Wu and May D. Wang and Geoffrey Wang},
keywords = {p-Health, Data integration, Clinical informatics, Genomics, Machine learning and artificial intelligence, Biomedical big data analytics},
abstract = {Achieving predictive, precise, participatory, preventive, and personalized health (abbreviated as p-Health) requires comprehensive evaluations of an individual's conditions captured by various measurement technologies. Since the 1950s, analysis of care providers' and physicians' notes and measurement data by computers to improve healthcare delivery has been termed clinical informatics. Since the 2010s, wide adoptions of Electronic Health Records (EHRs) have greatly improved clinical informatics development with fast growing pervasive wearable technologies that continuously capture the human physiological profile in-clinic (EHRs) and out-of-clinic (PHRs or Personal Health Records) to bolster mobile health (mHealth). In addition, after the Human Genome Project in the 1990s, medical genomics has emerged to capture the high-throughput molecular profile of a person. As a result, integrated data analytics is becoming one of the fast-growing areas under Biomedical Big Data to improve human healthcare outcomes. In this chapter, we first introduce the scope of data integration and review applications, data sources, and tools for clinical informatics and medical genomics. We then describe the data integration analytics at the raw data level, feature level, and decision level with case studies, and the opportunity for research and translation using advanced artificial intelligence (AI), such as deep learning. Lastly, we summarize the opportunities in biomedical big data integration that can reshape healthcare toward p-health.}
}
@article{WANG2022111488,
title = {An empirical study on the challenges that developers encounter when developing Apache Spark applications},
journal = {Journal of Systems and Software},
volume = {194},
pages = {111488},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111488},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001674},
author = {Zehao Wang and Tse-Hsun (Peter) Chen and Haoxiang Zhang and Shaowei Wang},
keywords = {Big data system, Empirical study, Stack Overflow},
abstract = {Apache Spark is one of the most popular big data frameworks that abstract the underlying distributed computation details. However, even though Spark provides various abstractions, developers may still encounter challenges related to the peculiarity of distributed computation and environment. To understand the challenges that developers encounter, and provide insight for future studies, in this paper, we conduct an empirical study on the questions that developers encounter. We manually analyze 1,000 randomly selected questions that we collected from Stack Overflow. We find that: 1) questions related to data processing (e.g., transforming data format) are the most common among the 11 types of questions that we uncovered. 2) Even though data processing questions are the most common ones, they require the least amount of time to receive an answer. Questions related to configuration and performance require the most time to receive an answer. 3) Most of the issues are caused by developers’ insufficient knowledge in API usages, data conversation across frameworks, and environment-related configurations. We also discuss the implication of our findings for researchers and practitioners. In summary, our work provides insights for future research directions and highlight the need for more software engineering research in this area.}
}
@article{WANG2022643,
title = {Does city construction improve life quality?-evidence from POI data of China},
journal = {International Review of Economics & Finance},
volume = {80},
pages = {643-653},
year = {2022},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1059056022000041},
author = {Yang Wang and Hong Zhang and Libing Liu},
keywords = {Quality of life, Point of interest, Happiness},
abstract = {To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the "clogging point" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for "meeting the people's increasing needs for a better life".}
}
@incollection{KOLTAY201671,
title = {Chapter 5 - Digital Research Data: Where are we Now?},
editor = {David Baker and Wendy Evans},
booktitle = {Digital Information Strategies},
publisher = {Chandos Publishing},
pages = {71-84},
year = {2016},
isbn = {978-0-08-100251-3},
doi = {https://doi.org/10.1016/B978-0-08-100251-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002513000056},
author = {Tibor Koltay},
keywords = {data citation, data curation, data literacy, data management, data quality, data sharing, research data},
abstract = {The key topic of digital research data raises a multitude of issues: big data, data sharing, data quality, data management, data curation, data citation, data literacy. This chapter addresses questions related to the definition of these concepts, to the frameworks constructed for a better understanding and treatment of the different phenomena, as well as ethical considerations. The potential of libraries and information professionals in fulfilling data-related activities is outlined, together with the associated requirements of them.}
}
@article{SOLANKI2019476,
title = {Towards a knowledge driven framework for bridging the gap between software and data engineering},
journal = {Journal of Systems and Software},
volume = {149},
pages = {476-484},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302772},
author = {Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan},
keywords = {Ontologies, Data engineering, Software engineering, Alignment, Integration},
abstract = {In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.}
}
@article{XIANG20191180,
title = {Dynamic cooperation strategies of the closed-loop supply chain involving the internet service platform},
journal = {Journal of Cleaner Production},
volume = {220},
pages = {1180-1193},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.01.310},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619303373},
author = {Zehua Xiang and Minli Xu},
keywords = {Big data marketing, Differential game, Closed-loop supply chain, Internet service platform},
abstract = {In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.}
}
@article{ESKANDARITORBAGHAN2022106543,
title = {Understanding the potential of emerging digital technologies for improving road safety},
journal = {Accident Analysis & Prevention},
volume = {166},
pages = {106543},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106543},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521005741},
author = {Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund},
keywords = {Safety, Road, Transport, Digital technology, Information},
abstract = {Each year, 1.35 million people are killed on the world’s roads and another 20–50 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data – provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.}
}
@article{DUPLESSIS2021100100,
title = {Necessity of making water smart for proactive informed decisive actions: A case study of the upper vaal catchment, South Africa},
journal = {Environmental Challenges},
volume = {4},
pages = {100100},
year = {2021},
issn = {2667-0100},
doi = {https://doi.org/10.1016/j.envc.2021.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2667010021000792},
author = {Anja {du Plessis}},
keywords = {Data quality, Decisive action, Smart water management, Water quality, South Africa, Upper vaal catchment},
abstract = {The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.}
}
@article{BASIRI2022100587,
title = {Missing data as data},
journal = {Patterns},
volume = {3},
number = {9},
pages = {100587},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100587},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002057},
author = {Anahid Basiri and Chris Brunsdon},
keywords = {missing data, big data paradox, under-representation, bias, crowdsourced data},
abstract = {Our “digified” lives have provided researchers with an unprecedented opportunity to study society at a much higher frequency and granularity. Such data can have a large sample size but can be sparse, biased, and exclusively contributed by the users of the technologies. We look at the increasing importance of missing data and under-representation and propose a new perspective that considers missing data as useful data to understand the underlying reasons for missingness and that provides a realistic view of the sample size of large but under-represented data.}
}
@article{DU2022,
title = {Intelligent Monitoring System Based on Spatio–Temporal Data for Underground Space Infrastructure},
journal = {Engineering},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S209580992200635X},
author = {Bowen Du and Junchen Ye and Hehua Zhu and Leilei Sun and Yanliang Du},
keywords = {Structure health monitoring, Underground space infrastructure, Machine learning, Spatio–temporal data},
abstract = {Intelligent sensing, mechanism understanding, and the deterioration forecasting based on spatio–temporal big data not only promote the safety of the infrastructure but also indicate the basic theory and key technology for the infrastructure construction to turn to intelligentization. The advancement of underground space utilization has led to the development of three characteristics (deep, big, and clustered) that help shape a tridimensional urban layout. However, compared to buildings and bridges overground, the diseases and degradation that occur underground are more insidious and difficult to identify. Numerous challenges during the construction and service periods remain. To address this gap, this paper summarizes the existing methods and evaluates their strong points and weak points based on real-world space safety management. The key scientific issues, as well as solutions, are discussed in a unified intelligent monitoring system.}
}
@article{ZORRILLA2022103595,
title = {A reference framework for the implementation of data governance systems for industry 4.0},
journal = {Computer Standards & Interfaces},
volume = {81},
pages = {103595},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000908},
author = {Marta Zorrilla and Juan Yebenes},
keywords = {Data governance, Data-Centric architecture, Industry 4.0, Big data, IoT},
abstract = {The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.}
}
@article{RAMSINGH2021107423,
title = {An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm},
journal = {Applied Soft Computing},
volume = {108},
pages = {107423},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107423},
url = {https://www.sciencedirect.com/science/article/pii/S156849462100346X},
author = {J. Ramsingh and V. Bhuvaneswari},
keywords = {Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus},
abstract = {In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.}
}
@article{MOLL2019100833,
title = {The role of internet-related technologies in shaping the work of accountants: New directions for accounting research},
journal = {The British Accounting Review},
volume = {51},
number = {6},
pages = {100833},
year = {2019},
note = {Innovative Governance and Sustainable Pathways in a Disruptive Environment},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0890838919300459},
author = {Jodie Moll and Ogan Yigitbasioglu},
keywords = {Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence},
abstract = {This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.}
}
@article{WANG2018139,
title = {Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data},
journal = {Computers & Geosciences},
volume = {111},
pages = {139-147},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S009830041730609X},
author = {Ruo-Qian Wang and Huina Mao and Yuan Wang and Chris Rae and Wesley Shaw},
abstract = {Hyper-resolution datasets for urban flooding are rare. This problem prevents detailed flooding risk analysis, urban flooding control, and the validation of hyper-resolution numerical models. We employed social media and crowdsourcing data to address this issue. Natural Language Processing and Computer Vision techniques are applied to the data collected from Twitter and MyCoast (a crowdsourcing app). We found these big data based flood monitoring approaches can complement the existing means of flood data collection. The extracted information is validated against precipitation data and road closure reports to examine the data quality. The two data collection approaches are compared and the two data mining methods are discussed. A series of suggestions is given to improve the data collection strategy.}
}
@article{ZHANG20221,
title = {Application of machine learning, deep learning and optimization algorithms in geoengineering and geoscience: Comprehensive review and future challenge},
journal = {Gondwana Research},
volume = {109},
pages = {1-17},
year = {2022},
issn = {1342-937X},
doi = {https://doi.org/10.1016/j.gr.2022.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S1342937X2200123X},
author = {Wengang Zhang and Xin Gu and Libin Tang and Yueping Yin and Dongsheng Liu and Yanmei Zhang},
keywords = {Machine learning, Deep learning, Optimization algorithms, Geoengineering and geoscience, VOSviewer},
abstract = {The so-called Fourth Paradigm has witnessed a boom during the past two decades, with large volumes of observational data becoming available to scientists and engineers. Big data is characterized by the rule of the five Vs: Volume, Variety, Value, Velocity and Veracity. The concept of big data naturally matches well with the features of geoengineering and geoscience. Large-scale, comprehensive, multidirectional and multifield geotechnical data analysis is becoming a trend. On the other hand, Machine learning (ML), Deep Learning (DL) and Optimization Algorithm (OA) provide the ability to learn from data and deliver in-depth insight into geotechnical problems. Researchers use different ML, DL and OA models to solve various problems associated with geoengineering and geoscience. Consequently, there is a need to extend its research with big data research through integrating the use of ML, DL and OA techniques. This work focuses on a systematic review on the state-of-the-art application of ML, DL and OA algorithms in geoengineering and geoscience. Various ML, DL, and OA approaches are firstly concisely introduced, concerning mainly the supervised learning, unsupervised learning, deep learning and optimization algorithms. Then their representative applications in the geoengineering and geoscience are summarized via VOSviewer demonstration. The authors also provided their own thoughts learnt from these applications as well as work ongoing and future recommendations. This review paper aims to make a comprehensive summary and provide fundamental guidelines for researchers and engineers in the discipline of geoengineering and geoscience or similar research areas on how to integrate and apply ML, DL and OA methods.}
}
@article{MARTINEZ2021100183,
title = {Data Science Methodologies: Current Challenges and Future Approaches},
journal = {Big Data Research},
volume = {24},
pages = {100183},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300514},
author = {Iñigo Martinez and Elisabeth Viles and Igor {G. Olaizola}},
keywords = {Data science, Big data, Data science methodology, Project life-cycle, Organizational impacts, Knowledge management},
abstract = {Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.}
}
@article{NARASIMHULU2022104690,
title = {HIGH PERFORMANCE SOCIAL DATA COMPUTING WITH DEVELOPMENT OF INTELLIGENT TOPIC MODELS FOR HEALTHCARE},
journal = {Microprocessors and Microsystems},
pages = {104690},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104690},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002204},
author = {K Narasimhulu and K.T. {Meena Abarna}},
keywords = {Topic Model, Ailments Aspects, ATAM, Healthcare Topic Model, Social Recommended Healthcare Results},
abstract = {Data mining and big data computing are the emerging domains in the current era of predictions for societal applications. Millions of people are interested in sharing their views through tweets. Healthcare predictions are one of the attractive researches in big data social mining. Healthcare predictions are derived by implementing topic models by the ailments data. An ailment refers to either illness or sign of a particular health problem. Millions of tweets are collected based on conditions and assessed with ailment topic aspect models. The existing topic model, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing, Probabilistic LSI (PLSI), limits the healthcare results assessment concerning any one of the ailments aspects. Recent ailments topic aspect model (ATAM) overcome the problems of these topic models and delivers the healthcare assessment results concerning the fundamental aspects of ailments data except side-effects analysis of treatments. The scalability performance of ATAM is degraded in showing healthcare results over the massive amounts of health data. A high-performance computing model of ATAM has been developed in the distributed environment to address scalability. Its intelligent model is designed in the cloud and multi-node Hadoop environment to deliver high-performance social computing results for healthcare. Experiments are conducted on many comparative studies is demonstrated between the existing and proposed high-performance models using the massive amount of health-related tweets concerning the ailments aspects.}
}
@article{SKAF2022104082,
title = {Topological data analysis in biomedicine: A review},
journal = {Journal of Biomedical Informatics},
volume = {130},
pages = {104082},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104082},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000983},
author = {Yara Skaf and Reinhard Laubenbacher},
keywords = {Biomedical informatics, Personalized medicine, Big data analytics, Topological data analysis, TDA, Applied topology, Mapper, Persistent homology, Machine learning},
abstract = {Significant technological advances made in recent years have shepherded a dramatic increase in utilization of digital technologies for biomedicine– everything from the widespread use of electronic health records to improved medical imaging capabilities and the rising ubiquity of genomic sequencing contribute to a “digitization” of biomedical research and clinical care. With this shift toward computerized tools comes a dramatic increase in the amount of available data, and current tools for data analysis capable of extracting meaningful knowledge from this wealth of information have yet to catch up. This article seeks to provide an overview of emerging mathematical methods with the potential to improve the abilities of clinicians and researchers to analyze biomedical data, but may be hindered from doing so by a lack of conceptual accessibility and awareness in the life sciences research community. In particular, we focus on topological data analysis (TDA), a set of methods grounded in the mathematical field of algebraic topology that seeks to describe and harness features related to the “shape” of data. We aim to make such techniques more approachable to non-mathematicians by providing a conceptual discussion of their theoretical foundations followed by a survey of their published applications to scientific research. Finally, we discuss the limitations of these methods and suggest potential avenues for future work integrating mathematical tools into clinical care and biomedical informatics.}
}
@incollection{CHUI2019111,
title = {Chapter 7 - Smart city is a safe city: information and communication technology–enhanced urban space monitoring and surveillance systems: the promise and limitations},
editor = {Anna Visvizi and Miltiadis D. Lytras},
booktitle = {Smart Cities: Issues and Challenges},
publisher = {Elsevier},
pages = {111-124},
year = {2019},
isbn = {978-0-12-816639-0},
doi = {https://doi.org/10.1016/B978-0-12-816639-0.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166390000077},
author = {Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu},
keywords = {Cyber security, Ethics, Policy-making, Security, Surveillance},
abstract = {Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.}
}
@article{KAZMIERSKA202043,
title = {From multisource data to clinical decision aids in radiation oncology: The need for a clinical data science community},
journal = {Radiotherapy and Oncology},
volume = {153},
pages = {43-54},
year = {2020},
note = {Physics Special Issue: ESTRO Physics Research Workshops on Science in Development},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2020.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S016781402030829X},
author = {Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso},
keywords = {Artificial intelligence, Big data, Data science, Personalized treatment, Radiotherapy, Shared decision making},
abstract = {Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.}
}
@article{LIU2022103138,
title = {Effects of governmental data governance on urban fire risk: A city-wide analysis in China},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103138},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103138},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003570},
author = {Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas},
keywords = {Urban fire risk, Fire risk management, Big data technologies, Data governance, Socio-economic factors, City-wide analysis},
abstract = {The effects of data governance (as a means to maximize big data value creation in fire risk management) performance on fire risk was analyzed based on multi-source statistical data of 105 cities in China from 2016 to 2018. Specifically, data governance was first quantified with ten detailed indicators, which were then selected for explaining urban fire risk through correlation analysis. Next, the sample cities were clustered in terms of major socio-economic characteristics, and then the effects of data governance were examined by constructing multivariate regression models for each city cluster with ordinary least squares (OLS). The results showed that the constructed regression models produced good interpretation of fire risk in different types of cities, with coefficient of determination (R2) in each model exceeding 0.65. Among the indicators, the development of infrastructures (e.g. data collection devices and data analysis platforms), the level of data use, and the updating of fire risk related data were proved to produce significant effects on the reduction of fire frequency and fire consequence. Moreover, the organizational maturity of data governance was proved to be helpful in reducing fire frequency. For the cities with large population, the cross-department sharing of high-value data was found to be another important determinant of urban fire frequency. In comparison with existing statistical models which interpreted fire risk with general social factors (with the highest R2 = 0.60), these new regression models presented a better statistical performance (with the average R2 = 0.72). These findings are expected to provide decision support for the local governments of China and other jurisdictions to facilitate big data projects in improving fire risk management.}
}
@article{LIU2020101495,
title = {Discovering and merging related analytic datasets},
journal = {Information Systems},
volume = {91},
pages = {101495},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101495},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300065},
author = {Rutian Liu and Eric Simon and Bernd Amann and Stéphane Gançarski},
keywords = {Schema augmentation, Schema complement, Data quality, SAP HANA},
abstract = {The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.}
}
@article{CAO2020107850,
title = {Online investigation of vibration serviceability limitations using smartphones},
journal = {Measurement},
volume = {162},
pages = {107850},
year = {2020},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.107850},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120303882},
author = {Lei Cao and Jun Chen},
keywords = {Vibration serviceability, Online sampling, Big data, Data cleaning},
abstract = {Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.}
}
@article{EICHSTADT2021100232,
title = {Metrology for the digital age},
journal = {Measurement: Sensors},
volume = {18},
pages = {100232},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421001951},
author = {Sascha Eichstädt and Anke Keidel and Julia Tesch},
keywords = {Digital transformation, Digital certificates, Systemic metrology, Big data, Industry 4.0, Open science, FAIR},
abstract = {Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.}
}
@article{BALTI20221,
title = {Multidimensional architecture using a massive and heterogeneous data: Application to drought monitoring},
journal = {Future Generation Computer Systems},
volume = {136},
pages = {1-14},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001753},
author = {Hanen Balti and Ali Ben Abbes and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Big data, Data storage, Spatio-temporal querying, Decision-making, Earth observation, Disaster management},
abstract = {The rapid increase in the number of Earth Observation (EO) systems generates a massive amount of heterogeneous data. It has raised big issues in collecting, preprocessing, storing, and the visualization these data. However, traditional techniques are facing serious challenges when dealing with big EO data dimensions (i.e., Volume, Veracity, Variety, and Velocity), especially in natural hazards management. Therefore, big data techniques and tools attract more attention. In this paper we propose a multidimensional model framework for Big EO data warehousing. This framework includes 3 parts: (1) Data collection and preprocessing, being responsible for collecting data and improving their quality; (2) Data loading and storage, performing the ingestion task which consists of transferring the data from external resources to the Big data platform for storage; and (3) Visualization and interpretation, aiming to provide spatio-temporal analysis. This framework could be useful for decision-makers in monitoring the effects of drought disasters and, consequently, planning the mitigation and remediation measures. Experiments are carried out on drought monitoring in China along the period 2000–2020. The input data include remote sensing data, biophysical data, and climatological data. The results reveal that the proposed framework has a higher retrieval speed and a greater elasticity with different kinds (i.e. spatial, temporal, or spatiotemporal) of requests compared to traditional frameworks, indicating its superiority.}
}