@incollection{SIMON201577,
title = {Chapter 8 - Considerations for the Big Data Era},
editor = {Alan Simon},
booktitle = {Enterprise Business Intelligence and Data Warehousing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {77-82},
year = {2015},
isbn = {978-0-12-801540-7},
doi = {https://doi.org/10.1016/B978-0-12-801540-7.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128015407000081},
author = {Alan Simon},
keywords = {Big Data, business intelligence, BI, analytics, predictive analytics, program manager, program management, project manager, project management, challenges, data warehouse, data warehousing, enterprise data warehouse, EDW},
abstract = {The Big Data era is creating seismic shifts in how we approach enterprise business intelligence and data warehousing. This final chapter discusses considerations related to technology and architecture, analytics-oriented requirements collection, and organizational structure.}
}
@article{CHEN201798,
title = {Data quality of electricity consumption data in a smart grid environment},
journal = {Renewable and Sustainable Energy Reviews},
volume = {75},
pages = {98-105},
year = {2017},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2016.10.054},
url = {https://www.sciencedirect.com/science/article/pii/S1364032116307109},
author = {Wen Chen and Kaile Zhou and Shanlin Yang and Cheng Wu},
keywords = {Electricity consumption data, Data quality, Outlier detection, Outlier data, Smart grid},
abstract = {With the increasing penetration of traditional and emerging information technologies in the electric power industry, together with the rapid development of electricity market reform, the electric power industry has accumulated a large amount of data. Data quality issues have become increasingly prominent, which affect the accuracy and effectiveness of electricity data mining and energy big data analytics. It is also closely related to the safety and reliability of the power system operation and management based on data-driven decision support. In this paper, we study the data quality of electricity consumption data in a smart grid environment. First, we analyze the significance of data quality. Also, the definition and classification of data quality issues are explained. Then we analyze the data quality of electricity consumption data and introduce the characteristics of electricity consumption data in a smart grid environment. The data quality issues of electricity consumption data are divided into three types, namely noise data, incomplete data and outlier data. We make a detailed discussion on these three types of data quality issues. In view of that outlier data is one of the most prominent issues in electricity consumption data, so we mainly focus on the outlier detection of electricity consumption data. This paper introduces the causes of electricity consumption outlier data and illustrates the significance of the electricity consumption outlier data from the negative and positive aspects respectively. Finally, the focus of this paper is to provide a review on the detection methods of electricity consumption outlier data. The methods are mainly divided into two categories, namely the data mining-based and the state estimation-based methods.}
}
@article{WANG2015782,
title = {Privacy trust crisis of personal data in China in the era of Big Data: The survey and countermeasures},
journal = {Computer Law & Security Review},
volume = {31},
number = {6},
pages = {782-792},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364915001296},
author = {Zhong Wang and Qian Yu},
keywords = {Personal data, Privacy trust, Questionnaires, Interview, Big data},
abstract = {Privacy trust directly affects the personal willingness to share data and thus influences the quality and size of the data, thus affecting the development of big data technology and industry. As China is probably the largest personal data pool and vastest application market of big data, the situation of Chinese privacy trust plays a significant role. Based on the 17 most common data collection scenarios, the following aspects have been observed through 508 questionnaires and interviews of 20 samples. To start with, there is a severe privacy trust crisis in China, both in the field of enterprise services such as online shopping and social networks, etc. and in some public services like medical care and education, etc. Besides, there are also doubts about data collected by the government since individuals refuse to offer personal information or give false information as much as possible. Some people even buy two phone numbers, one is in use, while the other is not carried around or used by them, which is only bought to be offered to data collectors. Secondly, in terms of gender, females have lower trust in enterprises and social associations than males, especially in the fields of social networks and personal consumption. However, there is no obvious difference in fields of government and public services. Females possess stronger awareness but less skilled in precautions than males. Thirdly, people between the ages of 18 and 50 are more suspicious of data collected by enterprises, while age exerts little obvious influence on the credibility of data collected by the government, social associations and public services. Older people are less aware of precautions than people at other ages. In addition, from the perspective of education background, people with higher degrees possess stronger awareness of precautions and thus lower degree of trust. Therefore, it is suggested that more education on privacy consciousness should be given, and relative laws as well as regulations need improving. Besides, innovation in privacy protection technologies should be encouraged. What is more, we need to reinforce the management of the internet industry and strictly regulate personal data collection of the government.}
}
@article{KNEPPER20151504,
title = {Big Data on Ice: The Forward Observer System for In-flight Synthetic Aperture Radar Processing},
journal = {Procedia Computer Science},
volume = {51},
pages = {1504-1513},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.340},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915011485},
author = {Richard Knepper and Matthew Standish and Matthew Link},
keywords = {Big Data, Network Filesystems, Synthetic Aperture Radar, Ice Sheet Data},
abstract = {We introduce the Forward Observer system, which is designed to provide data assurance in field data acquisition while receiving significant amounts (several terabytes per flight) of Synthetic Aperture Radar data during flights over the polar regions, which provide unique requirements for developing data collection and processing systems. Under polar conditions in the field and given the difficulty and expense of collecting data, data retention is absolutely critical. Our system provides a storage and analysis cluster with software that connects to field instruments via standard protocols, replicates data to multiple stores automatically as soon as it is written, and provides pre-processing of data so that initial visualizations are available immediately after collection, where they can provide feedback to researchers in the aircraft during the flight.}
}
@article{ZHAO20171085,
title = {An optimization model for green supply chain management by using a big data analytic approach},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1085-1097},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616300579},
author = {Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang},
keywords = {Hazardous materials, Inherent risk, Carbon emissions, Multi-objective optimization, Green supply chain management, Big data analysis},
abstract = {This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.}
}
@incollection{KRISHNAN2013199,
title = {Chapter 10 - Integration of Big Data and Data Warehousing},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {199-217},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000106},
author = {Krish Krishnan},
keywords = {Big Data, Big Data appliances, Hadoop, NoSQL, RDBMS, data virtualization, semantic framework},
abstract = {The focus of this chapter is to discuss the integration of Big Data and the data warehouse, the possible techniques and pitfalls, and where we leverage a technology. How do we deal with complexity and heterogeneity of technologies? What are the performance and scalabilities of each technology, and how can we sustain performance for the new environment?}
}
@article{HAZEN2016592,
title = {Big data and predictive analytics for supply chain sustainability: A theory-driven research agenda},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {592-598},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S036083521630225X},
author = {Benjamin T. Hazen and Joseph B. Skipper and Jeremy D. Ezell and Christopher A. Boone},
keywords = {Big data, Predictive analytics, Supply chain management},
abstract = {Big data and predictive analytics (BDPA) tools and methodologies are leveraged by businesses in many ways to improve operational and strategic capabilities, and ultimately, to positively impact corporate financial performance. BDPA has become crucial for managing supply chain functions, where data intensive processes can be vastly improved through its effective use. BDPA has also become a competitive necessity for the management of supply chains, with practitioners and scholars focused almost entirely on how BDPA is used to increase economic measures of performance. There is limited understanding, however, as to how BDPA can impact other aspects of the triple bottom-line, namely environmental and social sustainability outcomes. Indeed, this area is in immediate need of attention from scholars in many fields including industrial engineering, supply chain management, information systems, business analytics, as well as other business and engineering disciplines. The purpose of this article is to motivate such research by proposing an agenda based in well-established theory. This article reviews eight theories that can be used by researchers to examine and clarify the nature of BDPA’s impact on supply chain sustainability, and presents research questions based upon this review. Scholars can leverage this article as the basis for future research activity, and practitioners can use this article as a means to understand how company-wide BDPA initiatives might impact measures of supply chain sustainability.}
}
@incollection{EBBELS2019329,
title = {Chapter 11 - Big Data and Databases for Metabolic Phenotyping},
editor = {John C. Lindon and Jeremy K. Nicholson and Elaine Holmes},
booktitle = {The Handbook of Metabolic Phenotyping},
publisher = {Elsevier},
pages = {329-367},
year = {2019},
isbn = {978-0-12-812293-8},
doi = {https://doi.org/10.1016/B978-0-12-812293-8.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122938000116},
author = {Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen},
keywords = {Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing, High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal, Social implications, ELSI},
abstract = {Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure “PhenoMeNal,” which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.}
}
@article{SIVARAJAH2017263,
title = {Critical analysis of Big Data challenges and analytical methods},
journal = {Journal of Business Research},
volume = {70},
pages = {263-286},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S014829631630488X},
author = {Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody},
keywords = {Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review},
abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.}
}
@incollection{FIESCHI2018197,
title = {16 - Data for Epidemiology and Public Health, and Big Data11The questions posed by data processing for epidemiology and public health are often similar to those discussed in the chapters on clinical research (Chapter 18) and bioinformatics data (Chapter 17). For the sake of clarity, we address these questions in different chapters, although the problems are of the same nature and the solutions are isomorphic. In order to avoid too much repetition, the issue of big data is discussed here without going into the content of the other chapters.},
editor = {Marius Fieschi},
booktitle = {Health Data Processing},
publisher = {Elsevier},
pages = {197-212},
year = {2018},
isbn = {978-1-78548-287-8},
doi = {https://doi.org/10.1016/B978-1-78548-287-8.50016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548287850016X},
author = {Marius Fieschi},
keywords = {Data processing, Data-sharing, e-health, Epidemiology, Health security, Monitoring systems, Preventive action, Public health, SurSaUD system},
abstract = {Abstract:
The approaches used by epidemiologists are diverse: they range from “field studies” for modeling and healthcare monitoring, to methods developed for researching and combating the emergence of diseases. Their analytical tools focus on the bio-statistics used as a tool to objectify phenomena studied in well-defined populations.}
}
@article{LIN2018220,
title = {An on-demand coverage based self-deployment algorithm for big data perception in mobile sensing networks},
journal = {Future Generation Computer Systems},
volume = {82},
pages = {220-234},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313262},
author = {Yaguang Lin and Xiaoming Wang and Fei Hao and Liang Wang and Lichen Zhang and Ruonan Zhao},
keywords = {Mobile sensing network, High performance sensing, Big data perception, Node self-deployment, On-demand coverage, Mobile cellular learning automata},
abstract = {Mobile Sensing Networks have been widely applied to many fields for big data perception such as intelligent transportation, medical health and environment sensing. However, in some complex environments and unreachable regions of inconvenience for human, the establishment of the mobile sensing networks, the layout of the nodes and the control of the network topology to achieve high performance sensing of big data are increasingly becoming a main issue in the applications of the mobile sensing networks. To deal with this problem, we propose a novel on-demand coverage based self-deployment algorithm for big data perception based on mobile sensing networks in this paper. Firstly, by considering characteristics of mobile sensing nodes, we extend the cellular automata model and propose a new mobile cellular automata model for effectively characterizing the spatial–temporal evolutionary process of nodes. Secondly, based on the learning automata theory and the historical information of node movement, we further explore a new mobile cellular learning automata model, in which nodes can self-adaptively and intelligently decide the best direction of movement with low energy consumption. Finally, we propose a new optimization algorithm which can quickly solve the node self-adaptive deployment problem, thus, we derive the best deployment scheme of nodes in a short time. The extensive simulation results show that the proposed algorithm in this paper outperforms the existing algorithms by as much as 40% in terms of the degree of satisfaction of network coverage, the iterations of the algorithm, the average moving steps of nodes and the energy consumption of nodes. Hence, we believe that our work will make contributions to large-scale adaptive deployment and high performance sensing scenarios of the mobile sensing networks.}
}
@incollection{CARNICERO2019121,
title = {Chapter 8 - Healthcare Decision-Making Support Based on the Application of Big Data to Electronic Medical Records: A Knowledge Management Cycle},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {121-131},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000083},
author = {Javier Carnicero and David Rojas},
keywords = {Big Data, Electronic medical record, Practice-based medicine, Learning health system, Semantic interoperability},
abstract = {Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.}
}
@article{BARASH201510,
title = {Harnessing big data for precision medicine: A panel of experts elucidates the data challenges and proposes key strategic decisions points},
journal = {Applied & Translational Genomics},
volume = {4},
pages = {10-13},
year = {2015},
issn = {2212-0661},
doi = {https://doi.org/10.1016/j.atg.2015.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212066115000046},
author = {Carol Isaacson Barash and Keith O. Elliston and W. {Andrew Faucett} and Jonathan Hirsch and Gauri Naik and Alice Rathjen and Grant Wood},
abstract = {A group of disparate translational bioinformatics experts convened at the 6th Annual Precision Medicine Partnership Meeting, October 29–30, 2014 to discuss big data challenges and key strategic decisions needed to advance precision medicine, emerging solutions, and the anticipated path to success. This article reports the panel discussion.}
}
@article{SHUKLA20191015,
title = {Next generation smart sustainable auditing systems using Big Data Analytics: Understanding the interaction of critical barriers},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1015-1026},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301992},
author = {Manish Shukla and Lana Mattar},
keywords = {Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive Structural Modelling},
abstract = {In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.}
}
@article{ABBASIAN201829,
title = {Improving early OSV design robustness by applying ‘Multivariate Big Data Analytics’ on a ship's life cycle},
journal = {Journal of Industrial Information Integration},
volume = {10},
pages = {29-38},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300869},
author = {Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett},
keywords = {External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering},
abstract = {Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.}
}
@article{ZUO2018839,
title = {Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing},
journal = {Environmental Pollution},
volume = {240},
pages = {839-847},
year = {2018},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2018.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0269749118307681},
author = {JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong},
keywords = {Indoor PM, Infiltration factor, Indoor/outdoor ratio, Beijing},
abstract = {Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5 μm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6 ± 18.4 μg/m3. Specifically, the concentration in non-heating season was 34.9 ± 15.8 μg/m3, which was 24% lower than that in heating season (46.1 ± 21.2 μg/m3). A significant correlation between indoor and ambient PM2.5 (p < 0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor/outdoor (I/O) ratio was estimated to be 0.73 ± 0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8 ± 27.4 μg/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.}
}
@article{VETRO2021101619,
title = {A data quality approach to the identification of discrimination risk in automated decision making systems},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101619},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101619},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000551},
author = {Antonio Vetrò and Marco Torchiano and Mariachiara Mecati},
keywords = {Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance},
abstract = {Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation – or even amplification – of bias in the input data of ADM systems.}
}
@incollection{KRISHNAN2013219,
title = {Chapter 11 - Data-Driven Architecture for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {219-240},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00011-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000118},
author = {Krish Krishnan},
keywords = {metadata, master data, machine learning, algorithms, semantic libraries, data governance},
abstract = {The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.}
}
@incollection{TALBURT20151,
title = {Chapter 1 - The Value Proposition for MDM and Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-16},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000016},
author = {John R. Talburt and Yinle Zhou},
keywords = {Master data, master data management, MDM, Big Data, reference data management, RDM},
abstract = {This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.}
}
@article{SEO201969,
title = {A pilot infrastructure for searching rainfall metadata and generating rainfall product using the big data of NEXRAD},
journal = {Environmental Modelling & Software},
volume = {117},
pages = {69-75},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218307667},
author = {Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski},
keywords = {NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology},
abstract = {The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.}
}
@article{BENDLE2016115,
title = {Uncovering the message from the mess of big data},
journal = {Business Horizons},
volume = {59},
number = {1},
pages = {115-124},
year = {2016},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681315001408},
author = {Neil T. Bendle and Xin (Shane) Wang},
keywords = {Big data, User-Generated content, Latent Dirichlet Allocation, Topic modeling, Market research, Qualitative data},
abstract = {User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses’ strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers’ minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.}
}
@article{YANG2019755,
title = {How big data enriches maritime research – a critical review of Automatic Identification System (AIS) data applications},
journal = {Transport Reviews},
volume = {39},
number = {6},
pages = {755-773},
year = {2019},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2019.1649315},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722001568},
author = {Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li},
keywords = {AIS data, data mining, navigation safety, ship behaviour analysis, environmental evaluation, advanced applications of AIS data},
abstract = {ABSTRACT
The information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.}
}
@article{SCHULER2019191,
title = {Big Data Readiness in Radiation Oncology: An Efficient Approach for Relabeling Radiation Therapy Structures With Their TG-263 Standard Name in Real-World Data Sets},
journal = {Advances in Radiation Oncology},
volume = {4},
number = {1},
pages = {191-200},
year = {2019},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S2452109418302240},
author = {Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill and Brooke Evans and Blanca Gallego},
abstract = {Purpose
To prepare for big data analyses on radiation therapy data, we developed Stature, a tool-supported approach for standardization of structure names in existing radiation therapy plans. We applied the widely endorsed nomenclature standard TG-263 as the mapping target and quantified the structure name inconsistency in 2 real-world data sets.
Methods and Materials
The clinically relevant structures in the radiation therapy plans were identified by reference to randomized controlled trials. The Stature approach was used by clinicians to identify the synonyms for each relevant structure, which was then mapped to the corresponding TG-263 name. We applied Stature to standardize the structure names for 654 patients with prostate cancer (PCa) and 224 patients with head and neck squamous cell carcinoma (HNSCC) who received curative radiation therapy at our institution between 2007 and 2017. The accuracy of the Stature process was manually validated in a random sample from each cohort. For the HNSCC cohort we measured the resource requirements for Stature, and for the PCa cohort we demonstrated its impact on an example clinical analytics scenario.
Results
All but 1 synonym group (“Hydrogel”) was mapped to the corresponding TG-263 name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures were exact matches (ie, following local naming convention), 284 were matched via a synonym, and 3 required manual matching. This original radiation therapy structure names therefore had a naming inconsistency rate of 4.81%. For the HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym, 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The clinician hours required were one-fifth of those that would be required for manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).
Conclusions
The Stature approach was highly accurate and had significant resource efficiencies compared with manual curation.}
}
@article{DREWER2017298,
title = {The BIG DATA Challenge: Impact and opportunity of large quantities of information under the Europol Regulation},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {298-308},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300699},
author = {Daniel Drewer and Vesela Miladinova},
keywords = {Europol, Big data, Privacy, Data protection, Data protection impact assessment, Risk assessment, Privacy by design, Advanced technologies, Europol Regulation, Integrated Data Management Concept (IDMC)},
abstract = {In the digital age, the interaction between privacy, data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the “paradox” of on one side protecting fundamental human rights against external terrorist and/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods.}
}
@incollection{2022vii,
title = {In praise of Meeting the Challenges of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {vii-ix},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500016X}
}
@article{RICHTER201537,
title = {Medicinal chemistry in the era of big data},
journal = {Drug Discovery Today: Technologies},
volume = {14},
pages = {37-41},
year = {2015},
note = {From Chemistry to Biology Database Curation},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2015.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1740674915000141},
author = {Lars Richter and Gerhard F. Ecker},
abstract = {In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.}
}
@incollection{KRESS201911,
title = {Big Data for Ecological Models},
editor = {Brian Fath},
booktitle = {Encyclopedia of Ecology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {11-20},
year = {2019},
isbn = {978-0-444-64130-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10557-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105573},
author = {Marin M. Kress},
keywords = {Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery, Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine readable, Metadata, Remote sensing, Social media},
abstract = {The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher׳s work, and more importantly, facilitate transparency and repeatability of a researcher׳s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of “supporting online material” (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.}
}
@article{KOZJEK2018209,
title = {Big data analytics for operations management in engineer-to-order manufacturing},
journal = {Procedia CIRP},
volume = {72},
pages = {209-214},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118302531},
author = {Dominik Kozjek and Rok Vrabič and Borut Rihtaršič and Peter Butala},
keywords = {Engineer-to-order manufacturing, Operations management, Data analytics, Industrial data, Data mining, Big data},
abstract = {Manufacturing data offers big potential for improving management of manufacturing operations. The paper addresses an approach to data analytics in engineer-to-order (ETO) manufacturing systems where the product quality and due-date reliability play a key role in management decisionmaking. The objective of the research is to investigate manufacturing data which are collected by a manufacturing execution system (MES) during operations in an ETO enterprise and to develop tools for supporting scheduling of operations. The developed tools can be used for simulation of production and forecasting of potential resource overloads.}
}
@article{DAMIANI20181,
title = {Large databases (Big Data) and evidence-based medicine},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {1-2},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518301961},
author = {Andrea Damiani and Graziano Onder and Vincenzo Valentini}
}
@article{KOBUSINSKA20181321,
title = {Big Data fingerprinting information analytics for sustainability},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1321-1337},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.061},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329965},
author = {Anna Kobusińska and Kamil Pawluczuk and Jerzy Brzeziński},
keywords = {Big Data, Fingerprinting, Web tracking, Security, Analytics},
abstract = {Web-based device fingerprinting is the process of collecting security information through the browser to perform stateless device identification. Fingerprints may then be used to identify and track computing devices in the web. There are various reasons why device-related information may be needed. Among the others, this technique could help to efficiently analyze security information for sustainability. In this paper we introduce a fingerprinting analytics tool that discovers the most appropriate device fingerprints and their corresponding optimal implementations. The fingerprints selected in the result of the performed analysis are used to enrich and improve an open-source fingerprinting analytics tool Fingerprintjs2, daily consumed by hundreds of websites. As a result, the paper provides a noticeable progress in analytics of dozens of values of device fingerprints, and enhances analysis of fingerprints security information.}
}
@article{BIBAULT2016110,
title = {Big Data and machine learning in radiation oncology: State of the art and future prospects},
journal = {Cancer Letters},
volume = {382},
number = {1},
pages = {110-117},
year = {2016},
issn = {0304-3835},
doi = {https://doi.org/10.1016/j.canlet.2016.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0304383516303469},
author = {Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun},
keywords = {Radiation oncology, Big Data, Predictive model, Machine learning},
abstract = {Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.}
}
@article{SONG2019288,
title = {Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data},
journal = {Environmental Pollution},
volume = {253},
pages = {288-296},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.06.057},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930418X},
author = {Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood},
keywords = {Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health},
abstract = {In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.}
}
@article{JANKE2016227,
title = {Exploring the Potential of Predictive Analytics and Big Data in Emergency Care},
journal = {Annals of Emergency Medicine},
volume = {67},
number = {2},
pages = {227-236},
year = {2016},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2015.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0196064415005302},
author = {Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy},
abstract = {Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.}
}
@article{BABAR2018155,
title = {Energy-harvesting based on internet of things and big data analytics for smart health monitoring},
journal = {Sustainable Computing: Informatics and Systems},
volume = {20},
pages = {155-164},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917302238},
author = {Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon},
keywords = {Big data analytics, IoT, Energy harvesting},
abstract = {Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.}
}
@article{RAJAN2019193,
title = {Towards a content agnostic computable knowledge repository for data quality assessment},
journal = {Computer Methods and Programs in Biomedicine},
volume = {177},
pages = {193-201},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306254},
author = {Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli},
keywords = {Data Quality Metadata Repository, Knowledge representation, Data quality assessment, Data quality dimensions, Data quality framework},
abstract = {Background and objective
In recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.
Methods
We identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.
Results
We identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.
Conclusion
We identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.}
}
@article{SARAN2017713,
title = {The China Kidney Disease Network (CK-NET): “Big Data—Big Dreams”},
journal = {American Journal of Kidney Diseases},
volume = {69},
number = {6},
pages = {713-716},
year = {2017},
issn = {0272-6386},
doi = {https://doi.org/10.1053/j.ajkd.2017.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0272638617306340},
author = {Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham}
}
@article{CALYAM20163,
title = {Synchronous Big Data analytics for personalized and remote physical therapy},
journal = {Pervasive and Mobile Computing},
volume = {28},
pages = {3-20},
year = {2016},
note = {Special Issue on Big Data for Healthcare; Guest Editors: Sriram Chellappan, Nirmalya Roy, Sajal K. Das and Special Issue on Security and Privacy in Mobile Clouds Guest; Editors: Sherman S.M. Chow, Urs Hengartner, Joseph K. Liu, Kui Ren},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2015.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574119215001704},
author = {Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic},
keywords = {Smart health care, Personalized remote physical therapy, Synchronous Big Data, Gigabit networking app},
abstract = {With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., “PhysicalTherapy-as-a-Service” (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a ‘Synchronous Big Data’ application due to its: (i) high data-in-motion velocity (i.e., peak data rate is ≈400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.}
}
@incollection{KRISHNAN2013101,
title = {Chapter 5 - Big Data Driving Business Value},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {101-123},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000052},
author = {Krish Krishnan},
keywords = {sensor data, machine data, social media, compliance, safety},
abstract = {The first four chapters provided you an introduction to Big Data, the complexities associated with Big Data, and the processing techniques and technologies for Big Data. This chapter will focus on use cases of Big Data and how real-world companies are implementing Big Data.}
}
@article{PFEIFFER2015213,
title = {Spatial and temporal epidemiological analysis in the Big Data era},
journal = {Preventive Veterinary Medicine},
volume = {122},
number = {1},
pages = {213-220},
year = {2015},
issn = {0167-5877},
doi = {https://doi.org/10.1016/j.prevetmed.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167587715002111},
author = {Dirk U. Pfeiffer and Kim B. Stevens},
keywords = {Data science, Exploratory analysis, Internet of Things, Modelling, Multi-criteria decision analysis, Spatial analysis, Visualisation},
abstract = {Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.}
}
@article{DEBAUCHE2018112,
title = {Cloud Platform using Big Data and HPC Technologies for Distributed and Parallels Treatments},
journal = {Procedia Computer Science},
volume = {141},
pages = {112-118},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318064},
author = {Olivier Debauche and Sidi Ahmed Mahmoudi and Saïd Mahmoudi and Pierre Manneback},
keywords = {GPU, FPGA, MIC, CPU, TPU, Cloud, Big Data, parallel, distributed processing, heterogeneous cloud architecture},
abstract = {Smart agriculture is one of the most diverse research. In addition, the quantity of data to be stored and the choice of the most efficient algorithms to process are significant elements in this field. The storage of collecting data from Internet of Things (IoT), existing on distributed, local databases and open data need a particular infrastructure to federate all these data to make complex treatments. The storage of this wide range of data that comes at high frequency and variable throughput is particularly difficult. In this paper, we propose the use of distributed databases and high-performance computing architecture in order to exploit multiple re-configurable computing and application specific processing such as CPUs, GPUs, TPUs and FPGAs efficiently. This exploitation allows an accurate training for an application to machine learning, deep learning and unsupervised modeling algorithms. The last ones are used for training supervised algorithms on images when it labels a set of images and unsupervised algorithms on IoT data which are unlabeled with variable qualities. The processing of data is based on Hadoop 3.1 MapReduce to achieve parallel processing and use containerization technologies to distribute treatments on Multi GPU, MIC and FPGA. This architecture allows efficient treatments of data coming from several sources with a cloud high-performance heterogeneous architecture. The proposed 4 layers infrastructure can also implement FPGA and MIC which are now natively supported by recent version of Hadoop. Moreover, with the advent of new technologies like Intel® MovidiusTM; it is now possible to deploy CNN at the Fog level in the IoT network and to make inference with the cloud and therefore limit significantly the network traffic that result in reducing the move of large amounts of data to the cloud.}
}
@article{CHOI2019139,
title = {Data quality challenges for sustainable fashion supply chain operations in emerging markets: Roles of blockchain, government sponsors and environment taxes},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {131},
pages = {139-152},
year = {2019},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2019.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S1366554519311494},
author = {Tsan-Ming Choi and Suyuan Luo},
keywords = {Fashion business operations, Supply chain centralization, Emerging markets, Sustainable operations, Social welfare},
abstract = {In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.}
}
@incollection{KRISHNAN2020175,
title = {10 - Building the big data application},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {175-197},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000107},
author = {Krish Krishnan},
keywords = {Big data, Business continuity, Research project, Software, Storyboard, User interface},
abstract = {This chapter will discuss the building and delivery of the application. We will look at all aspects of what needs to be done to complete the process from requirements to outcomes, program management, testing, methodology, and all risks and pitfalls. We will discuss KANBAN, budgets and finances, governance, timeline, increase of efficiency, maintenance, support, and application implementation.}
}
@incollection{AGOSTON201953,
title = {Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {53-75},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000046},
author = {Denes V. Agoston},
keywords = {Big Data, Artificial intelligence and machine learning in neurotrauma},
abstract = {Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.}
}
@article{SOHRABI2018280,
title = {Systematic method for finding emergence research areas as data quality},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {280-287},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517318140},
author = {Babak Sohrabi and Ahmad Khalilijafarabad},
keywords = {Data quality, Text mining, Science mapping, Data mining, Trend analysis},
abstract = {The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.}
}
@article{FUMEO2015437,
title = {Condition Based Maintenance in Railway Transportation Systems Based on Big Data Streaming Analysis},
journal = {Procedia Computer Science},
volume = {53},
pages = {437-446},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.321},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018244},
author = {Emanuele Fumeo and Luca Oneto and Davide Anguita},
keywords = {Big Data Streams, Data Analytics, Condition Based Maintenance, Intelligent Transporta- tion Systems, Online Learning, Model Selection},
abstract = {Streaming Data Analysis (SDA) of Big Data Streams (BDS) for Condition Based Maintenance (CBM) in the context of Rail Transportation Systems (RTS) is a state-of-the-art field of re- search. SDA of BDS is the problem of analyzing, modeling and extracting information from huge amounts of data that continuously come from several sources in real time through com- putational aware solutions. Among others, CBM for Rail Transportation is one of the most challenging SDA problems, consisting of the implementation of a predictive maintenance system for evaluating the future status of the monitored assets in order to reduce risks related to failures and to avoid service disruptions. The challenge is to collect and analyze all the data streams that come from the numerous on-board sensors monitoring the assets. This paper deals with the problem of CBM applied to the condition monitoring and predictive maintenance of train axle bearings based on sensors data collection, with the purpose of maximizing their Remaining Useful Life (RUL). In particular we propose a novel algorithm for CBM based on SDA that takes advantage of the Online Support Vector Regression (OL-SVR) for predicting the RUL. The novelty of our proposal is the heuristic approach for optimizing the trade-off between the accuracy of the OL-SVR models and the computational time and resources needed in order to build them. Results from tests on a real-world dataset show the actual benefits brought by the proposed methodology.}
}
@article{BIBRI2017449,
title = {ICT of the new wave of computing for sustainable urban forms: Their big data and context-aware augmented typologies and design concepts},
journal = {Sustainable Cities and Society},
volume = {32},
pages = {449-474},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716302475},
author = {Simon Elias Bibri and John Krogstie},
keywords = {Sustainable urban forms, Smart sustainable cities, Big data analytics, Context-aware computing, Typologies and design concepts, Technologies and applications, ICT of the new wave of computing},
abstract = {Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive—i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.}
}
@article{HECKMAN202019,
title = {The Role of Physicians in the Era of Big Data},
journal = {Canadian Journal of Cardiology},
volume = {36},
number = {1},
pages = {19-21},
year = {2020},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2019.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X19312826},
author = {George A. Heckman and John P. Hirdes and Robert S. McKelvie}
}
@article{BERNASCONI2021100009,
title = {Data quality-aware genomic data integration},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {1},
pages = {100009},
year = {2021},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2021.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2666990021000082},
author = {Anna Bernasconi},
keywords = {Data quality, Data integration, Data curation, Genomic datasets, Metadata, Interoperability},
abstract = {Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.}
}
@article{COLEMAN20151091,
title = {How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture},
journal = {American Journal of Ophthalmology},
volume = {160},
number = {6},
pages = {1091-1103.e3},
year = {2015},
issn = {0002-9394},
doi = {https://doi.org/10.1016/j.ajo.2015.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S000293941500598X},
author = {Anne Louise Coleman},
abstract = {Purpose
To characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.
Design
Review of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.
Methods
Statistical analysis of observational data.
Results
The overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P = .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1–7 days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20/40.
Conclusions
Big Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.}
}
@article{SEMANJSKI201738,
title = {Spatial context mining approach for transport mode recognition from mobile sensed big data},
journal = {Computers, Environment and Urban Systems},
volume = {66},
pages = {38-52},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516304367},
author = {Ivana Semanjski and Sidharta Gautama and Rein Ahas and Frank Witlox},
keywords = {Transport mode recognition, Mobile sensed big data, Spatial awareness, Geographic information systems, Smart city, Support vector machines, Context mining, Urban data},
abstract = {Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms.}
}
@incollection{LOSHIN2013105,
title = {Chapter 11 - Developing the Big Data Roadmap},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-120},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000119},
author = {David Loshin},
keywords = {Need for big data, organizational buy-in, team building, big data evangelist, application architect, data integration, platform architect, data scientist, proof of concept, big data pilot, technology evaluation, technology selection, data management, appliance, application development, YARN, MapReduce, SDLC, training, project scoping, platform scoping, problem data size, computational complexity, storage configuration, integration plan, maintenance, management, assessment},
abstract = {This final chapter reviews best practices for incrementally adopting big data into the enterprise. The chapter revisits assessing the need and value of big data, organizational buy-in, building the big data team, scoping and piloting a proof of concept, technology evaluation and selection, application development, testing, and implementation, platform and project scoping, the big data integration plan, management and maintenance, assessment of success criteria, and overall summary and considerations.}
}
@article{MAYO2016260,
title = {The big data effort in radiation oncology: Data mining or data farming?},
journal = {Advances in Radiation Oncology},
volume = {1},
number = {4},
pages = {260-271},
year = {2016},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452109416300550},
author = {Charles S. Mayo and Marc L. Kessler and Avraham Eisbruch and Grant Weyburne and Mary Feng and James A. Hayman and Shruti Jolly and Issam {El Naqa} and Jean M. Moran and Martha M. Matuszak and Carlos J. Anderson and Lynn P. Holevinski and Daniel L. McShan and Sue M. Merkel and Sherry L. Machnak and Theodore S. Lawrence and Randall K. {Ten Haken}},
abstract = {Although large volumes of information are entered into our electronic health care records, radiation oncology information systems and treatment planning systems on a daily basis, the goal of extracting and using this big data has been slow to emerge. Development of strategies to meet this goal is aided by examining issues with a data farming instead of a data mining conceptualization. Using this model, a vision of key data elements, clinical process changes, technology issues and solutions, and role for professional societies is presented. With a better view of technology, process and standardization factors, definition and prioritization of efforts can be more effectively directed.}
}
@article{RAMOS20151031,
title = {Primary Education Evaluation in Brazil Using Big Data and Cluster Analysis},
journal = {Procedia Computer Science},
volume = {55},
pages = {1031-1039},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015367},
author = {Thiago Graca Ramos and Jean Cristian Ferreira Machado and Bruna Principe Vieira Cordeiro},
keywords = {Big Data, Data Warehouse, Cluster, Education, IDEB},
abstract = {This study aims to understand the assessment of basic education in the perspective of the State Reviewer as a mechanism that generates information regarding the positivity and weaknesses of a school or an educational system to provide improvements. For this reason, a Data Warehouse was created and later some analysis of the indicators were performed through clustering.}
}
@article{KUMAR2018428,
title = {A big data driven sustainable manufacturing framework for condition-based maintenance prediction},
journal = {Journal of Computational Science},
volume = {27},
pages = {428-439},
year = {2018},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877750316305129},
author = {Ajay Kumar and Ravi Shankar and Lakshman S. Thakur},
keywords = {data driven sustainable enterprise, fuzzy unordered induction algo, big data analytics, condition-based maintenance, machine learning techniques, backward feature elimination},
abstract = {Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.}
}
@article{LIU2020113381,
title = {Minimizing the data quality problem of information systems: A process-based method},
journal = {Decision Support Systems},
volume = {137},
pages = {113381},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113381},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301366},
author = {Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang},
keywords = {Data quality, Information system, Petri net, Optimization model, Process model},
abstract = {The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.}
}
@article{CORIZZO201918,
title = {Anomaly Detection and Repair for Accurate Predictions in Geo-distributed Big Data},
journal = {Big Data Research},
volume = {16},
pages = {18-35},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579618302119},
author = {Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz},
keywords = {Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation, Neural networks, Gradient-boosting},
abstract = {The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.}
}
@incollection{SAMPSON2015229,
title = {Chapter 15 - The Legal Challenges of Big Data Application in Law Enforcement},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {229-237},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00015-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801967200015X},
author = {Fraser Sampson},
keywords = {Dilemma, Human rights, Jurisdiction, Law enforcement, Privacy, Purpose limitation},
abstract = {This chapter considers the specific issues that Big Data presents for law enforcement agencies (LEAs). In particular, it looks at the dilemmas created for LEAs seeking to use the advantages Big Data gives them while remaining compliant with the developing legal framework governing privacy and the protection of personal data, and how those very advantages can present challenges in law enforcement.}
}
@incollection{SAHOO2019227,
title = {Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data Analytics},
editor = {Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera},
booktitle = {Big Data Analytics for Intelligent Healthcare Management},
publisher = {Academic Press},
pages = {227-246},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-818146-1},
doi = {https://doi.org/10.1016/B978-0-12-818146-1.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818146100009X},
author = {Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das},
keywords = {Big data analytics, Classification, Healthcare, Privacy preservation, Recommendation system},
abstract = {In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.}
}
@article{YE201965,
title = {A hybrid IT framework for identifying high-quality physicians using big data analytics},
journal = {International Journal of Information Management},
volume = {47},
pages = {65-75},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830834X},
author = {Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang},
keywords = {Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis},
abstract = {Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.}
}
@article{YAHIA20181,
title = {Preface: Special Issue on Big Data},
journal = {Fuzzy Sets and Systems},
volume = {348},
pages = {1-3},
year = {2018},
note = {SI: Fuzzy Approaches to Big Data},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2018.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0165011418302987},
author = {Sadok Ben Yahia and Anne Laurent and Gabriella Pasi}
}
@article{JESSE2016275,
title = {Internet of Things and Big Data – The Disruption of the Value Chain and the Rise of New Software Ecosystems},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {29},
pages = {275-282},
year = {2016},
note = {17th IFAC Conference on International Stability, Technology and Culture TECIS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.079},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316325174},
author = {Norbert Jesse},
keywords = {Internet of Things, Smart Factories, Big Data, Software Platforms, Data Science},
abstract = {Abstract:
IoT connects devices, humans, places, and even abstract items like events. Driven by smart sensors, powerful embedded microelectronics, high-speed connectivity and the standards of the internet, IoT is on the brink of disrupting today's value chains. Big Data, characterized by high volume, high velocity and a high variety of formats, is a result of and also a driving force for IoT. The datafication of business presents completely new opportunities and risks. To hedge the technical risks posed by the interaction between “everything”, IoT requires comprehensive modelling tools. Furthermore, new IT platforms and architectures are necessary to process and store the unprecedented flow of structured and unstructured, repetitive and non-repetitive data in real-time. In the end, only powerful analytics tools are able to extract “sense” from the exponentially growing amount of data and, as a consequence, data science becomes a strategic asset. The era of IoT relies heavily on standards for technologies which guarantee the interoperability of everything. This paper outlines some fundamental standardization activities. Big Data approaches for real-time processing are outlined and tools for analytics are addressed. As consequence, IoT is a (fast) evolutionary process whose success in penetrating all dimensions of life heavily depends on close cooperation between standardization organizations, open source communities and IT experts.}
}
@article{ANDREASEN201926,
title = {Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices},
journal = {Journal of Econometrics},
volume = {212},
number = {1},
pages = {26-46},
year = {2019},
note = {Big Data in Dynamic Predictive Econometric Modeling},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2019.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0304407619300740},
author = {Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch},
keywords = {Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model},
abstract = {Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.}
}
@incollection{BROWN2018277,
title = {Chapter Five - Big Data in Drug Discovery},
editor = {David R. Witty and Brian Cox},
series = {Progress in Medicinal Chemistry},
publisher = {Elsevier},
volume = {57},
pages = {277-356},
year = {2018},
issn = {0079-6468},
doi = {https://doi.org/10.1016/bs.pmch.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079646817300243},
author = {Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard},
keywords = {Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials},
abstract = {Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {0210-5691},
doi = {https://doi.org/10.1016/j.medin.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0210569118301827},
author = {Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{DUNCAN2019127,
title = {Big data sharing and analysis to advance research in post-traumatic epilepsy},
journal = {Neurobiology of Disease},
volume = {123},
pages = {127-136},
year = {2019},
note = {Antiepileptogenesis following Traumatic Brain Injury},
issn = {0969-9961},
doi = {https://doi.org/10.1016/j.nbd.2018.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0969996118301700},
author = {Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga},
keywords = {Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI},
abstract = {We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.}
}
@article{SOUIBGUI2019676,
title = {Data quality in ETL process: A preliminary study},
journal = {Procedia Computer Science},
volume = {159},
pages = {676-687},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314097},
author = {Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia},
keywords = {Business Intelligence & Analytics, ETL quality, Data, process quality, Talend Data Integration, Talend Data Quality},
abstract = {The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.}
}
@article{RANJAN2017495,
title = {A note on exploration of IoT generated big data using semantics},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {495-498},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313912},
author = {Rajiv Ranjan and Dhavalkumar Thakker and Armin Haller and Rajkumar Buyya},
abstract = {Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies.}
}
@article{DABEK2015265,
title = {Leveraging Big Data to Model the Likelihood of Developing Psychological Conditions After a Concussion},
journal = {Procedia Computer Science},
volume = {53},
pages = {265-273},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018062},
author = {Filip Dabek and Jesus J. Caban},
keywords = {Big Data, Machine Learning, Concussion, Informatics mild Traumatic Brain Injury},
abstract = {A concussion is an invisible and poorly understood mild traumatic brain injury (mTBI) that can alter the way the brain functions. Patients who have screened positive for mTBI are at an increased risk of depression, post-traumatic stress disorder (PTSD), headaches, sleep disorders, and other neurological and psychological problems. Early detection of psychological conditions such as PTSD following a concussion might improve the overall outcome of a patient and could potentially reduce the cost associated with intense interventions often required when conditions go untreated for a long time. Statistical and predictive models that leverage large-scale clinical repositories and use pre-existing conditions to determine the probability of a patient developing psychological conditions following a concussion have not been widely studied. This paper presents an SVM-based model that has been trained with a longitudinal dataset of over 5.3 million clinical encounters of 89,840 service members that have sustained a concussion. The model has been tested and validated with over 16,045 patients that developed PTSD and it has shown an accuracy of over 85% (AUC of 86.52%) at predicting the condition within the first year following the injury.}
}
@article{KIM2022100256,
title = {Organizational process maturity model for IoT data quality management},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100256},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100256},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000480},
author = {Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee},
keywords = {Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute},
abstract = {Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.}
}
@article{ENRIQUEZ201714,
title = {Entity reconciliation in big data sources: A systematic mapping study},
journal = {Expert Systems with Applications},
volume = {80},
pages = {14-27},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417301550},
author = {J.G. Enríquez and F.J. Domínguez-Mayo and M.J. Escalona and M. Ross and G. Staples},
keywords = {Systematic mapping study, Entity reconciliation, Heterogeneous databases, Big data},
abstract = {The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.}
}
@article{MARKOWETZ2014405,
title = {Psycho-Informatics: Big Data shaping modern psychometrics},
journal = {Medical Hypotheses},
volume = {82},
number = {4},
pages = {405-411},
year = {2014},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2013.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0306987713005598},
author = {Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer},
abstract = {For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.}
}
@article{NIMMAGADDA20171871,
title = {Big Data Guided Design Science Information System (DSIS) Development for Sustainability Management and Accounting},
journal = {Procedia Computer Science},
volume = {112},
pages = {1871-1880},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.233},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316381},
author = {Shastri L Nimmagadda and Torsten Reiners and and {Gary Burke}},
keywords = {Design Science, Digital Ecosystem, Sustainability, Multidimensional Artefacts, Data Interpretation},
abstract = {Sustainability is a dynamic, complex and composite data relationship among geographically distributed human and environment ecosystems. The ecosystems may have strong interactions among their elements and processes, but with dynamic implicit boundaries. Multi-scalable and multidimensional ecosystems have significance based on a commonality of basic structural units and domains. We intend to develop a holistic information system for managing different ecosystems within a sustainability framework/context, using an empirical qualitative and quantitative interpretation and analysis of the measured observations. Design Science Research (DSR) approach is aimed at developing an information system using the volumes of unstructured Big Data observations. Collaborating multiple domains, interpreting and evaluating the commonality, uncovering the connectivity among multiple systems are key aspects of the study. The Design Science Information System (DSIS), evolved from DSR approach is used in solving the ecosystem issues associated with multiple domains, in which the sustainability challenges manifest. In this context, we propose a human-environment-economic ecosystem (HEES) framework consisting of human, environment and economic elements and processes. In broad terms, human, environment and economic domains are conceptualized as different players/agents that operate within a range of sustainability scenarios. This approach recognizes the existing constraints of the systems as well as the emerging knowledge of the boundaries of ecosystems and their connectivity. The connectivity and interaction among the systems are analyzed by data mining, visualization and interpretation artefacts within a sustainability policy framework.}
}
@article{MENDESSAMPAIO20158304,
title = {DQ2S – A framework for data quality-aware information management},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8304-8326},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.050},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004522},
author = {Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio},
keywords = {Information management, Data quality, Query language extensions, Data profiling, Decision support systems, Big data},
abstract = {This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user’s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.}
}
@article{MACKIE2015189,
title = {Big data! Big deal?},
journal = {Public Health},
volume = {129},
number = {3},
pages = {189-190},
year = {2015},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2015.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033350615000621},
author = {P. Mackie and F. Sim and C. Johnman}
}
@incollection{SEBASTIANCOLEMAN2022187,
title = {Chapter 9 - Core Data Quality Management Capabilities},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {187-228},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000092},
author = {Laura Sebastian-Coleman},
keywords = {Data quality standards, assessing data quality, data quality monitoring, data quality reporting, data issue management, quality improvement methodology},
abstract = {This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.}
}
@article{LIN2014532,
title = {A New Idea of Study on the Influence Factors of Companies’ Debt Costs in the Big Data Era},
journal = {Procedia Computer Science},
volume = {31},
pages = {532-541},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.299},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004761},
author = {Li Lin and Wang Shuang and Liu Yifang and Wang Shouyang},
keywords = {debt cost, big data, quality of accounting information, corporate governance, LASSO method},
abstract = {Under the background of big data era today, once been widely used method – multiple linear regressions can not satisfy people's need to handle big data any more because of its bad characteristics such as multicollinearity, instability, subjectivity in model chosen etc. Contrary to MLR, LASSO method has many good natures. it is stable and can handle multicollinearity and successfully select the best model and do estimation in the same time. LASSO method is an effective improvement of multiple linear regressions. It is a natural change and innovation to introduce LASSO method into the accounting field and use it to deal with the debt costs problems. It helps us join the statistic field and accounting field together step by step. What's more, in order to proof the applicability of LASSO method in dealing with debt costs problems, we take 2301 companies’ data from Shanghai and Shenzhen A-share market in 2012 as samples, and chose 18 indexes to verify that the results of LASSO method is scientific, reasonable and accurate. In the end, we compare LASSO method with traditional multiple linear regressions and ridge regression, finding out that LASSO method can not only offer the most accurate prediction but also simplify the model.}
}
@article{LIU2021257,
title = {Massive-scale carbon pollution control and biological fusion under big data context},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {257-262},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000042},
author = {Yi Liu and Jie Xu and Weijie Yi},
keywords = {Internet-scale network, Dense subgraph mining, Low carbon, Multiple features, Biologically-aware},
abstract = {In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs’ cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.}
}
@incollection{SHEIKH2013185,
title = {Chapter 11 - Big Data, Hadoop, and Cloud Computing},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {185-197},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000116},
author = {Nauman Sheikh},
keywords = {Hadoop, Big Data, cloud computing},
abstract = {When the idea for this book was originally conceived, Big Data and Hadoop were not the most popular themes on the tech circuit, although cloud computing was somewhat more prominent. Some of the reviewer feedback suggested that these topics should be addressed in the context of the conceptual layout of analytics solutions. In this chapter their use in an overall analytics solution will be explained using the previous chapters as a foundation. Big Data, Hadoop, and cloud computing are presented as standalone material, each tying back into the overall analytics solution implementations presented in preceding chapters.}
}
@article{KOZIEL2021116057,
title = {Investments in data quality: Evaluating impacts of faulty data on asset management in power systems},
journal = {Applied Energy},
volume = {281},
pages = {116057},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920314896},
author = {Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh},
keywords = {Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off},
abstract = {Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.}
}
@article{201616,
title = {Clinical research and big data},
journal = {Dental Abstracts},
volume = {61},
number = {1},
pages = {16-17},
year = {2016},
issn = {0011-8486},
doi = {https://doi.org/10.1016/j.denabs.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0011848615010043}
}
@article{HUANG20181413,
title = {Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1413-1423},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.02.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17324500},
author = {Xiaohong Huang and Kun Xie and Supeng Leng and Tingting Yuan and Maode Ma},
keywords = {Data fusion, Multimedia Internet of Things, Big data, Quality of Experience, Machine learning, Neural network},
abstract = {With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting/managing/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.}
}
@article{MCDERMOTT2015303,
title = {What are the implications of the big data paradigm shift for disability and health?},
journal = {Disability and Health Journal},
volume = {8},
number = {3},
pages = {303-304},
year = {2015},
issn = {1936-6574},
doi = {https://doi.org/10.1016/j.dhjo.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1936657415000515},
author = {Suzanne McDermott and Margaret A. Turk}
}
@article{YU20171,
title = {Data pricing strategy based on data quality},
journal = {Computers & Industrial Engineering},
volume = {112},
pages = {1-10},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217303509},
author = {Haifei Yu and Mengxiao Zhang},
keywords = {Big data, Data marketplace, Data pricing, Production management, Bi-level programming model},
abstract = {This paper presents a bi-level mathematical programming model for the data-pricing problem that considers both data quality and data versioning strategies. Data products and data-related services differ from information products or services in terms of quality assessment methods. For this problem, we consider two aspects of data quality: (1) its multidimensionality and (2) the interaction between the dimensions. We designed a multi-version data strategy and propose a data-pricing bi-level programming model based on the data quality to maximize the profit by the owner of the data platform and the utility to consumers. A genetic algorithm was used to solve the model. The numerical solutions for the data-pricing model indicate that the multi-version strategy achieves a better market segmentation and is more profitable and feasible when the multiple dimensions of data quality are considered. These results also provide managerial guidance on data provision and data pricing for platform owners.}
}
@article{GIL201761,
title = {Big Data. New approaches of modelling and management},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {61-63},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917301022},
author = {David Gil and Il-Yeol Song and José F. Aldana and Juan Trujillo},
abstract = {Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.}
}
@article{SUN20171,
title = {Special Issue on Scalable Computing Systems for Big Data Applications},
journal = {Journal of Parallel and Distributed Computing},
volume = {108},
pages = {1-2},
year = {2017},
note = {Special Issue on Scalable Computing Systems for Big Data Applications},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301776},
author = {Xian-He Sun and Marc Frincu and Charalampos Chelmis}
}
@article{VITOLO2015185,
title = {Web technologies for environmental Big Data},
journal = {Environmental Modelling & Software},
volume = {63},
pages = {185-198},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002965},
author = {Claudia Vitolo and Yehia Elkhatib and Dominik Reusser and Christopher J.A. Macleod and Wouter Buytaert},
keywords = {Web-based modelling, Big Data, Web services, OGC standards},
abstract = {Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.}
}
@article{SCHUH201943,
title = {Data quality program management for digital shadows of products},
journal = {Procedia CIRP},
volume = {86},
pages = {43-48},
year = {2019},
note = {7th CIRP Global Web Conference – Towards shifted production value stream patterns through inference of data, models, and technology (CIRPe 2019)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120300366},
author = {Günther Schuh and Eric Rebentisch and Michael Riesener and Thorben Ipers and Christian Tönnes and Merle-Hendrikje Jank},
keywords = {data quality program, digital shadow, data quality management},
abstract = {Nowadays, companies are facing challenges due to increasingly dynamic market environments, a growing internal and external complexity, as well as globally intensifying competition. To keep pace, companies need to establish extensive knowledge about their business and its surroundings based on insights generated through the analysis of data. The digital shadow is a novel information system concept that integrates data of heterogeneous sources to provide product-related information to stakeholders across the company. The concept aims at improving the results of decision making, enabling advanced data analyses, and increasing information handling efficiency. As insufficient information quality has immediate effects on the utility of the information and induces significant costs, managing the quality of the digital shadow data basis is crucial. However, there are currently no comprehensive methodologies for the assessment and improvement of the data quality of digital shadows. Therefore, this paper introduces a methodology that supports the derivation of data quality projects aimed at optimizing the digital shadow data basis. The proposed methodology comprises four steps: First, digital shadow use cases along the product lifecycle are described. Next, the use cases are prioritized with regard to the expected benefits of applying the digital shadow. Third, quality deficiencies in the digital shadow data basis are assessed with respect to use case specific requirements. Finally, the prioritized use cases in relation with the identified quality deficits allow deriving needs for action, which are addressed by data quality projects. Together, the data quality projects constitute a data quality program. The methodology is applied in an industry case to prove the practical effectivity and efficiency.}
}
@article{FANG2022104070,
title = {BIM-integrated portfolio-based strategic asset data quality management},
journal = {Automation in Construction},
volume = {134},
pages = {104070},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104070},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005215},
author = {Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian},
keywords = {Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management},
abstract = {A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.}
}
@article{HAMMER2017715,
title = {Profit Per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure},
journal = {Procedia CIRP},
volume = {63},
pages = {715-720},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117302408},
author = {Markus Hammer and Ken Somers and Hugo Karre and Christian Ramsauer},
keywords = {Operations Management, Manufacturing Systems 4.0, Profit per Hour, Advanced Process Control, Big Data Analytics, Agile Manufacturing},
abstract = {The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility.}
}
@incollection{CHESSELL201733,
title = {Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {33-48},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805467300003X},
author = {Mandy Chessell and Dan Wolfson and Tim Vincent},
keywords = {Enterprise architecture, Self-service data, Systems of insight, Data-driven security, Business-driven governance, Trust and confidence, Hybrid-cloud, Information supply chains},
abstract = {Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization – increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.}
}
@article{GIL201696,
title = {Modeling and Management of Big Data: Challenges and opportunities},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {96-99},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15002514},
author = {David Gil and Il-Yeol Song},
keywords = {Conceptual modeling Big Data, Ecosystem, Integrate & analyze & visualize},
abstract = {The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.}
}
@incollection{KRISHNAN2013241,
title = {Chapter 12 - Information Management and Life Cycle for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {241-250},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405891000012X},
author = {Krish Krishnan},
keywords = {information life-cycle management, governance, program governance, data governance, data quality},
abstract = {This chapter deals with how to implement information life-cycle management principles to Big Data and create a sustainable process that will ensure that business continuity is not interrupted and data is available on demand.}
}
@incollection{KRISHNAN20133,
title = {Chapter 1 - Introduction to Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {3-14},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000015},
author = {Krish Krishnan},
keywords = {Big Data, data warehousing, sentiments, social media, machine data},
abstract = {Why this book? Why now? The goal of this book is to provide readers with a concise perspective into the biggest buzz in the industry—Big Data—and, more importantly, its impact on data processing, management, decision support, and data warehousing. At the time of this writing, there is a lot of interest to adopt a Big Data solution, but the profound confusion is what is the future of data warehousing and many investments that have been made over the years into building the decision support platform. This book addresses those areas of concern and provides readers an introduction to the next-generation of data management and data warehousing. This chapter provides you a concise and example driven introduction to what is Big Data, and how any organization needs to understand the value of Big Data.}
}
@article{DEMIRKAN2013412,
title = {Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud},
journal = {Decision Support Systems},
volume = {55},
number = {1},
pages = {412-421},
year = {2013},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2012.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S0167923612001595},
author = {Haluk Demirkan and Dursun Delen},
keywords = {Cloud computing, Service orientation, Service science, Data-as-a-service, Information-as-a-service, Analytics-as-a-service, Big data},
abstract = {Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.}
}
@article{XIAO2014594,
title = {Knowledge diffusion path analysis of data quality literature: A main path analysis},
journal = {Journal of Informetrics},
volume = {8},
number = {3},
pages = {594-605},
year = {2014},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2014.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1751157714000492},
author = {Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou},
keywords = {Data quality, Main path analysis, Knowledge diffusion, Citation analysis, Social network analysis, Big data},
abstract = {This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.}
}
@incollection{LOSHIN201329,
title = {Chapter 4 - Developing a Strategy for Integrating Big Data Analytics into the Enterprise},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {29-37},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000041},
author = {David Loshin},
keywords = {Strategic plan, business requirements, technology adoption, massive scalability, data reuse, data repurposing, oversight, governance, mainstreaming technology, enterprise integration},
abstract = {This chapter expands on the previous one by looking at some key issues that often plague new technology adoption and show that the key issues are not new ones, and that there is likely to be organizational knowledge that can help in fleshing out a reasonable strategic plan. We look at the typical hype cycle, and how its flaws can be mitigated by instituting good practices for defining expectations and continuing to measure performance. We help define the acceptability criteria for evaluating the result of a big data pilot that can be used to make a go/no-go decision. We then pose some thoughts about preparing the organization for massive scalability, data reuse, and the need for oversight and governance. The objective is to provide a pathway for mainstreaming big data into the technology infrastructure that is integrated with the existing investment.}
}
@incollection{SEBASTIANCOLEMAN202231,
title = {Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {31-45},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500002X},
author = {Laura Sebastian-Coleman},
keywords = {Data, history of data, data quality, data management, data governance, data stewardship, data and technology, process improvement, technology strategy, culture/organization, data literacy},
abstract = {This chapter describes the five challenges in data quality management (data, process, technology, people, and culture/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today’s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.}
}
@article{LIU2019242,
title = {How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected from Wearables},
journal = {Procedia Computer Science},
volume = {155},
pages = {242-249},
year = {2019},
note = {The 16th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2019),The 14th International Conference on Future Networks and Communications (FNC-2019),The 9th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919309494},
author = {Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha Röning},
keywords = {Data analytics, wearables, sleep quality, statistical methods},
abstract = {Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.}
}
@article{CLARK2016443,
title = {Big data and ophthalmic research},
journal = {Survey of Ophthalmology},
volume = {61},
number = {4},
pages = {443-465},
year = {2016},
issn = {0039-6257},
doi = {https://doi.org/10.1016/j.survophthal.2016.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039625716000023},
author = {Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens},
keywords = {data linkage, clinical registry, health services research, ophthalmic epidemiology, big data},
abstract = {Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.}
}
@article{NGUYEN2018254,
title = {Big data analytics in supply chain management: A state-of-the-art literature review},
journal = {Computers & Operations Research},
volume = {98},
pages = {254-264},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817301685},
author = {Truong Nguyen and Li ZHOU and Virginia Spiegler and Petros Ieromonachou and Yong Lin},
keywords = {Literature review, Big data, Big data analytics, Supply chain management, Research directions},
abstract = {The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.}
}