@article{MERINO2016123,
title = {A Data Quality in Use model for Big Data},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {123-130},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003817},
author = {Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini},
keywords = {Data Quality, Big Data, Measurement, Quality-in-Use, Model},
abstract = {Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.}
}
@article{LIU2022100279,
title = {Geographic information science in the era of geospatial big data: A cyberspace perspective},
journal = {The Innovation},
volume = {3},
number = {5},
pages = {100279},
year = {2022},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2022.100279},
url = {https://www.sciencedirect.com/science/article/pii/S2666675822000753},
author = {Xintao Liu and Min Chen and Christophe Claramunt and Michael Batty and Mei-Po Kwan and Ahmad M. Senousi and Tao Cheng and Josef Strobl and Arzu Cöltekin and John Wilson and Temenoujka Bandrova and Milan Konecny and Paul M. Torrens and Fengyuan Zhang and Li He and Jinfeng Wang and Carlo Ratti and Olaf Kolditz and Alexander Klippel and Songnian Li and Hui Lin and Guonian Lü}
}
@article{SHARDT2020104,
title = {Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {104-113},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320303591},
author = {Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov},
keywords = {data quality assessment, system identification, big data, Industry 4.0, soft sensors},
abstract = {As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.}
}
@article{LEE20221728,
title = {Quality assurance of integrative big data for medical research within a multihospital system},
journal = {Journal of the Formosan Medical Association},
volume = {121},
number = {9},
pages = {1728-1738},
year = {2022},
issn = {0929-6646},
doi = {https://doi.org/10.1016/j.jfma.2021.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S092966462100591X},
author = {Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang},
keywords = {Big data, Electronic health record, Evidence based healthcare management, Validation study},
abstract = {Background
The need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.
Methods
Electronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006–2016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.
Results
Data were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34/1000. Statistical differences (P < 0.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017–2018, in which 10 (3.5%) tables were corrected.
Conclusion
Significant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.}
}
@article{CREMIN2022138,
title = {Big data: Historic advances and emerging trends in biomedical research},
journal = {Current Research in Biotechnology},
volume = {4},
pages = {138-151},
year = {2022},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2590262822000090},
author = {Conor John Cremin and Sabyasachi Dash and Xiaofeng Huang},
keywords = {Big data, Big data in biomedicine, Data analytics in biomedical research, Multiomics big data, Biomedical data management, Big data in personalized medicine},
abstract = {Big data is transforming biomedical research by integrating massive amounts of data from laboratory experiments, clinical investigations, healthcare records, and the internet of things. Specifically, the increasing rate at which information is obtained from omics technologies (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and pharmacogenomics) is providing an opportunity for future advances in personalized medicine that are paving the way to improved patient care. The recent advances in omics technologies are profoundly contributing to big data in biomedicine and are anticipated to aid in disease diagnosis and patient care management. Herein, we critically review the major computational techniques, algorithms, and their outcomes that have contributed to recent advances in big data generated from biomedical research in various complex human diseases, such as cancer and infectious diseases. Finally, we discuss trends in the field and the future directions that must be considered to advance the influence of big data on biomedical research and its translation in the healthcare industry.}
}
@article{JACOBBRASSARD2022,
title = {Big Data: Using Databases and Registries},
journal = {Seminars in Vascular Surgery},
year = {2022},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2022.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S089579672200062X},
author = {Jean Jacob-Brassard and Charles {de Mestral}},
abstract = {ABSTRACT
The field of vascular surgery is in constant evolution. Administrative data and registries can provide important contemporary evidence to inform clinical decision making and the delivery of health services. The following review outlines some important considerations for retrospective studies using administrative health databases and registries. First, these data sources have advantages (e.g. real-world applicability, timely data access, relatively lower research cost) and disadvantages (e.g. potential missing data, selection bias, confounding bias) that may be more or less relevant to different administrative databases or registries. Second, we discuss a framework to guide data source selection and provide a summary of frequently used data sources in vascular surgery research. Third, a retrospective study design warrants pre-planned exposure, outcome and covariate definitions and, when studying an exposure-outcome association, careful consideration of confounders through direct acyclic graphs. Finally, investigators must plan the most appropriate analytic approach and we distinguish descriptive, explanatory, and predictive analyses.}
}
@article{ZHANG202134,
title = {Big data and human resource management research: An integrative review and new directions for future research},
journal = {Journal of Business Research},
volume = {133},
pages = {34-50},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002563},
author = {Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang},
keywords = {Human resource management research, Big data, Integrative review, Inductive and deductive paradigms},
abstract = {The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as “Inductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)”. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.}
}
@incollection{ZHANG20221681,
title = {Hashing-based just-in-time learning for big data quality prediction},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1681-1686},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50280-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502803},
author = {Xinmin Zhang and Jiang Zhai and Zhihuan Song and Yuan Li},
keywords = {Virtual sensor, soft-sensor, big data quality prediction, hashing-based just-in-time modeling},
abstract = {In recent years, the just-in-time (JIT) predictive models have attracted considerable attention due to their ability to prevent degradation of prediction accuracy. However, one of their practical limitations is expensive computation, which becomes a major factor that prevents them from being used for big data quality prediction. This is because the JIT modeling methods need to update the local regression model using the relevant samples that are searched through the lineal scan of the database during online operation. To solve this issue, the present work proposes a novel hashing-based JIT (HbJIT) modeling method that is suitable for big data quality prediction. In HbJIT, a family of locality-sensitive hash functions is firstly used to hash big data into a set of buckets, in which similar samples are grouped on themselves. During online prediction, HbJIT looks up multiple buckets that have a high probability of containing similar samples of a query object through the intelligent probing scheme, uses the data objects in the buckets as the candidate set of the results, and then filters the candidate objects using a linear scan. After filtering, the most relevant samples are used to construct the local regression model to yield the prediction of the query object. By integrating the multi-probe hashing strategy into the JIT learning framework, HbJIT can not only deal with process nonlinearity and time-varying characteristics but also is applicable to large-scale industrial processes. Experimental results on real-world dataset have demonstrated that the proposed HbJIT is time-efficient in processing large-scale datasets, and greatly reduces the online prediction time without compromising on the prediction accuracy.}
}
@article{KEDDY2022e130,
title = {Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications},
journal = {The Lancet Infectious Diseases},
volume = {22},
number = {5},
pages = {e130-e142},
year = {2022},
issn = {1473-3099},
doi = {https://doi.org/10.1016/S1473-3099(21)00585-5},
url = {https://www.sciencedirect.com/science/article/pii/S1473309921005855},
author = {Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke},
abstract = {Summary
Diarrhoea is an important cause of morbidity and mortality in children from low-income and middle-income countries (LMICs), despite advances in the management of this condition. Understanding of the causes of diarrhoea in children in LMICs has advanced owing to large multinational studies and big data analytics computing the disease burden, identifying the important variables that have contributed to reducing this burden. The advent of the mobile phone has further enabled the management of childhood diarrhoea by providing both clinical support to health-care workers (such as diagnosis and management) and communicating preventive measures to carers (such as breastfeeding and vaccination reminders) in some settings. There are still challenges in addressing the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented geographical areas, concerns about patient confidentiality, unequal partnerships between study investigators, and the reactive approach to outbreaks. A transparent approach to promote the inclusion of researchers in LMICs could address partnership imbalances. A big data umbrella encompassing cloud-based centralised databases to analyse interlinked human, animal, agricultural, social, and climate data would provide an informative solution to the development of appropriate management protocols in LMICs.}
}
@article{IBRAHIM2021121171,
title = {The convergence of big data and accounting: innovative research opportunities},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121171},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121171},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006041},
author = {Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat},
keywords = {Big data, Analytics, Accounting, Data science, Business intelligence},
abstract = {This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.}
}
@article{BRAVE2022481,
title = {The perils of working with big data, and a SMALL checklist you can use to recognize them},
journal = {Business Horizons},
volume = {65},
number = {4},
pages = {481-492},
year = {2022},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681321001178},
author = {Scott A. Brave and R. Andrew Butters and Michael Fogarty},
keywords = {Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator},
abstract = {The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.}
}
@article{PLAZIER2022S198,
title = {PO048 / #665 THE BIG CHANGE IS COMMING: BIG DATA: E-POSTER VIEWING},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {25},
number = {7, Supplement },
pages = {S198},
year = {2022},
note = {INS 15th World Congress},
issn = {1094-7159},
doi = {https://doi.org/10.1016/j.neurom.2022.08.222},
url = {https://www.sciencedirect.com/science/article/pii/S1094715922009977},
author = {Mark Plazier and Vincent Raymaekers and Wim Duyvendak and Sacha Meeuws and Maarten Wissels and Steven Vanvolsem and Gert Roosen and Sven Bamps and Salah-Edine Achabar and Stefan Schu and Anna Keil and Björn Carsten Schultheis and Philipp Slotty and Dirk {De Ridder} and Jan Vesper}
}
@article{TANG2022e198,
title = {The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review},
journal = {World Neurosurgery},
volume = {162},
pages = {e198-e217},
year = {2022},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2022.02.113},
url = {https://www.sciencedirect.com/science/article/pii/S1878875022002601},
author = {Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms},
keywords = {Big data, Disparities, Health care costs, Health policy, Hospital volume, Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS},
abstract = {Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.}
}
@article{HU2021107994,
title = {A blockchain-based trading system for big data},
journal = {Computer Networks},
volume = {191},
pages = {107994},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.107994},
url = {https://www.sciencedirect.com/science/article/pii/S138912862100116X},
author = {Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng},
keywords = {Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward},
abstract = {Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.}
}
@article{ARFANUZZAMAN2021100127,
title = {Harnessing artificial intelligence and big data for SDGs and prosperous urban future in South Asia},
journal = {Environmental and Sustainability Indicators},
volume = {11},
pages = {100127},
year = {2021},
issn = {2665-9727},
doi = {https://doi.org/10.1016/j.indic.2021.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2665972721000283},
author = {Md. Arfanuzzaman},
keywords = {Artificial intelligence, Big data, Climate resilience, Data infrastructure, South Asia, SDG, Technological readiness, Urban transformation},
abstract = {Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.}
}
@article{GHASEMAGHAEI201938,
title = {Can big data improve firm decision quality? The role of data quality and data diagnosticity},
journal = {Decision Support Systems},
volume = {120},
pages = {38-49},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300466},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data utilization, Data quality, Decision quality, Data diagnosticity},
abstract = {Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.}
}
@article{SHAMIM2021106777,
title = {Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings},
journal = {Computers in Human Behavior},
volume = {121},
pages = {106777},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106777},
url = {https://www.sciencedirect.com/science/article/pii/S074756322100100X},
author = {Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah},
keywords = {Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality},
abstract = {Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations’ capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.}
}
@article{LEON2021100253,
title = {Enhancing Precision Medicine: A Big Data-Driven Approach for the Management of Genomic Data},
journal = {Big Data Research},
volume = {26},
pages = {100253},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000708},
author = {Ana León and Óscar Pastor},
keywords = {Big Data, Genomics, Computer science, Theory and methods},
abstract = {The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.}
}
@article{GU2021132,
title = {SparkDQ: Efficient generic big data quality management on distributed data-parallel computation},
journal = {Journal of Parallel and Distributed Computing},
volume = {156},
pages = {132-147},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521001246},
author = {Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang},
keywords = {Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data},
abstract = {In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.}
}
@article{TALHA2019916,
title = {Big Data: Trade-off between Data Quality and Data Security},
journal = {Procedia Computer Science},
volume = {151},
pages = {916-922},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.127},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305915},
author = {M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI},
keywords = {Big Data, Data Quality, Data Security, Trade-off between Quality, Security},
abstract = {The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.}
}
@article{LUTFI2023103129,
title = {Drivers and impact of big data analytic adoption in the retail industry: A quantitative investigation applying structural equation modeling},
journal = {Journal of Retailing and Consumer Services},
volume = {70},
pages = {103129},
year = {2023},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2022.103129},
url = {https://www.sciencedirect.com/science/article/pii/S0969698922002223},
author = {Abdalwali Lutfi and Mahmaod Alrawad and Adi Alsyouf and Mohammed Amin Almaiah and Ahmad Al-Khasawneh and Akif Lutfi Al-Khasawneh and Ahmad Farhan Alshira'h and Malek Hamed Alshirah and Mohamed Saad and Nahla Ibrahim},
keywords = {Big data analytic (BDA), Technology adoption, Retail industry, Data volume, Data variety, Data velocity, Diffusion of innovations model, RBV theory},
abstract = {Big data analytics (BDA) adoption has gained attention in both practical and theoretical circles owing to the opportunities and advantages that can be reaped from it. In theory, the majority of researchers have evidenced the benefits of BDA, although barriers to its adoption have also been mentioned. This study draws upon the technology-organisation-environment framework and resource-based view theory to propose an integrated model that examines the drivers and impact of BDA adoption in the retail industry in Jordan. The proposed single model encapsulates the aspects of BDA adoption and performance. The study makes use of an online questionnaire survey to collect the required data, and the research model is eventually validated based on 132 responses gathered from the retail industry in Jordan. The findings highlight two major observations. The first is that relative advantage, organisational readiness, top management support, government support, data variety and data velocity all have a significant influence over BDA adoption. The second observation is that a significant association exists between BDA adoption and firm performance, providing information on the way firms can enhance their BDA adoption for enhanced performance. This study contributes to literature dedicated to examining BDA in terms of its drivers and impact on performance and can be used as a reference in developing nations.}
}
@article{MOSTEFAOUI2022374,
title = {Big data architecture for connected vehicles: Feedback and application examples from an automotive group},
journal = {Future Generation Computer Systems},
volume = {134},
pages = {374-387},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001492},
author = {Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables},
keywords = {Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA},
abstract = {Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).}
}
@article{YANG202223,
title = {Big data and reference intervals},
journal = {Clinica Chimica Acta},
volume = {527},
pages = {23-32},
year = {2022},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2022.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0009898122000018},
author = {Dan Yang and Zihan Su and Min Zhao},
keywords = {Indirect method, Reference interval, Data pre-processing, Verification, Big data},
abstract = {Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to “mine” laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.}
}
@article{LI2022119292,
title = {Data cleaning and restoring method for vehicle battery big data platform},
journal = {Applied Energy},
volume = {320},
pages = {119292},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.119292},
url = {https://www.sciencedirect.com/science/article/pii/S030626192200647X},
author = {Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng},
keywords = {Big data, Internet of vehicle, Electric vehicles, Data cleaning, Battery management system, Battery state estimation},
abstract = {Battery is one of the most important and costly devices in electric vehicles (EVs). Developing an efficient battery management method is of great significance to enhancing vehicle safety and economy. Recently developed big-data and cloud platform computing technologies bring a bright perspective for efficient utilization and protection of vehicle batteries. However, a reliable data transmission network and a high-quality cloud battery dataset are indispensable to enable this benefit. This paper makes the first effort to systematically solve data quality problems in cloud-based vehicle battery monitoring and management by developing a novel integrated battery data cleaning framework. In the first stage, the outlier samples are detected by analyzing the temporal features in the battery data time series. The outlier data in the dataset can be accurately detected to avoid their impacts on battery monitoring and management. Then, the abnormal samples, including the noise polluted data and missing value, are restored by a novel future fusion data restoring model. The real electric bus operation data collected by a cloud-based battery monitoring and management platform are used to verify the performance of the developed data cleaning method. More than 93.3% of outlier samples can be detected, and the data restoring error can be limited to 2.11%, which validates the effectiveness of the developed methods. The proposed data cleaning method provides an effective data quality assessment tool in cloud-based vehicle battery management, which can further boost the practical application of the vehicle big data platform and Internet of vehicle.}
}
@article{MOSTAFA2022100363,
title = {Renewable energy management in smart grids by using big data analytics and machine learning},
journal = {Machine Learning with Applications},
volume = {9},
pages = {100363},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000597},
author = {Noha Mostafa and Haitham Saad Mohamed Ramadan and Omar Elfarouk},
keywords = {Energy internet, Renewable energy, Smart grid, Big data analytics, Machine learning, Predictive models},
abstract = {The application of big data in the energy sector is considered as one of the main elements of Energy Internet. Crucial and promising challenges exist especially with the integration of renewable energy sources and smart grids. The ability to collect data and to properly use it for better decision-making is a key feature; in this work, the benefits and challenges of implementing big data analytics for renewable energy power stations are addressed. A framework was developed for the potential implementation of big data analytics for smart grids and renewable energy power utilities. A five-step approach is proposed for predicting the smart grid stability by using five different machine learning methods. Data from a decentralized smart grid data system consisting of 60,000 instances and 12 attributes was used to predict the stability of the system through three different machine learning methods. The results of fitting the penalized linear regression model show an accuracy of 96% for the model implemented using 70% of the data as a training set. Using the random forest tree model has shown 84% accuracy, and the decision tree model has shown 78% accuracy. Both the convolutional neural network model and the gradient boosted decision tree model yielded 87% for the classification model. The main limitation of this work is that the amount of data available in the dataset is considered relatively small for big data analytics; however the cloud computing and real-time event analysis provided was suitable for big data analytics framework. Future research should include bigger datasets with variety of renewable energy sources and demand across more countries.}
}
@article{BINDER2020307,
title = {Big Data Management Using Ontologies for CPQ Solutions},
journal = {Procedia Manufacturing},
volume = {52},
pages = {307-312},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321958},
author = {Alexander Binder and Eva-Maria Iwer and Werner Quint},
keywords = {CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality},
abstract = {In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called "ontology-based data matching", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.}
}
@article{OESTERREICH2022103685,
title = {What translates big data into business value? A meta-analysis of the impacts of business analytics on firm performance},
journal = {Information & Management},
volume = {59},
number = {6},
pages = {103685},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103685},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000945},
author = {Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg},
keywords = {Business analytics, Business intelligence, IT business value, Firm performance, Meta-analysis, Moderator analysis},
abstract = {The main purpose of this study is to examine the factors that are critical to create business value from business analytics (BA). Therefore, we conduct a meta-analysis of 125 firm-level studies spanning ten years of research from across 26 countries. We found evidence that the social factors of BA, such as human resources, management capabilities, and organizational culture show a greater impact on business value, whereas technical aspects play a minor role in enhancing firm performance. Through these findings, we contribute to the ongoing debate concerning BA business value by synthesizing and validating the findings of the body of knowledge.}
}
@article{RHAHLA2021102896,
title = {Guidelines for GDPR compliance in Big Data systems},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102896},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102896},
url = {https://www.sciencedirect.com/science/article/pii/S221421262100123X},
author = {Mouna Rhahla and Sahar Allegue and Takoua Abdellatif},
keywords = {The General Data Protection Regulation (GDPR), Big Data analytics, Privacy, Security},
abstract = {The implementation of the GDPR that aims at protecting European citizens’ privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR’s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.}
}
@article{WILKIN2020100470,
title = {Big data prioritization in SCM decision-making: Its role and performance implications},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100470},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100470},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300385},
author = {Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan},
keywords = {Big data, Big data availability, Big data prioritization, Supply chain management, Performance},
abstract = {Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.}
}
@article{WEERASINGHE2022121222,
title = {Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121222},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121222},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006557},
author = {Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin},
keywords = {Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory},
abstract = {The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.}
}
@article{LI202245,
title = {Spatially gap free analysis of aerosol type grids in China: First retrieval via satellite remote sensing and big data analytics},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {193},
pages = {45-59},
year = {2022},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0924271622002374},
author = {Ke Li and Kaixu Bai and Mingliang Ma and Jianping Guo and Zhengqiang Li and Gehui Wang and Ni-Bin Chang},
keywords = {Aerosol types, Fine mode fraction, Haze reduction, Satellite remote sensing, Big data analytics},
abstract = {Spatially contiguous aerosol type grids were rarely available for air quality management in the past. To bridge the gap, we developed an integrated remote sensing and big data analytics framework to generate spatially gap-free aerosol type grids between 2000 and 2020 in China. The effect of emission control via environmental management on haze reduction was fully realized for the first time with the aid of satellite-based gap-free aerosol type data. Daily gap-free aerosol fine mode fraction (FMF) data were first derived via a data-driven regression model based on remote sensing big data. According to empirically determined FMF probability distributions over regions with typical emission sources, aerosols in China were classified into eight major types, including typical/atypical/mixed anthropogenic aerosols, typical/atypical/mixed dust, and typical mixed and multiple modes. The results indicated that the gridded FMF estimates derived in this study agreed well with FMF retrievals from AERONET, with correlation coefficient of 0.81 and root mean square error of 0.13. The long-term variations in major aerosol types showed that in China the territory covered by typical anthropogenic aerosols was reduced from 21.38% to 11.76% over the past two decades, while dust aerosols decreased from 6.99% to 2.15%. The declining trend in anthropogenic aerosols could be attributed to reduced coal consumption and/or favorable dispersion conditions, whereas decreasing dust aerosols were largely associated with increased vegetation cover and/or weakened wind speed in the west. Overall, such advancements provide fresh evidence to improve our understanding of the emission control effect on haze pollution variations in China.}
}
@article{PENG2022102674,
title = {The relationship between soil microbial diversity and angelica planting based on network big data},
journal = {Sustainable Energy Technologies and Assessments},
volume = {53},
pages = {102674},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102674},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822007238},
author = {Yinan Peng and Ze Ye and Peng Xi and Hongshan Qi and Bin Ji and Zhiye Wang},
keywords = {Big data, Soil microorganisms, Biodiversity analysis, Angelica cultivation, Hadoop systems, Metagenome research},
abstract = {Angelica sinensis is a kind of traditional Chinese medicine with very good blood nourishing effect, and it is cultivated in many regions of China. But with the increasingly severe climate, angelica cultivation has become a big problem. Therefore, this paper starts from the soil microorganisms of angelica planting, and studies the influence of soil biodiversity and angelica planting in the context of big data. This paper proposes a Hadoop system for big data analysis, combining the biocommunity characteristics and metagenomes of soil microorganisms. It then calculates the distance between samples and generates a dissimilarity matrix. Finally, this paper proposes a soil optimization method for angelica planting based on big data analysis of soil microorganisms. In order to optimize the Angelica planting soil designed in this paper, a soil microbial genome comparison experiment and a big data concurrent control test experiment were designed in this paper. It then analyzes the data obtained from the experiment, and the results of the analysis are used to optimize the soil for angelica planting. It finally compares the soil method of Angelica planting designed in this paper with the traditional Angelica planting method. The experimental results show that the survival rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms has increased by 16.09% compared with the traditional Angelica sinensis planting site. The growth rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms increased by 9.64% compared with the traditional Angelica sinensis planting area.}
}
@incollection{DOMINGUEZ2022,
title = {Big Data applications in power systems},
booktitle = {Reference Module in Materials Science and Materials Engineering},
publisher = {Elsevier},
year = {2022},
isbn = {978-0-12-803581-8},
doi = {https://doi.org/10.1016/B978-0-12-821204-2.00073-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212042000738},
author = {Xavier Dominguez and Alvaro Prado and Pablo Arboleya},
keywords = {Artificial Intelligence, Big Data, Big Data Analytics, Data mining, Machine learning, Power systems},
abstract = {Abstract
In the last years, the term Big Data (BD) is becoming ubiquitous in almost every scientific field given the information era we are living. In this respect, power systems is not the exception. This overflood of information can be overwhelming for the different energy stakeholders, nonetheless, numerous opportunities can also be derived from this BD. To do so, it is necessary to perform the corresponding knowledge extraction to this large data. This has been referred as Big Data Analytics (BDA) in the recent literature. In this respect, this work provides a comprehensive understanding of the attributes of BD in the power sector, along with the most representative tools, techniques, applications and challenges related to the incorporation of BDA in this field.}
}
@article{GODOY2021100079,
title = {Transformations of trust in society: A systematic review of how access to big data in energy systems challenges Scandinavian culture},
journal = {Energy and AI},
volume = {5},
pages = {100079},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100079},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000331},
author = {Jaqueline de Godoy and Kathrin Otrel-Cass and Kristian Høyer Toft},
keywords = {Surveillance capitalism, Smart meters, Energy transition, Trust, Data Ethics, Big Data},
abstract = {In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users’ privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.}
}
@article{ARDAGNA2018548,
title = {Context-aware data quality assessment for big data},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {548-562},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329151},
author = {Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali},
keywords = {Data quality, Big data, Context-awareness, Data profiling, DQ assessment},
abstract = {Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.}
}
@article{ULLAH2022103294,
title = {On the scalability of Big Data Cyber Security Analytics systems},
journal = {Journal of Network and Computer Applications},
volume = {198},
pages = {103294},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103294},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002897},
author = {Faheem Ullah and M. Ali Babar},
keywords = {Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark},
abstract = {Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.}
}
@article{ALAOUI2019803,
title = {The Impact of Big Data Quality on Sentiment Analysis Approaches},
journal = {Procedia Computer Science},
volume = {160},
pages = {803-810},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317077},
author = {Imane El Alaoui and Youssef Gahi},
keywords = {Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining},
abstract = {Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.}
}
@article{MUHEIDAT202215,
title = {Emerging Concepts Using Blockchain and Big Data},
journal = {Procedia Computer Science},
volume = {198},
pages = {15-22},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024455},
author = {Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo’ai A. Tawalbeh and Mais Tawalbeh},
keywords = {Blockchain, Bigdata, Data analytics, Smart Cities applications, Healthcare},
abstract = {Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.}
}
@article{FERNANDES2022817,
title = {Big Data Analytics for Vehicle Multisensory Anomalies Detection},
journal = {Procedia Computer Science},
volume = {204},
pages = {817-824},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.099},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008377},
author = {Ana Xavier Fernandes and Pedro Guimarães and Maribel Yasmina Santos},
keywords = {Big Data Warehouse, Big Data, ETL},
abstract = {Autonomous driving is assisted by different sensors, each providing information about certain parameters. What we are looking for is an integrated perspective of all these parameters to drive us into better decisions. To achieve this goal, a system that can handle these Big Data issues regarding volume, velocity and variety is needed. This paper aims to design and develop a real-time Big Data Warehouse repository, integrating the data generated by the multiple sensors developed in the context of IVS (In-Vehicle Sensing) systems; the data to be stored in this repository should be merged, which will imply its processing, consolidation and preparation for the analytical mechanisms that will be required. This multisensory fusion is important because it allows the integration of different perspectives in terms of sensor data, since they complement each other. Therefore, it can enrich the entire analysis process at the decision-making level, for instance, understanding what is going on inside the cockpit.}
}
@article{TANG2022100289,
title = {Big Data in Forecasting Research: A Literature Review},
journal = {Big Data Research},
volume = {27},
pages = {100289},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100289},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621001064},
author = {Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang},
keywords = {Big data, Forecasting, Literature review, Prediction models, Information},
abstract = {With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching/visiting, online/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.}
}
@incollection{WICKERSHAM202359,
title = {Chapter 3 - Clinical applications of big data to child and adolescent mental health care},
editor = {Matthew Hodes and Petrus J. {De Vries}},
booktitle = {Shaping the Future of Child and Adolescent Mental Health},
publisher = {Academic Press},
pages = {59-79},
year = {2023},
isbn = {978-0-323-91709-4},
doi = {https://doi.org/10.1016/B978-0-323-91709-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917094000056},
author = {Alice Wickersham and Johnny Downs},
keywords = {Big data, Child and adolescent mental health, Data linkage, Electronic healthcare records, Natural language processing},
abstract = {Over the past twodecades, healthcare providers across the world have adopted digital methods for capturing clinical and administrative information. Clinicians take contemporaneous records of their interactions with patients, so many health service providers have accrued vast repositories of longitudinally collected data. These data, coupled with advances in data extraction methods, computer processing power, and linkage to nonhealth public services data, now provide child and adolescent mental health researchers unique opportunities for tackling a broad range of clinical questions; especially those where the considerations of scale and generalizability make individually funded studies unaffordable. However, these “big” data have their limitations. Best practice requires clinicians, informaticians, and data scientists to work together, so assumptions over data quality or validity are not misplaced. This chapter explains why the evidence base for child and adolescent mental healthcare needs big data applications as well as conventional research, to move the field forward. This chapter provides illustrations of big data applications to child and adolescent mental healthcare, primarily from England and the United Kingdom, but also offers a section on the global perspective. This chapter also reviews the methodological strengths and weaknesses of big data and describes the ethical and governance implications for their use.}
}
@article{KESKAR2022532,
title = {Perspective of anomaly detection in big data for data quality improvement},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {532-537},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.597},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321042243},
author = {Vinaya Keskar and Jyoti Yadav and Ajay Kumar},
keywords = {Credit card, Validation, LUHN, Big data, Bank},
abstract = {The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.}
}
@article{CISNEROSCABRERA2021114858,
title = {Experimenting with big data computing for scaling data quality-aware query processing},
journal = {Expert Systems with Applications},
volume = {178},
pages = {114858},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114858},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002992},
author = {Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris},
keywords = {Data quality-aware queries, Big data computing, Empirical evaluation},
abstract = {Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.}
}
@article{WANG2022663,
title = {A novel oscillation identification method for grid-connected renewable energy based on big data technology},
journal = {Energy Reports},
volume = {8},
pages = {663-671},
year = {2022},
note = {2021 International Conference on New Energy and Power Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722002682},
author = {Jian Wang},
keywords = {Oscillation identification, Big data, Evidence theory, Support vector machine},
abstract = {With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.}
}
@article{GEORGIADIS2022105640,
title = {Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review},
journal = {Computer Law & Security Review},
volume = {44},
pages = {105640},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105640},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001138},
author = {Georgios Georgiadis and Geert Poels},
keywords = {Big data analytics, Data protection, Data protection directive, General data protection regulation, Governance, Information security, Privacy, Privacy impact assessment, Systematic literature review},
abstract = {Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.}
}
@article{OESTERREICH2022128,
title = {The role of the social and technical factors in creating business value from big data analytics: A meta-analysis},
journal = {Journal of Business Research},
volume = {153},
pages = {128-149},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.08.028},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322007111},
author = {Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg and Yogesh K Dwivedi},
keywords = {Big data analytics, IT business value, Firm performance, Meta-analysis, Moderator analysis, Sociotechnical theory},
abstract = {Big data analytics (BDA) has recently gained importance as an emerging technology for handling big data. The use of advanced techniques with differing levels of intelligence, such as descriptive, predictive, prescriptive, and autonomous analytics, is expected to create value for firms. By viewing BDA as a sociotechnical system, we conduct a meta-analysis of 107 individual studies to integrate prior evidence on the role of the technical and social factors of BDA in creating BDA business value. The findings underline the predominant role of the social components in enhancing firm performance, such as the BDA system’s human factors and a nurturing organizational structure, in contrast to the minor role of the technological factors. However, both the technical and social factors are found to be strong determinants of BDA business value. Through the combined lens of sociotechnical theory and the IS business value framework, we contribute to research and practice by enhancing the understanding of the main technical and social determinants of BDA business value at the firm level.}
}
@article{KWON2014387,
title = {Data quality management, data usage experience and acquisition intention of big data analytics},
journal = {International Journal of Information Management},
volume = {34},
number = {3},
pages = {387-394},
year = {2014},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214000127},
author = {Ohbyung Kwon and Namyeon Lee and Bongsik Shin},
keywords = {Big data analytics, Resource-based view, Data quality management, IT capability, Data usage},
abstract = {Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.}
}
@article{LIU2016134,
title = {Rethinking big data: A review on the data quality and usage issues},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {134-142},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002567},
author = {Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu},
keywords = {Big data, Data quality and error, Data ethnics, Spatial information sciences},
abstract = {The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.}
}
@article{WANG2022e10312,
title = {Impact of big data resources on clinicians’ activation of prior medical knowledge},
journal = {Heliyon},
volume = {8},
number = {9},
pages = {e10312},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10312},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022016000},
author = {Sufen Wang and Junyi Yuan and Changqing Pan},
keywords = {Big data resources, Activation of prior medical knowledge, Shared big data resources, Private big data resources},
abstract = {Background
Activating prior medical knowledge in diagnosis and treatment is an important basis for clinicians to improve their care ability. However, it has not been systematically explained whether and how various big data resources affect the activation of prior knowledge in the big data environment faced by clinicians.
Objective
The aim of this study is to contribute to a better understanding on how the activation of prior knowledge of clinicians is affected by a wide range of shared and private big data resources, to reveal the impact of big data resources on clinical competence and professional development of clinicians.
Method
Through the comprehensive analysis of extant research results, big data resources are classified as big data itself, big data technology and big data services at the public and institutional levels. A survey was conducted on clinicians and IT personnel in Chinese hospitals. A total of 616 surveys are completed, involving 308 medical institutions. Each medical institution includes a clinician and an IT personnel. SmartPLS version 2.0 software package was used to test the direct impact of big data resources on the activation of prior knowledge. We further analyze their indirect impact of those big data resources without direct impact.
Results
(1) Big data quality environment at the institutional level and the big data sharing environment at the public level directly affect activation of prior medical knowledge; (2) Big data service environment at the institutional level directly affects activation of prior medical knowledge; (3) Big data deployment environment at the institutional level and big data service environment at the public level have no direct impact on activation of prior knowledge of clinicians, but they have an indirect impact through big data quality environment and service environment at the institutional level and the big data sharing environment at the public level.
Conclusions
Big data technology, big data itself and big data service at the public level and institutional level interact and influence each other to activate prior medical knowledge. This study highlights the implications of big data resources on improvement of clinicians’ diagnosis and treatment ability.}
}
@article{YANG2021100234,
title = {Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data},
journal = {Big Data Research},
volume = {25},
pages = {100234},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000514},
author = {Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino},
keywords = {Renal failure, Risk prediction, Electronic health record, Health big data, Machine learning},
abstract = {Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.}
}
@article{YAO20181,
title = {Big data quality prediction in the process industry: A distributed parallel modeling framework},
journal = {Journal of Process Control},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418300660},
author = {Le Yao and Zhiqiang Ge},
keywords = {Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics},
abstract = {With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.}
}
@article{CORTEREAL2020103141,
title = {Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103141},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308662},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira},
keywords = {Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory},
abstract = {Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.}
}
@article{HORNG202222,
title = {Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view},
journal = {Journal of Hospitality and Tourism Management},
volume = {51},
pages = {22-38},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022000389},
author = {Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu},
keywords = {Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy},
abstract = {To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.}
}
@article{LI2022129190,
title = {Big data nanoindentation characterization of cross-scale mechanical properties of oilwell cement-elastomer composites},
journal = {Construction and Building Materials},
volume = {354},
pages = {129190},
year = {2022},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2022.129190},
url = {https://www.sciencedirect.com/science/article/pii/S095006182202846X},
author = {Yucheng Li and Yunhu Lu and Li Liu and Shengmin Luo and Li He and Yongfeng Deng and Guoping Zhang},
keywords = {Big data nanoindentation, Cross-scale characterization, Elastomer, Microstructure, Oilwell cement, Young’s modulus},
abstract = {Intensive big data nanoindentation (BDNi) characterization was performed to reveal the cross-scale mechanical properties of, and hence distinguish the different phases in, inorganic–organic hybrid oilwell cement-elastomer composites, hydrothermally cured at 160 °C and 20 MPa for 28 days. Totally-three emulsified and particulate elastomers, including styrene-butadiene latex (SBL) emulsion (6, 12, and 14 wt.%), polypropylene (PP) powder (12 wt.%), and nitrile rubber (NR) powder (6 wt.%), and a weighting agent, hematite (50 wt.%), were used as additives to finely adjust the mechanical properties and microstructure of the hybrid composites, which were respectively examined by the BDNi and mercury intrusion porosimetry and scanning electron microscopy. BDNi data were statistically deconvoluted by the Gaussian mixture modeling (GMM) to discern mechanically distinct phases and their Young’s moduli and hardness at the micro/nano scale and the bulk composites’ properties at the macro scale. Results show that the SBL emulsion can be more homogeneously dispersed into the cement matrix, due to its emulsified soft consistency and hydrophilicity, resulting in the formation of soft coatings on, and softer infills intermixed with, the cement hydration products (CHPs). In contrast, the two hydrophobic, inert, particulate elastomers, PP and NR powders, only act as isolated soft inclusions embedded in the hydrated cement matrix. The NR melts at high temperatures and permeates into the pores of the cement matrix, leading to the formation of complex intervened micromorphology and hence functions better than the PP. All elastomers can effectively reduce the composites’ Young’s moduli: with increasing the elastomer contents, while the modulus of a BDNi-identified major CHP phase decreases from 20.9 to 11.3 GPa, the bulk composites’ counterpart from 17.3 to 10.7 GPa. The BDNi enables the identification of multiple mechanically distinct phases in the hybrid composites and quantification of the property changes of these phases.}
}
@article{ALI2021101600,
title = {Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101600},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101600},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000368},
author = {Hamza Ali and Ryad Titah},
keywords = {Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government},
abstract = {It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.}
}
@article{GUPTA2021120986,
title = {Big data and firm marketing performance: Findings from knowledge-based view},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120986},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120986},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004182},
author = {Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen},
keywords = {Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view},
abstract = {A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.}
}
@article{PENG2022104,
title = {Industrial big data-driven mechanical performance prediction for hot-rolling steel using lower upper bound estimation method},
journal = {Journal of Manufacturing Systems},
volume = {65},
pages = {104-114},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S027861252200142X},
author = {Gongzhuang Peng and Yinliang Cheng and Yufei Zhang and Jian Shao and Hongwei Wang and Weiming Shen},
keywords = {Industrial big data, Mechanical performances prediction, Lower upper bound estimation, Broad learning system, Hot-rolling},
abstract = {Industrial big data technology has become one of the important driving forces to intelligent manufacturing in the steel industry. In this study, the characteristics of data in steel production are analyzed and an industrial big data platform for steeling process is developed to extract the quality-related parameters. A data-driven approach to construct prediction intervals (PIs) of mechanical performances for hot-rolling strips is proposed to represent the uncertainty and reliability of the prediction results. The proposed method employs a new manifold visualization method, SLISEMAP, to reduce the feature dimensions with interpretability, utilizes lower upper bound estimation (LUBE) method to obtain the PIs, in which the broad learning system (BLS) is used as the basic training network model and the artificial bee colony (ABC) algorithm is applied to optimize the weighting parameters of BLS under the LUBE framework. A hot-rolling steel coil dataset consisting of 39 variables and 1335 coil samples is used to validate the proposed method. Two Delta-based approaches, namely back propagation neural network (BPNN) and extreme learning machine (ELM); and three LUBE-based approaches, namely ABC-BPNN, ABC-ELM, and ABC-support vector regression (SVR) are compared with the proposed method. Results show that the proposed ABC-BLS in LUBE is effective and efficient in constructing the PIs with a higher coverage probability and a narrower interval width.}
}
@article{CECH20211947,
title = {Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {1947-1953},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00061f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008856},
author = {Nadja B. Cech and Marnix H. Medema and Jon Clardy},
abstract = {ABSTRACT
Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.}
}
@article{HAZEN201472,
title = {Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications},
journal = {International Journal of Production Economics},
volume = {154},
pages = {72-80},
year = {2014},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314001339},
author = {Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer},
keywords = {Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory},
abstract = {Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.}
}
@article{OUAFIQ2022102093,
title = {AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
journal = {Sustainable Energy Technologies and Assessments},
volume = {52},
pages = {102093},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S221313882200145X},
author = {El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon},
keywords = {Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability},
abstract = {The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.}
}
@article{XU2022,
title = {Smart breeding driven by big data, artificial intelligence and integrated genomic-enviromic prediction},
journal = {Molecular Plant},
year = {2022},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2022.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674205222002957},
author = {Yunbi Xu and Xingping Zhang and Huihui Li and Hongjian Zheng and Jianan Zhang and Michael S. Olsen and Rajeev K. Varshney and Boddupalli M. Prasanna and Qian Qian},
keywords = {smart breeding, genomic selection, integrated genomic-enviromic selection, spatiotemporal omics, crop design, machine and deep learning, big data, artificial intelligence},
abstract = {The first paradigm of plant breeding involves direct selection based phenotypic observation, followed by predictive breeding using statistical models constructed for quantitative traits based on genetic experimental design and more recently by incorporating molecular marker genotypes. However, plant performance or phenotype (P) is determined by the combining effects of genotype (G), envirotype (E) and genotype by environment interaction (GEI). Phenotypes can be predicted more precisely by training a model using data collected from multiple sources, including spatiotemporal omics (genomics, phenomics and enviromics across time and space). Integration of 3D information profiles (G-P-E), each with multidimensionality, provides predictive breeding with both tremendous opportunities and great challenges. Here, we first review innovative technologies for predictive breeding. We then evaluate multidimensional information profiles that can be integrated with a predictive breeding strategy, particularly envirotypic data, which have largely been neglected in data collection and nearly untouched in model construction. We propose a smart breeding scheme, integrated genomic-enviromic prediction (iGEP), as an extension of genomic prediction, using integrated multiomics information, big data technology and artificial intelligence (mainly focus on machine and deep learning). How to implement iGEP was discussed, including spatiotemporal models, environmental indices, factorial and spatiotemporal structure of plant breeding data, and cross-species prediction. A strategy is then proposed for prediction-based crop redesign at both the macro (individual, population and species) and micro (gene, metabolism and network) scales. Finally, we provide perspectives on translating the smart breeding into genetic gain through integrative breeding platforms and open-source breeding initiatives. We call for coordinated efforts in smart breeding through iGEP, institutional partnerships, and innovative technological support.}
}
@article{BENITEZHIDALGO2021107489,
title = {TITAN: A knowledge-based platform for Big Data workflow management},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107489},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107489},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007516},
author = {Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado},
keywords = {Big Data analytics, Semantics, Knowledge extraction},
abstract = {Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.}
}
@article{YANG2021107550,
title = {Optimal timing of big data application in a two-period decision model with new product sales},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107550},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107550},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100454X},
author = {Lei Yang and Anqian Jiang and Jiahua Zhang},
keywords = {Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare},
abstract = {We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.}
}
@article{RAUT2021103368,
title = {Big data analytics: Implementation challenges in Indian manufacturing supply chains},
journal = {Computers in Industry},
volume = {125},
pages = {103368},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103368},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306023},
author = {Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede},
keywords = {Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis},
abstract = {Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.}
}
@article{SILVA2019532,
title = {Risk Analysis of Using Big Data in Computer Sciences},
journal = {Procedia Computer Science},
volume = {160},
pages = {532-537},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317521},
author = {Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández},
keywords = {Data management, data quality, decision making, data analysis},
abstract = {Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.}
}
@article{ZHANG2022101626,
title = {Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development},
journal = {Government Information Quarterly},
volume = {39},
number = {1},
pages = {101626},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101626},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000629},
author = {Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui},
keywords = {Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues},
abstract = {Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.}
}
@article{LIU2022103622,
title = {scenario modeling for government big data governance decision-making: Chinese experience with public safety services},
journal = {Information & Management},
volume = {59},
number = {3},
pages = {103622},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103622},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000349},
author = {Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU},
keywords = {Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services},
abstract = {In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.}
}
@article{KOZIARA202184,
title = {Introduction to Big Data in trauma and orthopaedics},
journal = {Orthopaedics and Trauma},
volume = {35},
number = {2},
pages = {84-89},
year = {2021},
issn = {1877-1327},
doi = {https://doi.org/10.1016/j.mporth.2021.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S187713272100004X},
author = {Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley},
keywords = {Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy},
abstract = {The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.}
}
@article{LEI2021101570,
title = {Modelling and analysis of big data platform group adoption behaviour based on social network analysis},
journal = {Technology in Society},
volume = {65},
pages = {101570},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101570},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21000452},
author = {Zhimei Lei and Yandan Chen and Ming K. Lim},
keywords = {Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis},
abstract = {Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks—i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network—are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.}
}
@article{YOUSSEF2022102827,
title = {Cross-national differences in big data analytics adoption in the retail industry},
journal = {Journal of Retailing and Consumer Services},
volume = {64},
pages = {102827},
year = {2022},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102827},
url = {https://www.sciencedirect.com/science/article/pii/S0969698921003933},
author = {Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag},
keywords = {Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry},
abstract = {Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.}
}
@article{LACAM2021100406,
title = {Big data and Smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop},
journal = {The Journal of High Technology Management Research},
volume = {32},
number = {1},
pages = {100406},
year = {2021},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2021.100406},
url = {https://www.sciencedirect.com/science/article/pii/S1047831021000031},
author = {Jean-Sébastien Lacam and David Salvetat},
keywords = {Big data, Smart data, Volume, Velocity, Variety, Automotive distribution},
abstract = {This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.}
}
@article{HE2022112372,
title = {A rule-based data preprocessing framework for chiller rooms inspired by the analysis of engineering big data},
journal = {Energy and Buildings},
volume = {273},
pages = {112372},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112372},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822005436},
author = {Ruikai He and Tong Xiao and Shunian Qiu and Jiefan Gu and Minchen Wei and Peng Xu},
keywords = {Data pre-processing, Big engineering data, Building energy management},
abstract = {The rapid development of building energy consumption monitoring platforms makes engineering data more diverse, which facilitates the goal of reducing emissions. It is increasingly acknowledged that data preprocessing deserves the same attention as intelligent algorithms. In this work, the data quality issue of the engineering big data from non-demonstration complexes in China are analyzed thoroughly, and the analysis is based on clustering-based algorithms. We can conclude that the data of the hourly power of equipment groups are quality and stable, which is suitable for the benchmark to check other data. The quality of the data about pipes is acceptable. The number of data types about cooling towers is less, and the quality is worse. Regarding other data, the quality is unstable, so researchers should deal with those case-by-case. According to the above analysis, we proposed a convenient, rule-based data preprocessing framework that utilizes the law of physics, ensuring the strong coupling of multi-variants. After the data preprocessing, these engineering data are more reliable and can be used to improve performance or train models. Additionally, the proposed framework is more suitable for preprocessing multi-variant engineering data.}
}
@article{LEEPOST2019113135,
title = {Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development},
journal = {Decision Support Systems},
volume = {126},
pages = {113135},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113135},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301642},
author = {Anita Lee-Post and Ram Pakath},
keywords = {Data quality, Big data, Secondary data, Numerical data, Quality threshold},
abstract = {An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.}
}
@article{DEEPA2022209,
title = {A survey on blockchain for big data: Approaches, opportunities, and future directions},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {209-226},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000243},
author = {N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana},
keywords = {Blockchain, Big data, Vertical applications, Smart city, Smart healthcare, Smart transportation, Security},
abstract = {Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.}
}
@article{GHALLAB2020131,
title = {Detection outliers on internet of things using big data technology},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {3},
pages = {131-138},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519301616},
author = {Haitham Ghallab and Hanan Fahmy and Mona Nasr},
keywords = {Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs},
abstract = {Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.}
}
@article{KASTOUNI20222758,
title = {Big data analytics in telecommunications: Governance, architecture and use cases},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part A},
pages = {2758-2770},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S131915782030553X},
author = {Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}},
keywords = {Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases},
abstract = {With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.}
}
@article{PICCAROZZI20221746,
title = {The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies},
journal = {Procedia Computer Science},
volume = {200},
pages = {1746-1755},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.375},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003842},
author = {Michela Piccarozzi and Barbara Aquilani},
keywords = {Big Data, Covid-19, systematic literature review, management},
abstract = {2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.}
}
@article{DHAND2022101010,
title = {Deep enriched salp swarm optimization based bidirectional -long short term memory model for healthcare monitoring system in big data},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101010},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101010},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001538},
author = {Geetika Dhand and Kavita Sheoran and Parul Agarwal and Siddhartha Sankar Biswas},
keywords = {Big data, Enriched sea salp optimization, Wearable sensors, Health care monitoring systems},
abstract = {The rapid development of communication technologies and expert systems have resulted in a large volume of medical data. Big data such as clinical data, omics data, and electronic health data are difficult to manage in real-time due to noise, large size, different formats, missing values and large features. Hence, it is more difficult for the health monitoring system to extract the correct information. Low quality and noisy data can lead to unnecessary treatment. To overcome these issues, we proposed Enriched Salp Swarm Optimization based Bidirectional Long Short Term Memory (ESSOBiLSTM) to monitor health. This method consists of four layers, such as the data collection layer, data storage layer, data analytics, and presentation layer. The initial layer handles a variety of information from main sources: wearable sensor devices (WSD), social network data, and medical records (MR). The second layer stores all the collected data from WSD, MR, and social network data to the cloud server through the wireless network. The proposed framework for performing big data analytics steps like preprocessing, filtering, dimensionality reduction, and classification is performed in the third layer. In the final layer, the doctor analyzes the patient's condition based on the classification results of the enriched SSO-BiLSTM. Based on the evaluation report, the proposed ESSOBiLSTM gives an accuracy of 85%, precision of 80%, RMSE of 0.6, MAE of 0.58, recall of 85% and F-measure of 79%. As a result, ESSOBiLSTM has proven to be more effective in monitoring health in large datasets.}
}
@article{RIDZUAN2022685,
title = {Diagnostic analysis for outlier detection in big data analytics},
journal = {Procedia Computer Science},
volume = {197},
pages = {685-692},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024133},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {Big data, data quality, outlier, Sustainable Development Goals},
abstract = {Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.}
}
@article{SFAXI2020101862,
title = {DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101862},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101862},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19303830},
author = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
keywords = {Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality},
abstract = {Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.}
}
@article{GOH20221029,
title = {Are batch effects still relevant in the age of big data?},
journal = {Trends in Biotechnology},
volume = {40},
number = {9},
pages = {1029-1040},
year = {2022},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2022.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167779922000361},
author = {Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong},
keywords = {artificial intelligence, batch effect, machine learning, RNA sequencing, single cell},
abstract = {Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.}
}
@incollection{BIRKIN2020303,
title = {Big Data},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-311},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10616-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102295510616X},
author = {Mark Birkin},
keywords = {Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information},
abstract = {Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.}
}
@article{LI2021102064,
title = {Big data driven vehicle battery management method: A novel cyber-physical system perspective},
journal = {Journal of Energy Storage},
volume = {33},
pages = {102064},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2020.102064},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X20318971},
author = {Shuangqi Li and Pengfei Zhao},
keywords = {Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning},
abstract = {The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.}
}
@article{CAISSIE2022100925,
title = {Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium},
journal = {Advances in Radiation Oncology},
pages = {100925},
year = {2022},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2022.100925},
url = {https://www.sciencedirect.com/science/article/pii/S245210942200032X},
author = {Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo},
abstract = {Purpose/Objective
Outside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this “Big Data” study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.
Materials/Methods
Dosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.
Results
Plans for 4664 patients treated to a mean plan dose of 64.4 ± 13.2 Gy in 32 ± 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.
Conclusion
Quantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.}
}
@article{SURBAKTI2020103146,
title = {Factors influencing effective use of big data: A research framework},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103146},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308649},
author = {Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq},
keywords = {Big data, Effective use, Factors, Framework},
abstract = {Information systems (IS) research has explored “effective use” in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.}
}
@article{RIDZUAN2019731,
title = {A Review on Data Cleansing Methods for Big Data},
journal = {Procedia Computer Science},
volume = {161},
pages = {731-738},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919318885},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {data cleansing, big data, data quality},
abstract = {Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.}
}
@article{XIN2022100197,
title = {Review on A big data-based innovative knowledge teaching evaluation system in universities},
journal = {Journal of Innovation & Knowledge},
volume = {7},
number = {3},
pages = {100197},
year = {2022},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2022.100197},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X22000373},
author = {Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan},
keywords = {Big data, Knowledge teaching evaluation, Performance management},
abstract = {With the widespread use of digital technologies such as big data, cloud computing and artificial intelligence in higher education, how to establish a scientific and systematic evaluation system to turn the traditional classroom with the one-way transmission of knowledge into an interactive space for exchanging ideas and inspiring wisdom has become an essential task for human resource management in universities, and a key to improving teaching quality. However, due to the debate between scientism and humanism in teaching evaluation, studies related to teaching performance have been isolated from human resource management, resulting in the lack of a systematic vision and framework for such studies. Relevant studies are still limited to the evaluation contents of different evaluation subjects. Evaluations also tend to focus only on the teaching process, ignoring the objectives of talent training, making it difficult for evaluations to play a goal-oriented role and hindering the further development of relevant studies. Therefore, this paper draws on human resource management methodologies and analyzes knowledge teaching evaluation system characteristics in colleges and universities in a big data context to construct a “multiple evaluations, trinity and four-step closed-loop” big data-based knowledge teaching evaluation system. “Trinity” represents evaluation from three performance dimensions: teaching effect, teaching behavior and teaching ability. “Multiple evaluations” represents the design of teaching performance indicators based on teaching data, breaking the barriers between different evaluation subjects. “Four-step closed-loop” draws on performance management theory to standardize the teaching performance management process from four aspects: planning, implementation, evaluation, and feedback. This evaluation system provides a systematic methodology for unifying the theory and practice of innovative knowledge teaching evaluation system in universities in a big data context.}
}
@article{SUN2022112331,
title = {Understanding building energy efficiency with administrative and emerging urban big data by deep learning in Glasgow},
journal = {Energy and Buildings},
volume = {273},
pages = {112331},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112331},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822005023},
author = {Maoran Sun and Changyu Han and Quan Nie and Jingying Xu and Fan Zhang and Qunshan Zhao},
keywords = {Building energy efficiency, Energy performance certificate, Deep learning, Google street view, SHapley additive explanations},
abstract = {With buildings consuming nearly 40% of energy in developed countries, it is important to accurately estimate and understand the building energy efficiency in a city. A better understanding of building energy efficiency is beneficial for reducing overall household energy use and providing guidance for future housing improvement and retrofit. In this research, we propose a deep learning-based multi-source data fusion framework to estimate building energy efficiency. We consider the traditional factors associated with the building energy efficiency from the Energy Performance Certificate (EPC) for 160,000 properties (30,000 buildings) in Glasgow, UK (e.g., property structural attributes and morphological attributes), as well as the Google Street View (GSV) building façade images as a complement. We compare the performance improvements between our data-fusion framework with traditional morphological attributes and image-only models. The results show that including the building façade images from GSV, the overall model accuracy increases from 79.7% to 86.8%. A further investigation and explanation of the deep learning model are conducted to understand the relationships between building features and building energy efficiency by using SHapley Additive exPlanations (SHAP). Our research demonstrates the potential of using multi-source data in building energy efficiency prediction with high accuracy and short inference time. Our paper also helps understand building energy efficiency at the city level to help achieve the net-zero target by 2050.}
}
@article{YE2021106293,
title = {Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {209},
pages = {106293},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106293},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003679},
author = {Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang},
keywords = {Integrated learning, Health care system, Elaboration Likelihood Machine, System design, Medical big data, Internet of Medical Things},
abstract = {Purpose
We present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.
Method
The method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.
Results
The HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450 ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3×28TB.
Conclusion
The experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.}
}
@incollection{BUDNITZ2021665,
title = {Transport Modes and Big Data},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {665-670},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10601-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717106013},
author = {Hannah D Budnitz and Emmanouil Tranos and Lee Chapman},
keywords = {Application programming interfaces, Automation, Big data, Crowd-sourced, Digitization, Geolocation, Information and communication technology, Intelligent transport systems, Internet of things, Location-based services, Mobility as a service, Real time information systems, Timestamp, Vehicle telematics},
abstract = {Transport modes and big data considers the characteristics of “Big Data” as described by the 5 “Vs,” Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.}
}
@article{LI2022101021,
title = {A review of industrial big data for decision making in intelligent manufacturing},
journal = {Engineering Science and Technology, an International Journal},
volume = {29},
pages = {101021},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001336},
author = {Chunquan Li and Yaqiong Chen and Yuling Shang},
keywords = {Intelligent manufacturing, Artificial intelligence, Industrial big data, Big data-driven technology, Decision-making},
abstract = {Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.}
}
@article{LIN2022103680,
title = {How big data analytics enables the alliance relationship stability of contract farming in the age of digital transformation},
journal = {Information & Management},
volume = {59},
number = {6},
pages = {103680},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103680},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000891},
author = {Shunzhi Lin and Jiabao Lin and Feiyun Han and Xin (Robert) Luo},
keywords = {Big data analytics, Risk management capability, Data quality, Alliance relationship stability},
abstract = {Notwithstanding the potential of big data analytics technology for alliance management, there is a lack of understanding of how such digital technology influences alliance relationship stability (ARS). Drawing on the information technology-enabled organizational capabilities (IT-enabled OCs) perspective, this study empirically verifies that big data analytics promotes ARS and risk management capability. Moreover, market risk management capability (MRM) enhances ARS, and data quality moderates the relationship between big data analytics usage (BDU) and MRM. This research reveals the impact mechanism of BDU on the ARS. Implications for management and future research are presented as well.}
}
@article{KONG2020123142,
title = {A systematic review of big data-based urban sustainability research: State-of-the-science and future directions},
journal = {Journal of Cleaner Production},
volume = {273},
pages = {123142},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123142},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620331875},
author = {Lingqiang Kong and Zhifeng Liu and Jianguo Wu},
keywords = {Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning},
abstract = {The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.}
}
@article{LIU202053,
title = {Semantic-aware data quality assessment for image big data},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {53-65},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302304},
author = {Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu},
keywords = {Semantic-aware, Quality assessment, Image big data, IDSTH, SHR},
abstract = {Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.}
}
@article{LI2022121355,
title = {Evaluating the impact of big data analytics usage on the decision-making quality of organizations},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121355},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121355},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521007861},
author = {Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo},
keywords = {Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms},
abstract = {Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.}
}
@article{CORALLO2022102331,
title = {Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {76},
pages = {102331},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102331},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000205},
author = {Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi},
keywords = {Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection},
abstract = {Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.}
}
@article{SCHMUCKER2022100061,
title = {Measuring tourism with big data? Empirical insights from comparing passive GPS data and passive mobile data},
journal = {Annals of Tourism Research Empirical Insights},
volume = {3},
number = {2},
pages = {100061},
year = {2022},
issn = {2666-9579},
doi = {https://doi.org/10.1016/j.annale.2022.100061},
url = {https://www.sciencedirect.com/science/article/pii/S2666957922000295},
author = {Dirk Schmücker and Julian Reif},
keywords = {Big Data, Mobile Network Data, Passive GPS Data, Spatio-temporal behaviour, Tourist classification},
abstract = {In this paper we aim to classify digital data sources for the measurement of tourist mobility, to establish a set of assessment indicators, and to compare two Big Data sources to gain empirical insights into how we can measure tourism with Big Data. For three holiday destinations in Germany, passive mobile data and passive global positioning systems (GPS) data are compared with reference data from the destinations for twelve weeks in the summer of 2019. Results show that mobile network data are on a plausible level compared to the local reference data and are able to predict the temporal pattern to a very high degree. GPS app-based data also perform well, but are less plausible and precise than mobile network data.}
}
@article{NIKOLOV2021100440,
title = {Conceptualization and scalable execution of big data workflows using domain-specific languages and software containers},
journal = {Internet of Things},
volume = {16},
pages = {100440},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100440},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000834},
author = {Nikolay Nikolov and Yared Dejene Dessalk and Akif Quddus Khan and Ahmet Soylu and Mihhail Matskin and Amir H. Payberah and Dumitru Roman},
keywords = {Big data workflows, Internet of Things, Domain-specific languages, Software containers},
abstract = {Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.}
}
@article{SHEN2022102529,
title = {Personal big data pricing method based on differential privacy},
journal = {Computers & Security},
volume = {113},
pages = {102529},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102529},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003539},
author = {Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang},
keywords = {Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation},
abstract = {Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.}
}
@article{JOSE2022100081,
title = {Integrating big data and blockchain to manage energy smart grids—TOTEM framework},
journal = {Blockchain: Research and Applications},
volume = {3},
number = {3},
pages = {100081},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000227},
author = {Dhanya Therese Jose and Jørgen Holme and Antorweep Chakravorty and Chunming Rong},
keywords = {Blockchain, Big data analytic, Hyperledger fabric, Hadoop, MapReduce, Docker, PIVT},
abstract = {The demand for electricity is increasing exponentially day by day, especially with the arrival of electric vehicles. In the smart community neighborhood project, electricity should be produced at the household or community level and sold or bought according to the demands. Since the actors can produce, sell, and buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways, such as machine learning for analyzing the household data for customer demand and peak hours for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensuring data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here, in this article, we will show the importance of the TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.}
}