@article{FOSSOWAMBA2015234,
title = {How ‘big data’ can make big impact: Findings from a systematic review and a longitudinal case study},
journal = {International Journal of Production Economics},
volume = {165},
pages = {234-246},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004253},
author = {Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou},
keywords = {‘Big data’, Analytics, Business value, Issues, Case study, Emergency services, Literature review},
abstract = {Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.}
}
@article{WANG2016747,
title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
journal = {Information Sciences},
volume = {367-368},
pages = {747-765},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516304868},
author = {Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu},
keywords = {Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science},
abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.}
}
@article{LAU2019209,
title = {Pitfalls in big data analysis: next-generation technologies, last-generation data},
journal = {Diagnostic Microbiology and Infectious Disease},
volume = {94},
number = {2},
pages = {209-210},
year = {2019},
issn = {0732-8893},
doi = {https://doi.org/10.1016/j.diagmicrobio.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732889318306710},
author = {Susanna K.P. Lau and Patrick C.Y. Woo}
}
@article{MIKALEF2020103237,
title = {Big data and business analytics: A research agenda for realizing business value},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103237},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103237},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619310687},
author = {Patrick Mikalef and Ilias O. Pappas and John Krogstie and Paul A. Pavlou}
}
@article{GENENDERFELTHEIMER2018112,
title = {Visualizing High Dimensional and Big Data},
journal = {Procedia Computer Science},
volume = {140},
pages = {112-121},
year = {2018},
note = {Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319811},
author = {Amy Genender-Feltheimer},
keywords = {Data Visualization, Dimensionality Reduction, Parallel Processing, Hadoop, Machine Learning, Big Data},
abstract = {The amount of data created by people, machines and corporations around the world is growing every year. Thanks to innovations such as the Internet of Things, this trend will continue, giving rise to the creation of Big Data. Data visualization leverages principles of visual psychology to help stakeholders identify patterns, trends and correlations that might go undetected in text-based or spreadsheet data. The return on investment (ROI) of big data visualization is well-documented in numerous studies and use cases. However, to achieve ROI from analytics investments, key insights must be uncovered, understood and communicated. Synthesizing huge quantities of data into key insights grows more challenging as data volumes and varieties increase. To address visualization challenges posed by big and high-dimensional data, this paper explores algorithms and techniques that compress the amount of data and/or reduce the number of attributes to be analyzed and visualized. Specifically, this paper examines applying dimensionality reduction and data compression algorithms to reduce attributes, tuples and data points returned to the visualization. By reducing data returned to the visualization, trends, patterns and correlations are easier to view and visualization tool performance is optimized.}
}
@article{CHENG20181,
title = {Data and knowledge mining with big data towards smart production},
journal = {Journal of Industrial Information Integration},
volume = {9},
pages = {1-13},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300584},
author = {Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao},
keywords = {Big data, Data mining techniques (DMTs), Production management, Smart manufacturing, Statistical analysis, Knowledge discovery},
abstract = {Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.}
}
@article{PAPANAGNOU2018343,
title = {Coping with demand volatility in retail pharmacies with the aid of big data exploration},
journal = {Computers & Operations Research},
volume = {98},
pages = {343-354},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817302162},
author = {Christos I. Papanagnou and Omeiza Matthews-Amune},
keywords = {Retail pharmacy, Data mining, Time series, Forecasting, Big data, Demand uncertainty},
abstract = {Data management tools and analytics have provided managers with the opportunity to contemplate inventory performance as an ongoing activity by no longer examining only data agglomerated from ERP systems, but also, considering internet information derived from customers’ online buying behaviour. The realisation of this complex relationship has increased interest in business intelligence through data and text mining of structured, semi-structured and unstructured data, commonly referred to as “big data” to uncover underlying patterns which might explain customer behaviour and improve the response to demand volatility. This paper explores how sales structured data can be used in conjunction with non-structured customer data to improve inventory management either in terms of forecasting or treating some inventory as “top-selling” based on specific customer tendency to acquire more information through the internet. A medical condition is considered - namely pain - by examining 129 weeks of sales data regarding analgesics and information seeking data by customers through Google, online newspapers and YouTube. In order to facilitate our study we consider a VARX model with non-structured data as exogenous to obtain the best estimation and we perform tests against several univariate models in terms of best fit performance and forecasting.}
}
@article{BLAZQUEZ201899,
title = {Big Data sources and methods for social and economic analyses},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {99-113},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517310946},
author = {Desamparados Blazquez and Josep Domenech},
keywords = {Big Data architecture, Forecasting, Nowcasting, Data lifecycle, Socio-economic data, Non-traditional data sources, Non-traditional analysis methods},
abstract = {The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.}
}
@incollection{VALEEV2021209,
title = {Chapter 6 - Big data analytics and process safety},
editor = {Sagit Valeev and Natalya Kondratyeva},
booktitle = {Process Safety and Big Data},
publisher = {Elsevier},
pages = {209-270},
year = {2021},
isbn = {978-0-12-822066-5},
doi = {https://doi.org/10.1016/B978-0-12-822066-5.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220665000017},
author = {Sagit Valeev and Natalya Kondratyeva},
keywords = {Analytics, Machine learning, Prediction, Clustering, Classification, Regression, Time series, Text analysis, Image analysis},
abstract = {The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.}
}
@article{PHILIPCHEN2014314,
title = {Data-intensive applications, challenges, techniques and technologies: A survey on Big Data},
journal = {Information Sciences},
volume = {275},
pages = {314-347},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514000346},
author = {C.L. {Philip Chen} and Chun-Yang Zhang},
keywords = {Big Data, Data-intensive computing, e-Science, Parallel and distributed computing, Cloud computing},
abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.}
}
@article{CANO201736,
title = {Perspectives on Big Data applications of health information},
journal = {Current Opinion in Systems Biology},
volume = {3},
pages = {36-42},
year = {2017},
note = {• Mathematical modelling • Mathematical modelling, Dynamics of brain activity at the systems level • Clinical and translational systems biology},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300409},
author = {Isaac Cano and Akos Tenyi and Emili Vela and Felip Miralles and Josep Roca},
keywords = {Digital health, Secondary use of data, Health analytics, Predictive modeling, Health forecasting},
abstract = {Recent advances on prospective monitoring and retrospective analysis of health information at national or regional level are generating high expectations for the application of Big Data technologies that aim to analyze at real time high-volumes and/or complex of data from healthcare delivery (e.g., electronic health records, laboratory and radiology information, electronic prescriptions, etc.) and citizens' lifestyles (e.g., personal health records, personal monitoring devices, social networks, etc.). Along these same lines, advances in the field of genomics are revolutionizing biomedical research, both in terms of data volume and prospects, as well as in terms of the social impact it entails. The potential of Big Data applications that consider all of the above levels of health information lies in the possibility of combining and integrating de-identified health information to allow secondary uses of data. This is the use and re-use of various sources of health information for purposes in addition to the direct clinical care of specific patients or the direct investigation of specific biomedical research hypotheses. Current applications include: epidemiological and pharmacovigilance studies, facilitating recruitment to randomized controlled trials, carrying out audits and benchmarking studies, financial and service planning, and ultimately supporting the generation of novel biomedical research outcomes.}
}
@article{BIVAND201887,
title = {Big data sampling and spatial analysis: “which of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?”},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {87-91},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300579},
author = {Roger Bivand and Konstantin Krivoruchko},
keywords = {Change of support, Sampling design, Data transformation, Prediction standard error},
abstract = {Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.}
}
@article{DALLAVALLE201876,
title = {Social media big data integration: A new approach based on calibration},
journal = {Expert Systems with Applications},
volume = {111},
pages = {76-90},
year = {2018},
note = {Big Data Analytics for Business Intelligence},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417308667},
author = {Luciana {Dalla Valle} and Ron Kenett},
keywords = {Bayesian networks, Calibration, Data integration, Social media, Information quality (InfoQ), Resampling techniques},
abstract = {In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.}
}
@article{THADURI2015457,
title = {Railway Assets: A Potential Domain for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {53},
pages = {457-467},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018268},
author = {Adithya Thaduri and Diego Galar and Uday Kumar},
keywords = {Big Data, Railways, Maintenance, Transportation},
abstract = {Two concepts currently at the leading edge of todays information technology revolution are Analytics and Big Data. The public transportation industry has been at the forefront in utilizing and implementing Analytics and Big Data, from ridership forecasting to transit operations Rail transit systems have been especially involved with these IT concepts, and tend to be especially amenable to the advantages of Analytics and Big Data because they are generally closed systems that involve sophisticated processing of large volumes of data. The more that public transportation professionals and decision makers understand the role of Analytics and Big Data in their industry in perspective, the more effectively they will be able to utilize its promise. This paper gives an overview of Big Data technologies in context of transportation with specific to Railways. This paper also gives an insight on how the existing data modules from the transport authority combines Big Data and how can be incorporated in providing maintenance decision making.}
}
@article{MOKTADIR20191063,
title = {Barriers to big data analytics in manufacturing supply chains: A case study from Bangladesh},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1063-1075},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301505},
author = {Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla},
keywords = {AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication technology (ICT), Manufacturing supply chains},
abstract = {Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.}
}
@article{BALA2017114,
title = {A Fine‐Grained Distribution Approach for ETL Processes in Big Data Environments},
journal = {Data & Knowledge Engineering},
volume = {111},
pages = {114-136},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16300611},
author = {Mahfoud Bala and Omar Boussaid and Zaia Alimazighi},
keywords = {Data Warehousing, ETL, Parallel and Distributed Processing, Big Data, MapReduce},
abstract = {Among the so-called “4Vs” (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of “Volume” in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the “process” level for coarse-grained distribution and the “functionality” and “elementary functions” levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.}
}
@article{GUPTA2019466,
title = {Circular economy and big data analytics: A stakeholder perspective},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {466-474},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314488},
author = {Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez}},
keywords = {Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability},
abstract = {The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.}
}
@article{KUM2015127,
title = {Using big data for evidence based governance in child welfare},
journal = {Children and Youth Services Review},
volume = {58},
pages = {127-136},
year = {2015},
issn = {0190-7409},
doi = {https://doi.org/10.1016/j.childyouth.2015.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0190740915300591},
author = {Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan},
keywords = {Big data, Evidence based governance, Knowledge discovery and data mining (KDD), Data science, Population informatics, Policy informatics, Academic government partnership, Administrative data},
abstract = {Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information – big data – can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency–university partnerships and research-to-practice.}
}
@article{PERRONS2015117,
title = {Data as an asset: What the oil and gas sector can learn from other industries about “Big Data”},
journal = {Energy Policy},
volume = {81},
pages = {117-121},
year = {2015},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2015.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0301421515000932},
author = {Robert K. Perrons and Jesse W. Jensen},
keywords = {Big data, Oil and gas, Information technologies, Data},
abstract = {The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.}
}
@article{SHIN2015311,
title = {Ecological views of big data: Perspectives and issues},
journal = {Telematics and Informatics},
volume = {32},
number = {2},
pages = {311-320},
year = {2015},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2014.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0736585314000665},
author = {Dong-Hee Shin and Min Jae Choi},
keywords = {Big data, Data ecosystem, Ecology, South Korea, Socio-technical perspective, Big data policy, Big data for development},
abstract = {From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.}
}
@article{MEHMOOD20151107,
title = {Big Data Logistics: A health-care Transport Capacity Sharing Model},
journal = {Procedia Computer Science},
volume = {64},
pages = {1107-1114},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.566},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027015},
author = {Rashid Mehmood and Gary Graham},
keywords = {future city, Big data, transport operation management, healthcare informationsystems, integrated systems, shared resources},
abstract = {The growth of cities in the 21st century has put more pressure on resources and conditions of urban life. There are several reasons why the health-care industry is the focus of this investigation. For instance, in the UK various studies point to the lack of failure of basic quality control procedures and misalignment between customer needs and provider services and duplication of logistics practices. The development of smart cities and big data present unprecedented challenges and opportunities for operations managers; they need to develop new tools and techniques for network planning and control. Our paper aims to make a contribution to big data and city operations theory by exploring how big data can lead to improvements in transport capacity sharing. We explore using Markov models the integration of big data with future city (health-care) transport sharing. A mathematical model was designed to illustrate how sharing transport load (and capacity) in a smart city can improve efficiencies in meeting demand for city services. The results from our analysis of 13 different sharing/demand scenarios are presented. A key finding is that the probability for system failure and performance variance tends to be highest in a scenario of high demand/zero sharing.}
}
@article{JIA20191652,
title = {Opportunities and challenges of using big data for global health},
journal = {Science Bulletin},
volume = {64},
number = {22},
pages = {1652-1654},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319305523},
author = {Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang}
}
@article{RAIKOV2016147,
title = {Big Data Refining on the Base of Cognitive Modeling},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {32},
pages = {147-152},
year = {2016},
note = {Cyber-Physical & Human-Systems CPHS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.12.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316328774},
author = {Alexander N. Raikov and Z. Avdeeva and A. Ermakov},
keywords = {data refining, cognitive modeling, Big Data, intellectual agents, networked expertise},
abstract = {Abstract:
In conditions of rapid external changes the requirement to quality of control of purposeful development of complex system (states, regions, corporations etc.) dramatically increases. Automation support of the key stages of decision making process is one of the ways to cope with the challenges. This paper focuses on the approach based on the Big Data Refining during cognitive modeling that proves the correctness of modeling and decision-making. The approach uses the requests to the Big Data for cognitive model components verification. The requests are created by intelligent agents with feedback from decision makers. Some practical results confirm the adequacy of the proposed approach.}
}
@article{HUI2019S90,
title = {PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS},
journal = {Value in Health},
volume = {22},
pages = {S90},
year = {2019},
note = {ISPOR 2019: Rapid. Disruptive. Innovative: A New Era in HEOR},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2019.04.303},
url = {https://www.sciencedirect.com/science/article/pii/S1098301519304954},
author = {Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li}
}
@article{HUANG2018165,
title = {A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {165-176},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311927},
author = {Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu},
keywords = {Technology delivery system, Tech mining, Emerging technology, Big Data, Technology assessment, Impact assessment},
abstract = {While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (“BDA”).}
}
@article{CHEN2016184,
title = {On Big Data and Hydroinformatics},
journal = {Procedia Engineering},
volume = {154},
pages = {184-191},
year = {2016},
note = {12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.443},
url = {https://www.sciencedirect.com/science/article/pii/S187770581631832X},
author = {Yiheng Chen and Dawei Han},
keywords = {Big data, Hydroinformatics},
abstract = {Big data is an increasingly hot concept in the past five years in the area of computer science, e-commence, and bioinformatics, because more and more data has been collected by the internet, remote sensing network, wearable devices and the Internet of Things. The big data technology provides techniques and analytical tools to handle large datasets, so that creative ideas and new values can be extracted from them. However, the hydroinformatics research community are not so familiar with big data. This paper provides readers who are embracing the data-rich era with a timely review on big data and its relevant technology, and then points out the relevance with hydroinformatics in three aspects.}
}
@article{ZAMAN2017537,
title = {Challenges and Opportunities of Big Data Analytics for Upcoming Regulations and Future Transformation of the Shipping Industry},
journal = {Procedia Engineering},
volume = {194},
pages = {537-544},
year = {2017},
note = {10th International Conference on Marine Technology, MARTEC 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.182},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817333386},
author = {Ibna Zaman and Kayvan Pazouki and Rose Norman and Shervin Younessi and Shirley Coleman},
keywords = {Carbon emission, data-oriented, MRV, big data},
abstract = {Shipping is a heavily regulated industry and responsible for around 3% of global carbon emissions. Global trade is highly dependent on shipping which covers around 90% of commercial demand. Now the industry is expected to navigate through many twists and turns of different situations like upcoming regulations, climate change, energy shortages and technological revolutions. Technological development is apparent across all marine sectors due to the rapid development of sensor technology, IT, automation and robotics. The industry must continue to develop at a rapid pace over the next decade in order to be able to adapt to upcoming regulations and market pressure. Ship intelligence will be the driving force shaping the future of the industry. Ships generate a large volume of data from different sources and in different formats. So big data has become the talk of the industry nowadays. Big data analysis discovers correlations between different measurable or unmeasurable parameters to determine hidden patterns and trends. This analysis will have a significant impact on vessel performance monitoring and provide performance prediction, real-time transparency, and decision-making support to the ship operator. Big data will also bring new opportunities and challenges for the maritime industry. It will increase the capability of performance monitoring, remove human error and increase interdependencies of components. However, the industry will have to face many challenges such as data processing, reliability, and data security. Many regulations rely on ship data including the new EU MRV (Monitoring, Reporting and Verification) regulation to quantify the CO2 emissions for ships above 5000 gross tonnage. As a result, ship operators will have to monitor and report the verified amount of CO2 emitted by their vessels on voyages to, from and between EU ports and will also be required to provide information on energy efficiency parameters. The MRV is a data-oriented regulation requiring ship operators to capture and monitor the ship emissions and other related data and although it is a regional regulation at the moment there is scope for the International Maritime Organisation (IMO) to implement it globally in the near future.}
}
@article{REHMAN2016917,
title = {Big data reduction framework for value creation in sustainable enterprises},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part A},
pages = {917-928},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216303097},
author = {Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah},
keywords = {Sustainable enterprises, Value creation, Big data analytics, Data reduction, Business model},
abstract = {Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.}
}
@article{HONG20191387,
title = {Energy forecasting in the big data world},
journal = {International Journal of Forecasting},
volume = {35},
number = {4},
pages = {1387-1388},
year = {2019},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301062},
author = {Tao Hong and Pierre Pinson}
}
@article{LIU201797,
title = {Practical-oriented protocols for privacy-preserving outsourced big data analysis: Challenges and future research directions},
journal = {Computers & Security},
volume = {69},
pages = {97-113},
year = {2017},
note = {Security Data Science and Cyber Threat Management},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2016.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404816301778},
author = {Zhe Liu and Kim-Kwang Raymond Choo and Minghao Zhao},
keywords = {Big data analysis, Privacy-preserving, Outsourced big data, Oblivious RAM, Security, Practical-oriented, Secure query},
abstract = {With the significant increase in the volume, variety, velocity and veracity of data generated, collected and transmitted through computing and networking systems, it is of little surprise that big data analysis and processing is the subject of focus from enterprise, academia and government. Outsourcing is one popular solution considered in big data processing, although security and privacy are two key concerns often attributed to the underutilization of outsourcing and other promising big data analysis and processing technologies. In this paper, we survey the state-of-the-art literature on cryptographic solutions designed to ensure the security and/or privacy in big data outsourcing. For example, we provide concrete examples to explain how these cryptographic solutions can be deployed. We summarize the existing state-of-play before discussing research opportunities.}
}
@article{GU201722,
title = {Visualizing the knowledge structure and evolution of big data research in healthcare informatics},
journal = {International Journal of Medical Informatics},
volume = {98},
pages = {22-32},
year = {2017},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1386505616302556},
author = {Dongxiao Gu and Jingjing Li and Xingguo Li and Changyong Liang},
keywords = {Big data, Healthcare informatics, Bibliometrics, Knowledge structure, Knowledge management},
abstract = {Background
In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.
Methods
To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers’ production trends in the field and the trend of each paper’s co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.
Results
By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People’s Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).
Conclusion
This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.}
}
@article{OLMEDILLA201679,
title = {Harvesting Big Data in social science: A methodological approach for collecting online user-generated content},
journal = {Computer Standards & Interfaces},
volume = {46},
pages = {79-87},
year = {2016},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916300034},
author = {M. Olmedilla and M.R. Martínez-Torres and S.L. Toral},
keywords = {Big Data, User-generated content, e-Social science, Computing, Data gathering},
abstract = {Online user-generated content is playing a progressively important role as information source for social scientists seeking for digging out value. Advances procedures and technologies to enable the capture, storage, management, and analysis of the data make possible to exploit increasing amounts of data generated directly by users. In that regard, Big Data is gaining meaning into social science from quantitative datasets side, which differs from traditional social science where collecting data has always been hard, time consuming, and resource intensive. Hence, the emergent field of computational social science is broadening researchers' perspectives. However, it also requires a multidisciplinary approach involving several and different knowledge areas. This paper outlines an architectural framework and methodology to collect Big Data from an electronic Word-of-Mouth (eWOM) website containing user-generated content. Although the paper is written from the social science perspective, it must be also considered together with other complementary disciplines such as data accessing and computing.}
}
@article{HERRMANN2022194,
title = {An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks},
journal = {Procedia Computer Science},
volume = {200},
pages = {194-204},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002277},
author = {Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and Jörg Hartlief and Jens Rautenstengel and Christine Loeser and Jörg Böhme},
keywords = {Data Quality Assessment, Advanced Planning, Scheduling, Bayesian Network, Enterprise Resource Planning},
abstract = {In today’s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN’s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN’s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework’s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.}
}
@article{DESILVA2021104305,
title = {Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data},
journal = {Computers in Biology and Medicine},
volume = {132},
pages = {104305},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104305},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521000998},
author = {Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel Jönsson and Ryan T. Demmer and Joanne Enticott},
keywords = {Critical care, Diabetes, Electronic health records, LASSO, Machine learning, Mortality, Natural language processing, Prognosis, Text mining},
abstract = {Background
Clinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.
Objective
To determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.
Materials and methods
Mortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n = 45253), nurses (159027), and both (n = 204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.
Results
Models built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians’ and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.
Conclusion
Clinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.}
}
@incollection{APPEL2021193,
title = {Chapter 6 - Psychological targeting in the age of Big Data},
editor = {Dustin Wood and Stephen J. Read and P.D. Harms and Andrew Slaughter},
booktitle = {Measuring and Modeling Persons and Situations},
publisher = {Academic Press},
pages = {193-222},
year = {2021},
isbn = {978-0-12-819200-9},
doi = {https://doi.org/10.1016/B978-0-12-819200-9.00015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128192009000156},
author = {Ruth E. Appel and Sandra C. Matz},
keywords = {Psychological targeting, Psychological profiling, Psychologically-informed interventions, Big Data, Digital footprints, Methods, Ethics, Privacy, Contextual integrity},
abstract = {Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals’ psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.}
}
@article{RICHTER201929,
title = {Efficient learning from big data for cancer risk modeling: A case study with melanoma},
journal = {Computers in Biology and Medicine},
volume = {110},
pages = {29-39},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519301477},
author = {Aaron N. Richter and Taghi M. Khoshgoftaar},
keywords = {Big data, Cloud computing, Machine learning, Electronic health records, Early detection of cancer},
abstract = {Background
Building cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.
Methods
We used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.
Results
Among 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC = 0.799, Sensitivity = 0.753, Specificity = 0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.
Conclusions
We produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.}
}
@article{OPRESNIK2015174,
title = {The value of Big Data in servitization},
journal = {International Journal of Production Economics},
volume = {165},
pages = {174-184},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004307},
author = {David Opresnik and Marco Taisch},
keywords = {Servitization, Big Data, Manufacturing, Competitive advantage, Value, Information},
abstract = {Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product–services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five “Vs” in Big Data–Value, in addition to the other four “Vs”—Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value— “information”, beside the two existing ones: product and service. The results have strategic implications for managers.}
}
@article{SIMPAO2015350,
title = {Big data and visual analytics in anaesthesia and health care†},
journal = {British Journal of Anaesthesia},
volume = {115},
number = {3},
pages = {350-356},
year = {2015},
issn = {0007-0912},
doi = {https://doi.org/10.1093/bja/aeu552},
url = {https://www.sciencedirect.com/science/article/pii/S0007091217311479},
author = {A.F. Simpao and L.M. Ahumada and M.A. Rehman},
keywords = {decision support systems, clinical, electronic health records, integrated advanced information management systems, medical informatics},
abstract = {Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics—the systematic use of data combined with quantitative and qualitative analysis to make decisions—can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.}
}
@article{KACFAHEMANI201570,
title = {Understandable Big Data: A survey},
journal = {Computer Science Review},
volume = {17},
pages = {70-81},
year = {2015},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2015.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574013715000064},
author = {Cheikh {Kacfah Emani} and Nadine Cullot and Christophe Nicolle},
keywords = {Big data, Hadoop, Reasoning, Coreference resolution, Entity linking, Information extraction, Ontology alignment},
abstract = {This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.}
}
@article{BILAL2016500,
title = {Big Data in the construction industry: A review of present status, opportunities, and future trends},
journal = {Advanced Engineering Informatics},
volume = {30},
number = {3},
pages = {500-521},
year = {2016},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616301938},
author = {Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha},
keywords = {Big Data Engineering, Big Data Analytics, Construction industry, Machine learning},
abstract = {The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon—dubbed as Big Data—has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.}
}
@article{LABRIE201845,
title = {Big data analytics sentiment: US-China reaction to data collection by business and government},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {45-55},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517308612},
author = {Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier},
keywords = {Big data ethics, Business data usage, Corporate data collection, Government data usage, Technology ethics, US-China similarities, US-China differences},
abstract = {As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.}
}
@article{ENGLEBRIGHT2016280,
title = {The Role of the Chief Nurse Executive in the Big Data Revolution},
journal = {Nurse Leader},
volume = {14},
number = {4},
pages = {280-284},
year = {2016},
issn = {1541-4612},
doi = {https://doi.org/10.1016/j.mnl.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1541461215300094},
author = {Jane Englebright and Barbara Caspers}
}
@article{BELLOORGAZ201645,
title = {Social big data: Recent achievements and new challenges},
journal = {Information Fusion},
volume = {28},
pages = {45-59},
year = {2016},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253515000780},
author = {Gema Bello-Orgaz and Jason J. Jung and David Camacho},
keywords = {Big data, Data mining, Social media, Social networks, Social-based frameworks and applications},
abstract = {Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the “umbrella” of the social networks, social media and big data paradigms.}
}
@article{NORORI2021100347,
title = {Addressing bias in big data and AI for health care: A call for open science},
journal = {Patterns},
volume = {2},
number = {10},
pages = {100347},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100347},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921002026},
author = {Natalia Norori and Qiyang Hu and Florence Marcelle Aellen and Francesca Dalia Faraci and Athina Tzovara},
keywords = {artificial intelligence, deep learning, health care, bias, open science, participatory science, data standards},
abstract = {Summary
Artificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.}
}
@article{ZHU2019229,
title = {The application of big data and the development of nursing science: A discussion paper},
journal = {International Journal of Nursing Sciences},
volume = {6},
number = {2},
pages = {229-234},
year = {2019},
issn = {2352-0132},
doi = {https://doi.org/10.1016/j.ijnss.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352013218305507},
author = {Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan},
keywords = {Artificial intelligence, Data mining, Knowledge bases, Nursing},
abstract = {Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.}
}
@article{KORTESNIEMI201890,
title = {The European Federation of Organisations for Medical Physics (EFOMP) White Paper: Big data and deep learning in medical imaging and in relation to medical physics profession},
journal = {Physica Medica},
volume = {56},
pages = {90-93},
year = {2018},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718313152},
author = {Mika Kortesniemi and Virginia Tsapaki and Annalisa Trianni and Paolo Russo and Ad Maas and Hans-Erik Källman and Marco Brambilla and John Damilakis},
abstract = {Big data and deep learning will profoundly change various areas of professions and research in the future. This will also happen in medicine and medical imaging in particular. As medical physicists, we should pursue beyond the concept of technical quality to extend our methodology and competence towards measuring and optimising the diagnostic value in terms of how it is connected to care outcome. Functional implementation of such methodology requires data processing utilities starting from data collection and management and culminating in the data analysis methods. Data quality control and validation are prerequisites for the deep learning application in order to provide reliable further analysis, classification, interpretation, probabilistic and predictive modelling from the vast heterogeneous big data. Challenges in practical data analytics relate to both horizontal and longitudinal analysis aspects. Quantitative aspects of data validation, quality control, physically meaningful measures, parameter connections and system modelling for the future artificial intelligence (AI) methods are positioned firmly in the field of Medical Physics profession. It is our interest to ensure that our professional education, continuous training and competence will follow this significant global development.}
}
@article{TAN2015223,
title = {Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph},
journal = {International Journal of Production Economics},
volume = {165},
pages = {223-233},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004289},
author = {Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang},
keywords = {Big data, Analytic infrastructure, Competence set, Deduction graph, Supply chain innovation},
abstract = {Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm׳s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.}
}
@article{JAN2019275,
title = {Deep learning in big data Analytics: A comparative study},
journal = {Computers & Electrical Engineering},
volume = {75},
pages = {275-287},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315835},
author = {Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon},
keywords = {Big data, Deep learning, Deep belief networks, Convolutional Neural Networks},
abstract = {Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.}
}
@incollection{SEBASTIANCOLEMAN2022229,
title = {Chapter 10 - Dimensions of Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {229-256},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000109},
author = {Laura Sebastian-Coleman},
keywords = {Data quality dimensions, data completeness, data integrity, validity, data currency, metadata management, reference data management, data modeling},
abstract = {This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: “What do we mean by high-quality data?” “How do we detect low-quality data?” and “What action will we take when data does not meet quality standards?” This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.}
}
@incollection{VIDHYALAKSHMI20201,
title = {Chapter 1 - Medical big data mining and processing in e-health care},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar},
booktitle = {An Industrial IoT Approach for Pharmaceutical Industry Growth},
publisher = {Academic Press},
pages = {1-30},
year = {2020},
isbn = {978-0-12-821326-1},
doi = {https://doi.org/10.1016/B978-0-12-821326-1.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213261000012},
author = {A. Vidhyalakshmi and C. Priya},
keywords = {Big data, IoT, health care, telemedicine, WHO, image processing},
abstract = {Health care is thought to be one of the business fields with the largest big data potential. Based on the prevailing definition, big data has a large amount of data which can be processed easily and can be modified or updated easily. These data can be quickly stored, processed, and transformed into valuable information using older technologies. At present, many new trends regarding new data resources and innovative data analysis are followed in medicine and health care. In practice, electronic health-care records, free open-source data, and the “quantified self” provide new approaches for analyzing data. Some of these advancements have been made in information extraction from the text data based on analytics, which is useful in data unlocking for analytics purposes from clinical documentation. Choosing big data approaches in the medicine and health-care fields has been lagging. This has led to the rise specific problems regarding data complexity and organizational, legal, and ethical challenges. With the growth of the uptake of big data in general, and medicine and health care in specific, innovative ideas and solutions are expected. Telemedicine is a new opportunity for the Internet of Things (IoT). This enables the specialist to consult a patient despite them being in different places. Medical image segmentation is needed for the analysis, storage, and protection of medical images in telemedicine. Telemedicine is defined by the World Health Organization (WHO) as “the practice of medical care using interactive audiovisual and data communications. This includes the delivery of medical care services, diagnosis, consultation, treatment, as well as health education and the transfer of medical data.” IoT-based applications mainly include remote patient monitoring and clinical monitoring. In addition, preventive measures-based applications are also part of smart health care. These applications require image processing-based technologies which could be integrated into medical health-care systems. Various types of input taken from cameras and processing of CT and MRI images could be integrated into IoT-based medical applications.}
}
@article{MANTELERO2017584,
title = {Regulating big data. The guidelines of the Council of Europe in the context of the European data protection framework},
journal = {Computer Law & Security Review},
volume = {33},
number = {5},
pages = {584-602},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917301644},
author = {Alessandro Mantelero},
keywords = {Big data, Data protection, Council of Europe, Risk assessment, Data protection by design, Consent, Data anonymization, Open data, Algorithms},
abstract = {In January 2017 the Consultative Committee of Convention 108 adopted its Guidelines on the Protection of Individuals with Regard to the Processing of Personal Data in a World of Big Data. These are the first guidelines on data protection provided by an international body which specifically address the issues surrounding big data applications. This article examines the main provisions of these Guidelines and highlights the approach adopted by the Consultative Committee, which contextualises the traditional principles of data protection in the big data scenario and also takes into account the challenges of the big data paradigm. The analysis of the different provisions adopted focuses primarily on the core of the Guidelines namely the risk assessment procedure. Moreover, the article discusses the novel solutions provided by the Guidelines with regard to the data subject's informed consent, the by-design approach, anonymization, and the role of the human factor in big data-supported decisions. This critical analysis of the Guidelines introduces a broader reflection on the divergent approaches of the Council of Europe and the European Union to regulating data processing. Where the principle-based model of the Council of Europe differs from the approach adopted by the EU legislator in the detailed Regulation (EU) 2016/679. In the light of this, the provisions of the Guidelines and their attempt to address the major challenges of the new big data paradigm set the stage for concluding remarks about the most suitable regulatory model to deal with the different issues posed by the development of technology.}
}
@article{GOVINDAN2018343,
title = {Big data analytics and application for logistics and supply chain management},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {343-349},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1366554518302606},
author = {Kannan Govindan and T.C.E. Cheng and Nishikant Mishra and Nagesh Shukla},
keywords = {Big data analytics, Supply chain management, Logistics},
abstract = {This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.}
}
@article{REKHA2015295,
title = {Survey on Software Project Risks and Big Data Analytics},
journal = {Procedia Computer Science},
volume = {50},
pages = {295-300},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915005463},
author = {J.H. Rekha and R. Parvathi},
keywords = {software project, big data analytics, anlytics tools.},
abstract = {Software project is collaborative enterprise of making a desired software for the client. Each software is unique and is delivered by following the process. The process includes understanding the requirement, planning, designing the software and implementation. Risk occurs in the software project which need attention by the managers and workers to make the project efficient. Big data analytics is commonly used in all fields. Big data deals with huge data which are unstructured. Using analytics tools, it can be chunked down and analyzed to provide valuable solutions. In this paper, a review of risk in software project and big data analytics are briefed out.}
}
@article{AKTER2016113,
title = {How to improve firm performance using big data analytics capability and business strategy alignment?},
journal = {International Journal of Production Economics},
volume = {182},
pages = {113-131},
year = {2016},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2016.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527316302110},
author = {Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe},
keywords = {Capabilities, Entanglement view, Big data analytics, Hierarchical modeling},
abstract = {The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship.}
}
@article{ZHOU2016215,
title = {Big data driven smart energy management: From big data to big insights},
journal = {Renewable and Sustainable Energy Reviews},
volume = {56},
pages = {215-225},
year = {2016},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2015.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S1364032115013179},
author = {Kaile Zhou and Chao Fu and Shanlin Yang},
keywords = {Energy big data, Smart energy management, Big data analytics, Smart grid, Demand side management (DSM)},
abstract = {Large amounts of data are increasingly accumulated in the energy sector with the continuous application of sensors, wireless transmission, network communication, and cloud computing technologies. To fulfill the potential of energy big data and obtain insights to achieve smart energy management, we present a comprehensive study of big data driven smart energy management. We first discuss the sources and characteristics of energy big data. Also, a process model of big data driven smart energy management is proposed. Then taking smart grid as the research background, we provide a systematic review of big data analytics for smart energy management. It is discussed from four major aspects, namely power generation side management, microgrid and renewable energy management, asset management and collaborative operation, as well as demand side management (DSM). Afterwards, the industrial development of big data-driven smart energy management is analyzed and discussed. Finally, we point out the challenges of big data-driven smart energy management in IT infrastructure, data collection and governance, data integration and sharing, processing and analysis, security and privacy, and professionals.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva (English Edition)},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {2173-5727},
doi = {https://doi.org/10.1016/j.medine.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S217357271930013X},
author = {A. {Núñez Reiz}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{ZAIN2018140,
title = {Big Data Analytics based on PANFIS MapReduce},
journal = {Procedia Computer Science},
volume = {144},
pages = {140-152},
year = {2018},
note = {INNS Conference on Big Data and Deep Learning},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.514},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322233},
author = {Choiru Za’in and Mahardhika Pratama and Edwin Lughofer and Meftahul Ferdaus and Qing Cai and Mukesh Prasad},
keywords = {Big data stream analytic, Distributed evolving algorithm, Scalable real-time data mining, Parallel learning, Rule merging strategy},
abstract = {In this paper, a big data analytic framework is introduced for processing high-frequency data stream. This framework architecture is developed by combining an advanced evolving learning algorithm namely Parsimonious Network Fuzzy Inference System (PANFIS) with MapReduce parallel computation, where PANFIS has the capability of processing data stream in large volume. Big datasets are learnt chunk by chunk by processors in MapReduce environment and the results are fused by rule merging method, that reduces the complexity of the rules. The performance measurement has been conducted, and the results are showing that the MapReduce framework along with PANFIS evolving system helps to reduce the processing time around 22 percent in average in comparison with the PANFIS algorithm without reducing performance in accuracy.}
}
@article{VALENCIAPARRA2021113450,
title = {DMN4DQ: When data quality meets DMN},
journal = {Decision Support Systems},
volume = {141},
pages = {113450},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113450},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620302050},
author = {Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López},
keywords = {Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement},
abstract = {To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.}
}
@article{MILNE2019235,
title = {Big data and understanding change in the context of planning transport systems},
journal = {Journal of Transport Geography},
volume = {76},
pages = {235-244},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300984},
author = {Dave Milne and David Watling},
abstract = {This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.}
}
@article{RAMBUR2018176,
title = {A plea to nurse educators: Incorporate big data use as a foundational skill for undergraduate and graduate nurses},
journal = {Journal of Professional Nursing},
volume = {34},
number = {3},
pages = {176-181},
year = {2018},
issn = {8755-7223},
doi = {https://doi.org/10.1016/j.profnurs.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S8755722316303283},
author = {Betty Rambur and Therese Fitzpatrick}
}
@article{KHENNOU201860,
title = {Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR},
journal = {Procedia Computer Science},
volume = {127},
pages = {60-68},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301091},
author = {Fadoua Khennou and Youness Idrissi Khamlichi and Nour El Houda Chaoui},
keywords = {Electronic Health Records, EHRs, Analytic tools, Big Data, Health Practitioners},
abstract = {Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.}
}
@article{ZHANG2015606,
title = {A System for Tender Price Evaluation of Construction Project Based on Big Data},
journal = {Procedia Engineering},
volume = {123},
pages = {606-614},
year = {2015},
note = {Selected papers from Creative Construction Conference 2015},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.10.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815032154},
author = {Yongcheng Zhang and Hanbin Luo and Yi He},
keywords = {System, Bid price evaluation, Construction project, Big data},
abstract = {Tender price evaluation of construction project is one of the most important works for the clients to control project cost in the bidding stage. However,the previously underutilization of project cost data made the tender price evaluation of new projects lack of effective evaluation criterion, which brings challenge to cost control. With the improvement of companies’ information technology application and the advent of big data era, the project cost-related data can be completely and systematically recorded in real time, as well as fully utilized to support decision-making for construction project cost management. In this paper, a system for tender price evaluation of construction project based on big data is presented, aiming to use related technique of big data to analysis project cost data to give a reasonable cost range, which contributes to obtaining the evaluation criterion to support the tender price controls. The paper introduced the data sources, data extraction, data storage and data analysis of the system respectively. A case study is conducted in a metro station project to evaluate the system. The results show that the system based on big data is significant for tender price evaluation in construction project.}
}
@article{JI2017187,
title = {Big data analytics based fault prediction for shop floor scheduling},
journal = {Journal of Manufacturing Systems},
volume = {43},
pages = {187-194},
year = {2017},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2017.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612517300389},
author = {Wei Ji and Lihui Wang},
keywords = {Big data analytics, Fault prediction, Shop floor, Scheduling},
abstract = {The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.}
}
@article{LEFEVRE20181,
title = {Big data in forensic science and medicine},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {1-6},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X17301154},
author = {Thomas Lefèvre},
keywords = {Forensic science, Big data, Personalized medicine, Predictive medicine, Machine learning, Dimensionality},
abstract = {In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.}
}
@article{NATIVI20151,
title = {Big Data challenges in building the Global Earth Observation System of Systems},
journal = {Environmental Modelling & Software},
volume = {68},
pages = {1-26},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215000481},
author = {Stefano Nativi and Paolo Mazzetti and Mattia Santoro and Fabrizio Papeschi and Max Craglia and Osamu Ochiai},
keywords = {GEOSS, Big Data, Multidisciplinary systems, Earth System Science, Research infrastructures, Interoperability, Cloud systems},
abstract = {There are many expectations and concerns about Big Data in the sector of Earth Observation. It is necessary to understand whether Big Data is a radical shift or an incremental change for the existing digital infrastructures. This manuscript explores the impact of Big Data dimensionalities (commonly known as ‘V’ axes: volume, variety, velocity, veracity, visualization) on the Global Earth Observation System of Systems (GEOSS) and particularly its common digital infrastructure (i.e. the GEOSS Common Infrastructure). GEOSS is a global and flexible network of content providers allowing decision makers to access an extraordinary range of data and information. GEOSS is a pioneering framework for global and multidisciplinary data sharing in the EO realm. The manuscript introduces and discusses the general GEOSS strategies to address Big Data challenges, focusing on the cloud-based discovery and access solutions. A final section reports the results of the scalability and flexibility performance tests.}
}
@article{PURANIK2019838,
title = {The perils and pitfalls of big data analysis in medicine},
journal = {The Ocular Surface},
volume = {17},
number = {4},
pages = {838-839},
year = {2019},
issn = {1542-0124},
doi = {https://doi.org/10.1016/j.jtos.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1542012419301740},
author = {C.J. Puranik and Sreenivasa Rao and S. Chennamaneni}
}
@article{MAUDSLEY2018961,
title = {Intelligent and effective informatic deconvolution of “Big Data” and its future impact on the quantitative nature of neurodegenerative disease therapy},
journal = {Alzheimer's & Dementia},
volume = {14},
number = {7},
pages = {961-975},
year = {2018},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2018.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1552526018300402},
author = {Stuart Maudsley and Viswanath Devanarayan and Bronwen Martin and Hugo Geerts},
keywords = {Big data, Informatics, High-dimensionality, Alzheimer's disease, Aging, Molecular signature, Transcriptomics, Metabolomics, Proteomics, Genomics},
abstract = {Biomedical data sets are becoming increasingly larger and a plethora of high-dimensionality data sets (“Big Data”) are now freely accessible for neurodegenerative diseases, such as Alzheimer's disease. It is thus important that new informatic analysis platforms are developed that allow the organization and interrogation of Big Data resources into a rational and actionable mechanism for advanced therapeutic development. This will entail the generation of systems and tools that allow the cross-platform correlation between data sets of distinct types, for example, transcriptomic, proteomic, and metabolomic. Here, we provide a comprehensive overview of the latest strategies, including latent semantic analytics, topological data investigation, and deep learning techniques that will drive the future development of diagnostic and therapeutic applications for Alzheimer's disease. We contend that diverse informatic “Big Data” platforms should be synergistically designed with more advanced chemical/drug and cellular/tissue-based phenotypic analytical predictive models to assist in either de novo drug design or effective drug repurposing.}
}
@article{GAO2016952,
title = {A review of control loop monitoring and diagnosis: Prospects of controller maintenance in big data era},
journal = {Chinese Journal of Chemical Engineering},
volume = {24},
number = {8},
pages = {952-962},
year = {2016},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2016.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S1004954116305134},
author = {Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang},
keywords = {Control loop performance assessment, Industrial alarm system, Process knowledge, Root cause diagnosis, Big data},
abstract = {Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.}
}
@article{GARG2016940,
title = {Challenges and Techniques for Testing of Big Data},
journal = {Procedia Computer Science},
volume = {85},
pages = {940-948},
year = {2016},
note = {International Conference on Computational Modelling and Security (CMS 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.285},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916306354},
author = {Naveen Garg and Sanjay Singla and Surender Jangra},
keywords = {Big Data, Testing, Verasity, Hadoop},
abstract = {Big Data, the new buzz word in the industry, is data that exceeds the processing and analytic capacity of conventional database systems within the time necessary to make them useful. With multiple data stores in abundant formats, billions of rows of data with hundreds of millions of data combinations and the urgent need of making best possible decisions, the challenge is big and the solution bigger, Big Data. Comes with it, new advances in computing technology together with its high performance analytics for simpler and faster processing of only relevant data to enable timely and accurate insights using data mining and predictive analytics, text mining, forecasting and optimization on complex data to continuously drive innovation and make the best possible decisions. While Big Data provides solutions to complex business problems like analyzing larger volumes of data than was previously possible to drive more precise answers, analyzing data in motion to capture opportunities that were previously lost, it poses bigger challenges in testing these scenarios. Testing such highly volatile data, which is more often than not unstructured generated from myriad sources such as web logs, radio frequency Id (RFID), sensors embedded in devices, GPS systems etc. and mostly clustered data for its accuracy, high availability, security requires specialization. One of the most challenging things for a tester is to keep pace with changing dynamics of the industry. While on most aspects of testing, the tester need not know the technical details behind the scene however this is where testing Big Data Technology is so different. A tester not only needs to be strong on testing fundamentals but also has to be equally aware of minute details in the architecture of the database designs to analyze several performance bottlenecks and other issues. Like in the example quoted above on In-Memory databases, a tester would need to know how the operating systems allocate and de-allocate memory and understand how much memory is being used at any given time. So, concluding, as the data- analytics Industry evolves further we would see the IT Testing Services getting closely aligned with the Database Engineering and the industry would need more skilled testing professional in this domain to grab the new opportunities.}
}
@article{TAGLANG201617,
title = {Use of “big data” in drug discovery and clinical trials},
journal = {Gynecologic Oncology},
volume = {141},
number = {1},
pages = {17-23},
year = {2016},
issn = {0090-8258},
doi = {https://doi.org/10.1016/j.ygyno.2016.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0090825816300464},
author = {Guillaume Taglang and David B. Jackson},
keywords = {Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers},
abstract = {Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.}
}
@article{GUALO2021110938,
title = {Data quality certification using ISO/IEC 25012: Industrial experiences},
journal = {Journal of Systems and Software},
volume = {176},
pages = {110938},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110938},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000352},
author = {Fernando Gualo and Moisés Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini},
keywords = {Data quality evaluation process, Data quality certification, Data quality management, ISO/IEC 25012, ISO/IEC 25024, ISO/IEC 25040},
abstract = {The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.}
}
@incollection{LOPES2017167,
title = {Chapter 10 - Big Data: A Practitioners Perspective},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {167-179},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000107},
author = {Darshan Lopes and Kevin Palmer and Fiona O'Sullivan},
keywords = {Pitfalls, Considerations, Implementation, Migration pattern, Practitioner's perspective, Open source, Data warehouse},
abstract = {Big data solutions represent a significant challenge for some organizations. There are a huge variety of software products, deployment patterns and solution options that need to be considered to ensure a successful outcome for an organization trying to implement a big data solution. With that in mind, the chapter “Big Data: a practitioner's perspective” will focus on four key areas associated with big data that require consideration from a practical and implementation perspective: (i) Big Data is a new Paradigm – Differences with Traditional Data Warehouse, Pitfalls and Considerations; (ii) Product considerations for Big Data – Use of Open Source products for Big Data, Pitfalls and Considerations; (iii) Use of Cloud for hosting Big Data – Why use Cloud, Pitfalls and Considerations; and (iv) Big Data Implementation – Architecture definition, processing framework and migration patterns from Data Warehouse to Big Data.}
}
@incollection{LEI202029,
title = {2 - Fundamentals of big data in radio astronomy},
editor = {Linghe Kong and Tian Huang and Yongxin Zhu and Shenghua Yu},
booktitle = {Big Data in Astronomy},
publisher = {Elsevier},
pages = {29-58},
year = {2020},
isbn = {978-0-12-819084-5},
doi = {https://doi.org/10.1016/B978-0-12-819084-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128190845000109},
author = {Jiale Lei and Linghe Kong},
keywords = {Big data, Astronomy, Statistical challenges, Astronomical data analysis, Platforms for big data process},
abstract = {Large digital sky surveys are becoming the dominant source of data in astronomy. There are more than 100 terabytes of data in major archives, and that amount is growing rapidly. A typical sky survey archive has approximately 10 terabytes of image data and a billion detected sources (stars, galaxies, quasars, etc.), with hundreds of measured attributes per source. These surveys span the full range of wavelengths, radio through gamma ray, yet they are just a taste of the much larger datasets to come. Yearly advances in electronics bring new instruments that double the amount of data collected each year and lead to the exponential growth of information in astronomy. Thus, datasets that are orders of magnitude larger, more complex, and more homogeneous than in the past are on the horizon. In comparison, the size of the human genome is about 1 gigabyte and that of the Library of Congress is about 20 terabytes. Truly, astronomy has come to the big data era.}
}
@article{BROTHERS201884,
title = {Integrity, standards, and QC-related issues with big data in pre-clinical drug discovery},
journal = {Biochemical Pharmacology},
volume = {152},
pages = {84-93},
year = {2018},
issn = {0006-2952},
doi = {https://doi.org/10.1016/j.bcp.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0006295218301199},
author = {John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko},
keywords = {Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome},
abstract = {The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.}
}
@article{NIMMAGADDA20191155,
title = {On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information Systems},
journal = {Procedia Computer Science},
volume = {159},
pages = {1155-1164},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.284},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314814},
author = {Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood},
keywords = {Supply Chain Management, Project Management, Laws of Geography, Domain Ontologies, Data Mining},
abstract = {We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.}
}
@article{PIRRACCHIO2019377,
title = {Big data and targeted machine learning in action to assist medical decision in the ICU},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {4},
pages = {377-384},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818302169},
author = {Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard},
abstract = {Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.}
}
@article{ALLES201644,
title = {Incorporating big data in audits: Identifying inhibitors and a research agenda to address those inhibitors},
journal = {International Journal of Accounting Information Systems},
volume = {22},
pages = {44-59},
year = {2016},
note = {2015 Research Symposium on Information Integrity & Information Systems Assurance},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300811},
author = {Michael Alles and Glen L. Gray},
keywords = {Big Data, Auditing, Accounting information systems},
abstract = {With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.}
}
@article{SARALADEVI2015596,
title = {Big Data and Hadoop-a Study in Security Perspective},
journal = {Procedia Computer Science},
volume = {50},
pages = {596-601},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.091},
url = {https://www.sciencedirect.com/science/article/pii/S187705091500592X},
author = {B. Saraladevi and N. Pazhaniraja and P. Victer Paul and M.S. Saleem Basha and P. Dhavachelvan},
keywords = {Big data ;Hadoop ;HDFS ;Security},
abstract = {Big data is the collection and analysis of large set of data which holds many intelligence and raw information based on user data, Sensor data, Medical and Enterprise data. The Hadoop platform is used to Store, Manage, and Distribute Big data across several server nodes. This paper shows the Big data issues and focused more on security issue arises in Hadoop Architecture base layer called Hadoop Distributed File System (HDFS). The HDFS security is enhanced by using three approaches like Kerberos, Algorithm and Name node.}
}
@article{WESTRA2017549,
title = {Big data science: A literature review of nursing research exemplars},
journal = {Nursing Outlook},
volume = {65},
number = {5},
pages = {549-561},
year = {2017},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2016.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0029655416303967},
author = {Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney},
keywords = {Big data, Data science, Nursing informatics, Nursing research, Nurse scientist},
abstract = {Background
Big data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge.
Purpose
The purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals.
Methods
A literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice.
Discussion
Nurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods.
Conclusion
There are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science.}
}
@article{YU201621,
title = {Single-cell Transcriptome Study as Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {14},
number = {1},
pages = {21-30},
year = {2016},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022916000437},
author = {Pingjian Yu and Wei Lin},
keywords = {Single cell, RNA-seq, Big data, Transcriptional heterogeneity, Signal normalization},
abstract = {The rapid growth of single-cell RNA-seq studies (scRNA-seq) demands efficient data storage, processing, and analysis. Big-data technology provides a framework that facilitates the comprehensive discovery of biological signals from inter-institutional scRNA-seq datasets. The strategies to solve the stochastic and heterogeneous single-cell transcriptome signal are discussed in this article. After extensively reviewing the available big-data applications of next-generation sequencing (NGS)-based studies, we propose a workflow that accounts for the unique characteristics of scRNA-seq data and primary objectives of single-cell studies.}
}
@article{HUANG201846,
title = {Big-data-driven safety decision-making: A conceptual framework and its influencing factors},
journal = {Safety Science},
volume = {109},
pages = {46-56},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518300973},
author = {Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang},
keywords = {Big data (BD), Safety big data (SBD), Safety decision-making (SDM), Safety insight (SI), Data-driven},
abstract = {Safety data and information are the most valuable assets for organizations’ safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.}
}
@article{ASSUNCAO20153,
title = {Big Data computing and clouds: Trends and future directions},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {3-15},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514001452},
author = {Marcos D. Assunção and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya},
keywords = {Big Data, Cloud computing, Analytics, Data management},
abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.}
}
@article{GUO20181,
title = {Research on case retrieval of Bayesian network under big data},
journal = {Data & Knowledge Engineering},
volume = {118},
pages = {1-13},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18300624},
author = {Yuan Guo and Yuan Guo and K. Wu},
keywords = {Case retrieval, Big data, BN model, Hadoop platform},
abstract = {Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.}
}
@article{WOLFERT201769,
title = {Big Data in Smart Farming – A review},
journal = {Agricultural Systems},
volume = {153},
pages = {69-80},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2017.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16303754},
author = {Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt},
keywords = {Agriculture, Data, Information and communication technology, Data infrastructure, Governance, Business modelling},
abstract = {Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.}
}
@article{ZHU2018107,
title = {Review and big data perspectives on robust data mining approaches for industrial process modeling with outliers and missing data},
journal = {Annual Reviews in Control},
volume = {46},
pages = {107-133},
year = {2018},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578818301056},
author = {Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao},
keywords = {Data mining, Robustness, Process modeling, Statistical process monitoring, Big data analytics},
abstract = {Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.}
}
@article{SHADROO201819,
title = {Systematic survey of big data and data mining in internet of things},
journal = {Computer Networks},
volume = {139},
pages = {19-47},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618301579},
author = {Shabnam Shadroo and Amir Masoud Rahmani},
keywords = {Internet of things, Systematic survey, Big data, Data mining},
abstract = {In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.}
}
@article{HASHEM2016748,
title = {The role of big data in smart city},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {748-758},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216302778},
author = {Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma},
keywords = {Smart city, Big data, Internet of things, Smart environments, Cloud computing, Distributed computing},
abstract = {The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{MANTELERO2018754,
title = {AI and Big Data: A blueprint for a human rights, social and ethical impact assessment},
journal = {Computer Law & Security Review},
volume = {34},
number = {4},
pages = {754-772},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0267364918302012},
author = {Alessandro Mantelero},
keywords = {Data protection, Impact assessment, Data protection impact assessment, Human rights, Human rights impact assessment, Ethical impact assessment, Social impact assessment, General Data Protection Regulation},
abstract = {The use of algorithms in modern data processing techniques, as well as data-intensive technological trends, suggests the adoption of a broader view of the data protection impact assessment. This will force data controllers to go beyond the traditional focus on data quality and security, and consider the impact of data processing on fundamental rights and collective social and ethical values. Building on studies of the collective dimension of data protection, this article sets out to embed this new perspective in an assessment model centred on human rights (Human Rights, Ethical and Social Impact Assessment-HRESIA). This self-assessment model intends to overcome the limitations of the existing assessment models, which are either too closely focused on data processing or have an extent and granularity that make them too complicated to evaluate the consequences of a given use of data. In terms of architecture, the HRESIA has two main elements: a self-assessment questionnaire and an ad hoc expert committee. As a blueprint, this contribution focuses mainly on the nature of the proposed model, its architecture and its challenges; a more detailed description of the model and the content of the questionnaire will be discussed in a future publication drawing on the ongoing research.}
}
@article{RUAN2021100110,
title = {Health-adjusted life expectancy (HALE) in Chongqing, China, 2017: An artificial intelligence and big data method estimating the burden of disease at city level},
journal = {The Lancet Regional Health - Western Pacific},
volume = {9},
pages = {100110},
year = {2021},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2021.100110},
url = {https://www.sciencedirect.com/science/article/pii/S2666606521000195},
author = {Xiaowen Ruan and Yue Li and Xiaohui Jin and Pan Deng and Jiaying Xu and Na Li and Xian Li and Yuqi Liu and Yiyi Hu and Jingwen Xie and Yingnan Wu and Dongyan Long and Wen He and Dongsheng Yuan and Yifei Guo and Heng Li and He Huang and Shan Yang and Mei Han and Bojin Zhuang and Jiang Qian and Zhenjie Cao and Xuying Zhang and Jing Xiao and Liang Xu},
abstract = {Background
A universally applicable approach that provides standard HALE measurements for different regions has yet to be developed because of the difficulties of health information collection. In this study, we developed a natural language processing (NLP) based HALE estimation approach by using individual-level electronic medical records (EMRs), which made it possible to calculate HALE timely in different temporal or spatial granularities.
Methods
We performed diagnostic concept extraction and normalisation on 13•99 million EMRs with NLP to estimate the prevalence of 254 diseases in WHO Global Burden of Disease Study (GBD). Then, we calculated HALE in Chongqing, 2017, by using the life table technique and Sullivan's method, and analysed the contribution of diseases to the expected years “lost” due to disability (DLE).
Findings
Our method identified a life expectancy at birth (LE0) of 77•9 years and health-adjusted life expectancy at birth (HALE0) of 71•7 years for the general Chongqing population of 2017. In particular, the male LE0 and HALE0 were 76•3 years and 68•9 years, respectively, while the female LE0 and HALE0 were 80•0 years and 74•4 years, respectively. Cerebrovascular diseases, cancers, and injuries were the top three deterioration factors, which reduced HALE by 2•67, 2•15, and 1•19 years, respectively.
Interpretation
The results demonstrated the feasibility and effectiveness of EMRs-based HALE estimation. Moreover, the method allowed for a potentially transferable framework that facilitated a more convenient comparison of cross-sectional and longitudinal studies on HALE between regions. In summary, this study provided insightful solutions to the global ageing and health problems that the world is facing.
Funding
National Key R and D Program of China (2018YFC2000400).}
}
@article{BUDHIRAJA2016241,
title = {The Role of Big Data in the Management of Sleep-Disordered Breathing},
journal = {Sleep Medicine Clinics},
volume = {11},
number = {2},
pages = {241-255},
year = {2016},
note = {Novel Approaches to the Management of Sleep-Disordered Breathing},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X1630008X},
author = {Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline},
keywords = {Sleep-disordered breathing, Big data, Management, Sleep apnea}
}
@incollection{WHELAN2020365,
title = {Chapter 27 - Genetics, imaging, and cognition: big data approaches to addiction research},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {365-377},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00027-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000277},
author = {Robert Whelan and Zhipeng Cao and Laura O'Halloran and Brian Pennie},
keywords = {Addiction, Big Data, Cognition, Genetics, Machine learning, Neuroimaging, Online methods},
abstract = {The etiology and trajectory of addictions is complex, caused and moderated by individual differences in cognition that are themselves a function of genetics and of environment. In this chapter, we discuss how “Big Data” can shed light on the cognitive correlates of addiction. Big Data is primarily data-driven, using algorithms that search for patterns in data, with accurate prediction on previously unseen data as the metric of success. In this chapter, we introduce and provide practical advice on Big Data approaches for addiction. In the first part of this chapter, we describe how online methods of data collection facilitate the collection of large datasets. In the second section, we outline some recent advances in neuroimaging, with a focus on prediction of substance use using machine learning methods. In the final section, we present advances in genetics—meta- and megaanalyses—which may provide breakthroughs in our understanding of the genetics of addiction.}
}
@article{PAPADOPOULOS20171108,
title = {The role of Big Data in explaining disaster resilience in supply chains for sustainability},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1108-1118},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.059},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301275},
author = {Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba},
keywords = {Resilience, Big Data, Sustainability, Disaster, Exploratory factor analysis, Confirmatory factor analysis},
abstract = {The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public–private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.}
}
@article{SANTOSO201793,
title = {Data Warehouse with Big Data Technology for Higher Education},
journal = {Procedia Computer Science},
volume = {124},
pages = {93-99},
year = {2017},
note = {4th Information Systems International Conference 2017, ISICO 2017, 6-8 November 2017, Bali, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.12.134},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917329022},
author = {Leo Willyanto Santoso and  Yulia},
keywords = {Data Warehouse, Big Data, Academic, Hadoop, Higher Education, Analysis},
abstract = {Nowadays, data warehouse tools and technologies cannot handle the load and analytic process of data into meaningful information for top management. Big data technology should be implemented to extend the existing data warehouse solutions. Universities already collect vast amounts of data so the academic data of university has been growing significantly and become a big academic data. These datasets are rich and growing. University’s top-level management needs tools to produce information from the records. The generated information is expected to support the decision-making process of top-level management. This paper explores how big data technology could be implemented with data warehouse to support decision making process. In this framework, we propose Hadoop as big data analytic tools to be implemented for data ingestion/staging. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.}
}
@article{HE201835,
title = {Statistical process monitoring as a big data analytics tool for smart manufacturing},
journal = {Journal of Process Control},
volume = {67},
pages = {35-43},
year = {2018},
note = {Big Data: Data Science for Process Control and Operations},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2017.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959152417301257},
author = {Q. Peter He and Jin Wang},
keywords = {Statistical process monitoring, Big data, Smart manufacturing, Feature extraction, Internet of things},
abstract = {With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called ‘big data’. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.}
}
@article{VERBRUGGHE2019298,
title = {The electronic medical record: Big data, little information?},
journal = {Journal of Critical Care},
volume = {54},
pages = {298-299},
year = {2019},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2019.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883944119313723},
author = {Walter Verbrugghe and Kirsten Colpaert}
}
@article{LOFGREN2021R1312,
title = {Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes},
journal = {Current Biology},
volume = {31},
number = {19},
pages = {R1312-R1325},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.06.083},
url = {https://www.sciencedirect.com/science/article/pii/S096098222100912X},
author = {Lotus A. Lofgren and Jason E. Stajich},
abstract = {Summary
Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.}
}
@article{XU2018309,
title = {A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {309-314},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320007},
author = {Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}},
keywords = {Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing},
abstract = {High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.}
}
@incollection{GONCALVESPINHO2021155,
title = {Chapter 8 - The use of Big Data in Psychiatry—The role of administrative databases},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {155-165},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822884500009X},
author = {Manuel Gonçalves-Pinho and Alberto Freitas},
keywords = {Administrative database, Mental Health, Secondary data, Psychiatry, Research design},
abstract = {Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.}
}
@article{ZERBINO2018818,
title = {Big Data-enabled Customer Relationship Management: A holistic approach},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {818-846},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300067},
author = {Pierluigi Zerbino and Davide Aloini and Riccardo Dulmin and Valeria Mininno},
keywords = {Big Data, CRM, Literature review, Critical Success Factors (CSFs), Word tree},
abstract = {This paper aims to figure out the potential impact of Big Data (BD) on Critical Success Factors (CSFs) of Customer Relationship Management (CRM). In fact, while some authors have posited a relationship between BD and CRM, literature lacks works that go into the heart of the matter. Through an extensive up-to-date in-depth literature review about CRM, twenty (20) CSFs were singled out from 104 selected papers, and organized within an ad-hoc classification framework. The consistency of the classification was checked by means of a content analysis. Evidences were discussed and linked to the BD literature, and five propositions about how BD could affect CRM CSFs were formalized. Our results suggest that BD-enabled CRM initiatives could require several changes in the pertinent CSFs. In order to get rid of the hype effect surrounding BD, we suggest to adopt an explorative approach towards them by defining a mandatory business direction through sound business cases and pilot tests. From a general standpoint, BD could be framed as an enabling factor of well-known projects, like CRM initiatives, in order to reap the benefits from the new technologies by addressing the efforts through already acknowledged management paths.}
}
@incollection{JOINER201895,
title = {Chapter 5 - Information Seeking With Big Data: Not Just the Facts},
editor = {Ida Arlene Joiner},
booktitle = {Emerging Library Technologies},
publisher = {Chandos Publishing},
pages = {95-110},
year = {2018},
series = {Chandos Information Professional Series},
isbn = {978-0-08-102253-5},
doi = {https://doi.org/10.1016/B978-0-08-102253-5.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022535000058},
author = {Ida Arlene Joiner},
keywords = {Big data, libraries, security, privacy, infrastructure, Hadoop},
abstract = {As experts at searching, retrieving, analyzing, and managing information, librarians are uniquely suited to work with big data. This chapter provides an overview of the popular big data technology. We examine what big data is, challenges and opportunities, and how it is currently being used in many industries and libraries. The chapter concludes with additional resources, some technologies for managing big data, big data terminology, and questions for further discussion.}
}