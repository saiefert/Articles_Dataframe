@article{CAUDAI20215762,
title = {AI applications in functional genomics},
journal = {Computational and Structural Biotechnology Journal},
volume = {19},
pages = {5762-5790},
year = {2021},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2021.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2001037021004311},
author = {Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo},
keywords = {Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning},
abstract = {We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by “deep learning”, along with a burst of “big data” that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.}
}
@article{SHIROSAKIMARCALDESOUZA2021160,
title = {Evaluating and ranking secondary data sources to be used in the Brazilian LCA database – “SICV Brasil”},
journal = {Sustainable Production and Consumption},
volume = {26},
pages = {160-171},
year = {2021},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2020.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S2352550920303286},
author = {Luri {Shirosaki Marçal de Souza} and Andréa Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}},
keywords = {Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors},
abstract = {The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System – the “SICV Brasil” launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.}
}
@incollection{KRISHNAN202099,
title = {5 - Pharmacy industry applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {99-111},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000053},
author = {Krish Krishnan},
keywords = {Clinical Trials, Research, Multi-Teams, Non-Intrusive, Repeatable tests},
abstract = {One of the applications of big data applications and infrastructure is in the pharmaceutical industry. The complexity of the queries that are executed in these applications and the results they generate, make us feel the statement of torture the data and it will confess to anything. The relationships between the data in the different subject areas, the clinical trials and results, the communities in social media, the research labs and their outcomes, the clinical labs and patient results, and the financial outcomes of the pharmaceutical enterprise. Wow, think of all kinds of insights, add to this the markets, the competition, and the global industry, and we have phenomenal data to work with.}
}
@article{KAMPKER2018120,
title = {Enabling Data Analytics in Large Scale Manufacturing},
journal = {Procedia Manufacturing},
volume = {24},
pages = {120-127},
year = {2018},
note = {4th International Conference on System-Integrated Intelligence: Intelligent, Flexible and Connected Systems in Products and Production},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918305341},
author = {Achim Kampker and Heiner Heimes and Ulrich Bührer and Christoph Lienemann and Stefan Krotil},
keywords = {Automotive, Manufacturing, Data Analytics, Big Data, Optimization},
abstract = {Companies of the manufacturing industry face increasing process complexity. To remain competitive, increasing the knowledge concerning innovative manufacturing processes is necessary. In other areas, data analytics methods have been successfully applied for this purpose. Currently, their application in large scale manufacturing is hampered by insufficient data availability. Therefore, this study presents a solution approach that enables adaptive data availability by establishing a data-use-case-matrix (DUCM), which allows use case prioritization to support dimensioning of control systems and IT infrastructures. In order to support technology development, further proposed is a scalable implementation of the prioritized use cases starting in early prototyping phases.}
}
@article{HE2019320,
title = {Network-wide identification of turn-level intersection congestion using only low-frequency probe vehicle data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {108},
pages = {320-339},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18312543},
author = {Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen},
keywords = {Big data, Floating car data, Urban road network, Traffic congestion, Road intersection},
abstract = {Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles’ turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers’ workloads by providing information regarding where and when to investigate and solve traffic congestion problems.}
}
@article{ZHANG2020102659,
title = {Design and application of a personal credit information sharing platform based on consortium blockchain},
journal = {Journal of Information Security and Applications},
volume = {55},
pages = {102659},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2020.102659},
url = {https://www.sciencedirect.com/science/article/pii/S2214212620308139},
author = {Jing Zhang and Rong Tan and Chunhua Su and Wen Si},
keywords = {Consortium blockchain, Personal credit reporting, Credit information sharing, Big data crediting},
abstract = {The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.}
}
@article{MOZZONI2022402,
title = {Transfer’s monitoring in bus transit services by Automatic Vehicle Location data},
journal = {Transportation Research Procedia},
volume = {60},
pages = {402-409},
year = {2022},
note = {New scenarios for safe mobility in urban areasProceedings of the XXV International Conference Living and Walking in Cities (LWC 2021), September 9-10, 2021, Brescia, Italy},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.12.052},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521009534},
author = {Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino},
keywords = {Big Data, Transfer diagnosis, Automatic Vehicle Location Data},
abstract = {Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.}
}
@incollection{MILOSEVIC201639,
title = {Chapter 2 - Real-Time Analytics},
editor = {Rajkumar Buyya and Rodrigo N. Calheiros and Amir Vahid Dastjerdi},
booktitle = {Big Data},
publisher = {Morgan Kaufmann},
pages = {39-61},
year = {2016},
isbn = {978-0-12-805394-2},
doi = {https://doi.org/10.1016/B978-0-12-805394-2.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053942000027},
author = {Z. Milosevic and W. Chen and A. Berry and F.A. Rabhi},
keywords = {Real-time analytics, Complex event processing, Streaming, Event processing, Advanced analytics, Data analysis, Machine learning, Finance, EventSwarm},
abstract = {Real-time analytics is a special kind of Big Data analytics in which data elements are required to be processed and analyzed as they arrive in real time. It is important in situations where real-time processing and analysis can deliver important insights and yield business value. This chapter provides an overview of current processing and analytics platforms needed to support such analysis, as well as analytics techniques that can be applied in such environments. The chapter looks beyond traditional event processing system technology to consider a broader big data context that involves “data at rest” platforms and solutions. The chapter includes a case study showing the use of EventSwarm complex event processing engine for a class of analytics problems in finance. The chapter concludes with several research challenges, such as the need for new approaches and algorithms required to support real-time data filtering, data exploration, statistical data analysis, and machine learning.}
}
@article{YANG2019277,
title = {Ontology: Footstone for Strong Artificial Intelligence},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {4},
pages = {277-280},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003701},
url = {https://www.sciencedirect.com/science/article/pii/S1001929420300080},
author = {Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu},
keywords = {ontology, artificial intelligence, biomedicine, big data},
abstract = {Abstract
In the past ten years, the application of artificial intelligence (AI) in biomedicine has increased rapidly, which roots in the rapid growth of biomedicine data, the improvement of computing performance, and the development of deep learning methods. At present, there are great difficulties in front of AI for solving complex and comprehensive medical problems. Ontology can play an important role in how to make machines have stronger intelligence and has wider applications in the medical field. By using ontologies, (meta) data can be standardized so that data quality is improved and more data analysis methods can be introduced, data integration can be supported by the semantics relationships which are specified in ontologies, and effective logic expression in nature language can be better understood by machine. This can be a pathway to stronger AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and Terminology was held in Beijing in autumn 2019, with the theme “Making Machine Understand Data”. The success of this conference further improves the development of ontology in the field of biomedical information in China, and will promote the integration of Chinese ontology research and application with the international standards and the findability, accessibility, interoperability, and reusability(FAIR) Data Principle.}
}
@article{LIU2022102936,
title = {A review of spatially-explicit GeoAI applications in Urban Geography},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {112},
pages = {102936},
year = {2022},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2022.102936},
url = {https://www.sciencedirect.com/science/article/pii/S1569843222001339},
author = {Pengyuan Liu and Filip Biljecki},
keywords = {Urban studies, Deep learning, Socio-economics, Location encoder, Graph neural network},
abstract = {Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.}
}
@article{TAGLIAFERRI201873,
title = {A new standardized data collection system for interdisciplinary thyroid cancer management: Thyroid COBRA},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {73-78},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518300621},
author = {Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni},
keywords = {Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer management},
abstract = {The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.}
}
@article{JIANG2021172,
title = {Data consistency method of heterogeneous power IOT based on hybrid model},
journal = {ISA Transactions},
volume = {117},
pages = {172-179},
year = {2021},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821000665},
author = {Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li},
keywords = {Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method},
abstract = {The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.}
}
@article{PATONAI2021e00203,
title = {Integrating trophic data from the literature: The Danube River food web},
journal = {Food Webs},
volume = {28},
pages = {e00203},
year = {2021},
issn = {2352-2496},
doi = {https://doi.org/10.1016/j.fooweb.2021.e00203},
url = {https://www.sciencedirect.com/science/article/pii/S2352249621000161},
author = {Katalin Patonai and Ferenc Jordán},
keywords = {Aggregation, Danube River, Food web, Incomplete data, Taxonomy},
abstract = {In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.}
}
@article{LEE201820,
title = {Industrial Artificial Intelligence for industry 4.0-based manufacturing systems},
journal = {Manufacturing Letters},
volume = {18},
pages = {20-23},
year = {2018},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213846318301081},
author = {Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare},
keywords = {Industrial AI, Industry 4.0, Big data, Smart manufacturing, Cyber physical systems},
abstract = {The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.}
}
@article{LEAL2021100172,
title = {Smart Pharmaceutical Manufacturing: Ensuring End-to-End Traceability and Data Integrity in Medicine Production},
journal = {Big Data Research},
volume = {24},
pages = {100172},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100172},
url = {https://www.sciencedirect.com/science/article/pii/S221457962030040X},
author = {Fátima Leal and Adriana E. Chis and Simon Caton and Horacio González–Vélez and Juan M. García–Gómez and Marta Durá and Angel Sánchez–García and Carlos Sáez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier},
keywords = {ALCOA, Blockchain, Data anaytics, Data quality, Intelligent agents, Smart contracts},
abstract = {Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.}
}
@article{HEZEL20181,
title = {What we know about elemental bulk chondrule and matrix compositions: Presenting the ChondriteDB Database},
journal = {Geochemistry},
volume = {78},
number = {1},
pages = {1-14},
year = {2018},
issn = {0009-2819},
doi = {https://doi.org/10.1016/j.chemer.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0009281916302896},
author = {Dominik C. Hezel and Markus Harak and Guy Libourel},
keywords = {Chondrules, Matrix, Elemental composition, ChondritedDB, Database},
abstract = {Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as ‹big data› analyses and promises deep insights – in this case – to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.}
}
@incollection{NASSEHI2022317,
title = {Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems},
editor = {Dimitris Mourtzis},
booktitle = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
publisher = {Elsevier},
pages = {317-348},
year = {2022},
isbn = {978-0-12-823657-4},
doi = {https://doi.org/10.1016/B978-0-12-823657-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236574000026},
author = {Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu},
keywords = {Predictive maintenance, Artificial intelligence, Machine learning, Smart manufacturing, Industry 4.0},
abstract = {With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.}
}
@article{KARIM2016214,
title = {Maintenance Analytics – The New Know in Maintenance},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {28},
pages = {214-219},
year = {2016},
note = {3rd IFAC Workshop on Advanced Maintenance Engineering, Services and Technology AMEST 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316324612},
author = {Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar},
keywords = {big data, maintenance analytics, eMaintenance, Knowledge discovery, maintenance decision support},
abstract = {Abstract:
Decision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) “Maintenance Descriptive Analytics (monitoring)”; 2) “Maintenance Diagnostic Analytics”; 3) “Maintenance Predictive Analytics”; and 4) “Maintenance Prescriptive analytics”.}
}
@article{SETER201959,
title = {The data driven transport research train is leaving the station. Consultants all aboard?},
journal = {Transport Policy},
volume = {80},
pages = {59-69},
year = {2019},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X17305589},
author = {Hanne Seter and Petter Arnesen and Odd André Hjelkrem},
abstract = {This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.}
}
@article{HE201714946,
title = {Statistical Process Monitoring for IoT-Enabled Cybermanufacturing: Opportunities and Challenges},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {14946-14951},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2546},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317334717},
author = {Q. Peter He and Jin Wang and Devarshi Shah and Nader Vahdat},
keywords = {Cybermanufacturing, Internet of Things, sensors, statistical process monitoring, fault detection, fault diagnosis, statistics pattern analysis},
abstract = {Initiated from services and consumer products industries, there is a growing interest in using Internet of Things (IoT) technologies in various industries. In particular, IoT-enabled cybermanufacturing starts to draw increasing attention. Because IoT devices such as IoT sensors are usually much cheaper and smaller than the traditional sensors, there is a potential for instrumenting manufacturing systems with massive number of sensors. The premise is that the big data subsequently collected from IoT sensors can be utilized to advance manufacturing. Therefore, data-driven statistical process monitoring (SPM) is expected to contribute significantly to the advancement of cybermanufacturing. In this work, the state-of-the-art in cybermanufacturing is reviewed; an IoT-enabled manufacturing technology testbed (MTT) was built to explore the potential of IoT sensors for manufacturing, as well as to understand the characteristics of data produced by the IoT sensors; finally, the potentials and challenges associated with big data analytics presented by cybermanufacturing systems is discussed; and we propose statistics pattern analysis (SPA) as a promising SPM tool for cybermanufacturing.}
}
@article{JUNG202115,
title = {The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems},
journal = {Current Opinion in Biotechnology},
volume = {70},
pages = {15-22},
year = {2021},
note = {Food Biotechnology ● Plant Biotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0958166920301257},
author = {Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles},
abstract = {Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade’s agricultural and human nutrition challenges.}
}
@article{LEE2020157,
title = {Machine learning for enterprises: Applications, algorithm selection, and challenges},
journal = {Business Horizons},
volume = {63},
number = {2},
pages = {157-170},
year = {2020},
note = {ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301521},
author = {In Lee and Yong Jae Shin},
keywords = {Machine learning, Artificial intelligence, Deep learning, Big data, Neural networks, Chatbot, Innovation capability, Resources and capabilities},
abstract = {Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.}
}
@article{BOHNSACK2019799,
title = {What the hack? A growth hacking taxonomy and practical applications for firms},
journal = {Business Horizons},
volume = {62},
number = {6},
pages = {799-818},
year = {2019},
note = {Digital Transformation & Disruption},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301247},
author = {René Bohnsack and Meike Malena Liesner},
keywords = {Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data},
abstract = {As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.}
}
@article{WU202248,
title = {Process modeling by integrating quantitative and qualitative information using a deep embedding network and its application to an extrusion process},
journal = {Journal of Process Control},
volume = {115},
pages = {48-57},
year = {2022},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2022.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0959152422000749},
author = {Haibin Wu and Yu-Han Lo and Le Zhou and Yuan Yao},
keywords = {Process modeling, Small data, Deep neural network, Embedding, Autoencoder},
abstract = {In the big data era, small data problems still exist in many industrial sectors. Taking the high-value process industries as an example, a large number of materials and processing methods are often tested at the design stage. However, only a small amount of data can be collected for each material-process combination, which poses a serious challenge to data-driven process modeling. There is a great necessity to integrate the small data measured in different tasks and build the process model by sharing the information. In this work, a deep embedding neural network is proposed to extract the qualitative task information for process modeling. Specifically, an autoencoder is used to learn embeddings which are combined with the quantitative process conditions as the inputs of a feed-forward neural network to produce the final predictions. The feasibility, including interpretability and prediction accuracy, of the developed method is illustrated with an extrusion process.}
}
@article{EHRING2021163,
title = {SMART standards - concept for the automated transfer of standard contents into a machine-actionable form},
journal = {Procedia CIRP},
volume = {100},
pages = {163-168},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004868},
author = {Dominik Ehring and Janosch Luttmer and Robin Pluhnau and Arun Nagarajah},
keywords = {SMART Standards, knowledge representation, automatic extraction, transfer, 3M model of Duisburg},
abstract = {Standards are - not directly visible to everyone – omnipresent in nearly every development process. In times of digitalization, where buzzwords such as "connectivity of machines", "artificial intelligence", “big data”, “cloud computing” or “smart factories” are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today’s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the “3M Framework of Duisburg” and thus answers questions of modularization, modeling and management, consists of six steps "extraction", "modeling", “modification”, "fusion and storage", "provision" and "application", to digitalize existing content, is presented and discussed.}
}
@article{ABRAHAM2019424,
title = {Data governance: A conceptual framework, structured review, and research agenda},
journal = {International Journal of Information Management},
volume = {49},
pages = {424-438},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219300787},
author = {Rene Abraham and Johannes Schneider and Jan {vom Brocke}},
keywords = {Data governance, Information governance, Conceptual framework, Literature review, Research agenda},
abstract = {Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.}
}
@article{HAMILTON2020103926,
title = {Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development},
journal = {Food Quality and Preference},
volume = {83},
pages = {103926},
year = {2020},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2020.103926},
url = {https://www.sciencedirect.com/science/article/pii/S0950329319308304},
author = {Leah M. Hamilton and Jacob Lahne},
keywords = {Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning},
abstract = {As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.}
}
@article{LI2022101808,
title = {Urban population distribution in China: Evidence from internet population},
journal = {China Economic Review},
volume = {74},
pages = {101808},
year = {2022},
issn = {1043-951X},
doi = {https://doi.org/10.1016/j.chieco.2022.101808},
url = {https://www.sciencedirect.com/science/article/pii/S1043951X22000669},
author = {Huixuan Li and Jing Chen and Zihao Chen and Jianguo Xu},
keywords = {Internet population, Population distribution, Zipf's law, Public resource distortions, Big data},
abstract = {Based on mobile internet user data, we construct an “Internet population” measure and reexamine spatial population distribution in China. The location based service (LBS) data of mobile internet uses is able to capture the accurate location of users' residence and solve the underestimation problem of missing migrants. We have three main findings. First, contrary to previous studies based on traditional population statistics, city size distribution of Internet population fits well into Zipf's law with a R2 of 90.7%. Second, the Internet population indicator is superior to traditional population statistics in explaining inelastic household consumption such as water consumption, electricity consumption, and garbage disposal. It suggests that the “Internet population” is a better proxy of actual city population. Third, the traditional population statistics systematically overestimate population in small cities and underestimate population in large cities. It indicates that the public resource distortions will continue to exist or even worsen off in China if the allocation process relies greatly on traditional population statistics. Although no measures are perfect, our new population measure provides important incremental information for future discussion.}
}
@article{YU2022103483,
title = {Towards a privacy-preserving smart contract-based data aggregation and quality-driven incentive mechanism for mobile crowdsensing},
journal = {Journal of Network and Computer Applications},
volume = {207},
pages = {103483},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103483},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522001291},
author = {Ruiyun Yu and Ann Move Oguti and Dennis Reagan Ochora and Shuchen Li},
keywords = {Mobile crowdsensing, Smart contracts, Data aggregation, Incentive mechanism, dApp, IPFS},
abstract = {The crowd's power, combined with the sensing capabilities of smart mobile de-vices, has resulted in the emergence of a revolutionary data acquisition paradigm known as Mobile Crowdsensing. In exchange for rewards, mobile users collect and share location-specific data values. However, most existing crowdsensing systems built on traditional centralized architectures are highly prone to attacks, intrusions, single point of failure, manipulations, and low reliability. Recently, decentralized blockchain technologies are being applied in mobile crowdsensing systems to ensure workers' privacy, data privacy, and the quality of sensed data at a low service fee. By leveraging blockchain technology, this paper inherits the advantages of the public blockchain without the need for any trusted third-parties. We propose a smart contract-based privacy-preserving data aggregation and quality assessment protocol to infer reliable aggregated results and estimate data quality, wherein, we design a fair quality-driven incentive mechanism to distribute rewards based on the data quality. The protocol ensures a secure, cost-optimal, and reliable aggregation and estimation of the sensed data quality on the public blockchain without disclosing the sensed data's and participants' privacy. We adopt Interplanetary File Systems to offset the blockchain's expensive storage costs. Experiments were conducted using real-world datasets which were implemented on a full-stack on-chain and off-chain decentralized application on the Ethereum blockchain. The experimental results demonstrate our design is highly efficient in achieving privacy-preserving data aggregation and significantly reduces on-chain computation costs.}
}
@article{FAN2018296,
title = {Unsupervised data analytics in mining big building operational data for energy efficiency enhancement: A review},
journal = {Energy and Buildings},
volume = {159},
pages = {296-308},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817326671},
author = {Cheng Fan and Fu Xiao and Zhengdao Li and Jiayuan Wang},
keywords = {Unsupervised data mining, Big data, Building operational performance, Building energy management, Building energy efficiency},
abstract = {Building operations account for the largest proportion of energy use throughout the building life cycle. The energy saving potential is considerable taking into account the existence of a wide variety of building operation deficiencies. The advancement in information technologies has made modern buildings to be not only energy-intensive, but also information-intensive. Massive amounts of building operational data, which are in essence the reflection of actual building operating conditions, are available for knowledge discovery. It is very promising to extract potentially useful insights from big building operational data, based on which actionable measures for energy efficiency enhancement are devised. Data mining is an advanced technology for analyzing big data. It consists of two main types of data analytics, i.e., supervised and unsupervised analytics. Despite of the power of supervised analytics in predictive modeling, unsupervised analytics are more practical and promising in discovering novel knowledge given limited prior knowledge. This paper provides a comprehensive review on the current utilization of unsupervised data analytics in mining massive building operational data. The commonly used unsupervised analytics are summarized according to their knowledge representations and applications. The challenges and opportunities are elaborated as guidance for future research in this multi-disciplinary field.}
}
@article{QIU2020115,
title = {Research on Cost Management Optimization of Financial Sharing Center Based on RPA},
journal = {Procedia Computer Science},
volume = {166},
pages = {115-119},
year = {2020},
note = {Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920301538},
author = {Yu Lian Qiu and Guo Fang Xiao},
keywords = {RPA, Financial shared service center, Cost management, Process optimization, Big data},
abstract = {With the development of artificial intelligence technology, the widespread application of robot process automation (RPA) in the future financial field has become an inevitable trend. Through the review of the current situation of cost management of A Group’s financial shared service center, the article deeply expounds the problems that the current cross-system data cannot be automatically collected, the cost accounting is not timely, and the cost analysis report mode is too fixed. Based on Robot Process Automation (RPA), cost management process optimization and improvement were made on the cross-system data acquisition, "Cloud Purchasing Platform" construction, and comprehensive multi-dimensional cost analysis. It is expected to provide reference for the robot process automation application of the financial shared service center.}
}
@article{NASHAAT2019131,
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
journal = {Information and Software Technology},
volume = {113},
pages = {131-145},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301247},
author = {Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study},
abstract = {Context
The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.}
}
@article{CHOUVARDA201522,
title = {Connected health and integrated care: Toward new models for chronic disease management},
journal = {Maturitas},
volume = {82},
number = {1},
pages = {22-27},
year = {2015},
note = {PERSONALIZED HEALTHCARE FOR MIDLIFE AND BEYOND},
issn = {0378-5122},
doi = {https://doi.org/10.1016/j.maturitas.2015.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0378512215006052},
author = {Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras},
keywords = {Connected health, Integrated care, Personal health system, Electronic health},
abstract = {The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.}
}
@article{STEVENS201515,
title = {Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems},
journal = {Spatial and Spatio-temporal Epidemiology},
volume = {13},
pages = {15-29},
year = {2015},
issn = {1877-5845},
doi = {https://doi.org/10.1016/j.sste.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877584515000179},
author = {Kim B. Stevens and Dirk U. Pfeiffer},
keywords = {Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered geographic information},
abstract = {During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.}
}
@article{POWELL2022100261,
title = {Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100261},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100261},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000595},
author = {Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov},
keywords = {Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains},
abstract = {The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.}
}
@article{LIANG201887,
title = {Application and research of global grid database design based on geographic information},
journal = {Global Energy Interconnection},
volume = {1},
number = {1},
pages = {87-95},
year = {2018},
issn = {2096-5117},
doi = {https://doi.org/10.14171/j.2096-5117.gei.2018.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S2096511718300112},
author = {Xuming Liang},
keywords = {Big data collection, Geographic information, Grid database, Data mining},
abstract = {Energy crisis and climate change have become two seriously concerned issues universally. As a feasible solution, Global Energy Interconnection (GEI) has been highly praised and positively responded by the international community once proposed by China. From strategic conception to implementation, GEI development has entered a new phase of joint action now. Gathering and building a global grid database is a prerequisite for conducting research on GEI. Based on the requirement of global grid data management and application, combining with big data and geographic information technology, this paper studies the global grid data acquisition and analysis process, sorts out and designs the global grid database structure supporting GEI research, and builds a global grid database system.}
}
@article{AN2022107632,
title = {Variable screening based on Gaussian Centered L-moments},
journal = {Computational Statistics & Data Analysis},
pages = {107632},
year = {2022},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2022.107632},
url = {https://www.sciencedirect.com/science/article/pii/S0167947322002122},
author = {Hyowon An and Kai Zhang and Hannu Oja and J.S. Marron},
keywords = {Robust statistics, L-moments, L-statistics, Skewness, Kurtosis},
abstract = {An important challenge in big data is identification of important variables. For this purpose, methods of discovering variables with non-standard univariate marginal distributions are proposed. The conventional moments based summary statistics can be well-adopted, but their sensitivity to outliers can lead to selection based on a few outliers rather than distributional shape such as bimodality. To address this type of non-robustness, the L-moments are considered. Using these in practice, however, has a limitation since they do not take zero values at the Gaussian distributions to which the shape of a marginal distribution is most naturally compared. As a remedy, Gaussian Centered L-moments are proposed, which share advantages of the L-moments but have zeros at the Gaussian distributions. The strength of Gaussian Centered L-moments over other conventional moments is shown in theoretical and practical aspects such as their performances in screening important genes in cancer genetics data.}
}
@article{DEY20191317,
title = {Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review},
journal = {Journal of the American College of Cardiology},
volume = {73},
number = {11},
pages = {1317-1335},
year = {2019},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2018.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S0735109719302360},
author = {Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick},
keywords = {artificial intelligence, cardiovascular imaging, deep learning, machine learning},
abstract = {Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.}
}
@article{LAIFA2021981,
title = {Train delay prediction in Tunisian railway through LightGBM model},
journal = {Procedia Computer Science},
volume = {192},
pages = {981-990},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015891},
author = {Hassiba Laifa and Raoudha khcherif and Henda Hajjami {Ben Ghezalaa}},
keywords = {Delay prediction, Data Analysis, Machine learning, LightGBM},
abstract = {Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.}
}
@article{SAVOLAINEN202095,
title = {Organisational Constraints in Data-driven Maintenance: a case study in the automotive industry},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {95-100},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301592},
author = {P Savolainen and J Magnusson and M. Gopalakrishnan and E. {Turanoglu Bekar} and A. Skoogh},
keywords = {Maintenance Management, Decision Support, Data Quality, Data-driven Decisions, Organisational Factors, Smart Maintenance},
abstract = {Technological development and innovations has been the focus of research in the field of smart maintenance, whereas there is less research regarding how maintenance organisations adapt the development. This case study focuses to understand what constraints maintenance organisations in the transition into applying more data-driven decisions in maintenance. This paper aims to emphasize the organisational challenges in data-driven maintenance, such as trustworthiness of data-driven decisions, data quality, management and competences. Through a case study at a global company in the automotive industry these challenges are highlighted and discussed through a questionnaire survey participated by 72 people and interviews with 7 people from the maintenance organisation.}
}
@article{HURTER2014207,
title = {Interactive image-based information visualization for aircraft trajectory analysis},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {47},
pages = {207-227},
year = {2014},
note = {Special Issue: Emerging Technologies Special Issue of ICTIS 2013 – Guest Editors: Liping Fu and Ming Zhong and Special Issue: Visualization & Visual Analytics in Transportation – Guest Editors: Patricia S. Hu and Michael L. Pack},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2014.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X14000710},
author = {C. Hurter and S. Conversy and D. Gianazza and A.C. Telea},
keywords = {Trajectory manipulation, Air traffic control, Image based techniques, Information visualization},
abstract = {Objectives: The objective of the presented work is to present novel methods for big data exploration in the Air Traffic Control (ATC) domain. Data is formed by sets of airplane trajectories, or trails, which in turn records the positions of an aircraft in a given airspace at several time instants, and additional information such as flight height, speed, fuel consumption, and metadata (e.g. flight ID). Analyzing and understanding this time-dependent data poses several non-trivial challenges to information visualization. Materials and methods: To address this Big Data challenge, we present a set of novel methods to analyze aircraft trajectories with interactive image-based information visualization techniques.As a result, we address the scalability challenges in terms of data manipulation and open questions by presenting a set of related visual analysis methods that focus on decision-support in the ATC domain. All methods use image-based techniques, in order to outline the advantages of such techniques in our application context, and illustrated by means of use-cases from the ATC domain. Results: For each considered use-case, we outline the type of questions posed by domain experts, data involved in addressing these questions, and describe the specific image-based techniques we used to address these questions. Further, for each of the proposed techniques, we describe the visual representation and interaction mechanisms that have been used to address the above-mentioned goals. We illustrate these use-cases with real-life datasets from the ATC domain, and show how our techniques can help end-users in the ATC domain discover new insights, and solve problems, involving the presented datasets.}
}
@article{RISLING201789,
title = {Educating the nurses of 2025: Technology trends of the next decade},
journal = {Nurse Education in Practice},
volume = {22},
pages = {89-92},
year = {2017},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1471595316302748},
author = {Tracie Risling},
keywords = {Informatics, Curriculum development, Technology},
abstract = {The pace of technological evolution in healthcare is advancing. In this article key technology trends are identified that are likely to influence nursing practice and education over the next decade. The complexity of curricular revision can create challenges in the face of rapid practice change. Nurse educators are encouraged to consider the role of electronic health records (EHRs), wearable technologies, big data and data analytics, and increased patient engagement as key areas for curriculum development. Student nurses, and those already in practice, should be offered ongoing educational opportunities to enhance a wide spectrum of professional informatics skills. The nurses of 2025 will most certainly inhabit a very different practice environment than what exists today and technology will be key in this transformation. Nurse educators must prepare now to lead these practitioners into the future.}
}
@incollection{20151,
title = {Chapter One - Introduction},
editor = {Olivier Curé and Guillaume Blin},
booktitle = {RDF Database Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-8},
year = {2015},
isbn = {978-0-12-799957-9},
doi = {https://doi.org/10.1016/B978-0-12-799957-9.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780127999579000018},
keywords = {RDF, Web of Data, Semantic Web, Big Data, Data management},
abstract = {This chapter motivates the importance of RDF data management through the Big Data and Web of Data/Semantic Web phenomena. It also provides some insights of existing RDF stores and presents the dimensions used in this book to compare these systems.}
}
@article{KOURTESIS2014307,
title = {Semantic-based QoS management in cloud systems: Current status and future challenges},
journal = {Future Generation Computer Systems},
volume = {32},
pages = {307-323},
year = {2014},
note = {Special Section: The Management of Cloud Systems, Special Section: Cyber-Physical Society and Special Section: Special Issue on Exploiting Semantic Technologies with Particularization on Linked Data over Grid and Cloud Architectures},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1300232X},
author = {Dimitrios Kourtesis and Jose María Alvarez-Rodríguez and Iraklis Paraskakis},
keywords = {Cloud systems, Quality of service, Service oriented architectures, Semantics, Ontologies, Linked data, Sensor data, Big data},
abstract = {Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.}
}
@article{SCHLEICH20183,
title = {Geometrical Variations Management 4.0: towards next Generation Geometry Assurance},
journal = {Procedia CIRP},
volume = {75},
pages = {3-10},
year = {2018},
note = {The 15th CIRP Conference on Computer Aided Tolerancing, CIRP CAT 2018, 11-13 June 2018, Milan, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.078},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305948},
author = {Benjamin Schleich and Kristina Wärmefjord and Rikard Söderberg and Sandro Wartzack},
keywords = {Industry 4.0, Digital Twin, Geometry Assurance},
abstract = {Product realization processes are undergoing radical change considering the increasing digitalization of manufacturing fostered by cyber-physical production systems, the internet of things, big data, cloud computing, and the advancing use of digital twins. These trends are subsumed under the term “industry 4.0” describing the vision of a digitally connected manufacturing environment. The contribution gives an overview of future challenges and potentials for next generation geometry assurance and geometrical variations management in the context of industry 4.0. Particularly, the focus is set on potentials and risks of increasingly available manufacturing data and the use of digital twins in geometrical variations management.}
}
@article{LIU2022101687,
title = {Review on automated condition assessment of pipelines with machine learning},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101687},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101687},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001471},
author = {Yiming Liu and Yi Bao},
keywords = {Big data, Condition assessment, Machine learning, Nondestructive testing, Pipeline, SWOT},
abstract = {Pipelines carrying energy products play vital roles in economic wealth and public safety, but incidents continue occurring. Condition assessment of pipelines is essential to identify anomalies timely. Advanced sensing technologies obtain informative data for condition assessment, while data analysis by human has limited efficiency, accuracy, and reliability. Advances in machine learning offer exciting opportunities for automated condition assessment with minimum human intervention. This paper reviews machine learning approaches to detect, classify, locate, and quantify pipeline anomalies based on intelligent interpretation of routine operation data, nondestructive testing data, and computer vision data. Statistics and uncertainties of performance metrics of machine learning approaches are discussed. An analysis on strengths, weaknesses, opportunities, and threats (SWOT) is performed. Guides for practitioners to perform automated pipeline condition assessment are recommended. This review provide insights into the machine learning approaches for automated pipeline condition assessment. The SWOT analysis will support decision making in the pipeline industry.}
}
@article{MIA2022100238,
title = {A privacy-preserving National Clinical Data Warehouse: Architecture and analysis},
journal = {Smart Health},
volume = {23},
pages = {100238},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000544},
author = {Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed},
keywords = {Privacy, Standardization, Clinical data warehousing, Data modeling, Big data analytics, Decision supports},
abstract = {A centralized clinical data repository is essential for inspecting patients’ medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.}
}
@article{NYOMANKUTHAKRISNAWIJAYA2022106813,
title = {Data analytics platforms for agricultural systems: A systematic literature review},
journal = {Computers and Electronics in Agriculture},
volume = {195},
pages = {106813},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106813},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922001302},
author = {Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol},
keywords = {Data analytics platforms, Agriculture, Systematic literature review, Big Data},
abstract = {With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.}
}
@article{YUAN201786,
title = {Exploring inter-country connection in mass media: A case study of China},
journal = {Computers, Environment and Urban Systems},
volume = {62},
pages = {86-96},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2016.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516303313},
author = {Yihong Yuan and Yu Liu and Guixing Wei},
keywords = {Time series, Inter-country relations, Spatio-temporal data mining, Mass media events, GDELT},
abstract = {The development of theories and techniques for big data analytics offers tremendous possibility for investigating large-scale events and patterns that emerge over space and time. In this research, we utilize a unique open dataset “The Global Data on Events, Location and Tone” (GDELT) to model the image of China in mass media, specifically, how China has related to the rest of the world and how this connection has evolved upon time. The results of this research contribute to both the methodological and the empirical perspectives: We examined the effectiveness of the dynamic time warping (DTW) distances in measuring the differences between long-term mass media data. We identified four types of connection strength patterns between China and its top 15 related countries. With that, the distance decay effect in mass media is also examined and compared with social media and public transportation data. While using multiple datasets and focusing on mass media, this study generates valuable input regarding the interpretation of the diplomatic and regional correlation for the nation of China. It also provides methodological references for investigating international relations in other countries and regions in the big data era.}
}
@article{MA2020109941,
title = {A bi-directional missing data imputation scheme based on LSTM and transfer learning for building energy data},
journal = {Energy and Buildings},
volume = {216},
pages = {109941},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109941},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819333717},
author = {Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai},
keywords = {Bi-directional estimation, Building energy, Deep learning, Electric power, Missing data, Transfer learning},
abstract = {Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data/machine learning/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.}
}
@article{LI20191234,
title = {Internet of Things to network smart devices for ecosystem monitoring},
journal = {Science Bulletin},
volume = {64},
number = {17},
pages = {1234-1245},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319304013},
author = {Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou},
keywords = {Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless sensor network, Smart device},
abstract = {Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.}
}
@article{CHEN2020104344,
title = {Robust Bayesian networks for low-quality data modeling and process monitoring applications},
journal = {Control Engineering Practice},
volume = {97},
pages = {104344},
year = {2020},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2020.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0967066120300289},
author = {Guangjie Chen and Zhiqiang Ge},
keywords = {Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis},
abstract = {In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.}
}
@article{JAVAID2022124,
title = {Evolutionary trends in progressive cloud computing based healthcare: Ideas, enablers, and barriers},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {3},
pages = {124-135},
year = {2022},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666307422000134},
author = {Mohd Javaid and Abid Haleem and Ravi Pratap Singh and Shanay Rab and Rajiv Suman and Ibrahim Haleem Khan},
keywords = {Cloud computing, Healthcare, Applications, Data, Information},
abstract = {Cloud computing is one of the significant facilitators of the health information revolution in the healthcare business. The global exchange of records in the health sector through electronic media is facilitated by cloud computing. In healthcare, this technology increases safety and creates innovation. Communication with the health matrix throughout the world makes feasible by the application of this technology. Cloud computing has been utilised in health care for many years and has evolved in conjunction with developments in business. This technology establishes standard accessible hardware for diverse healthcare applications via a network connection. Cloud computing and processing ensure safe communication, and the cloud servers secure all essential data. Doctors can counsel their individuals on their health and broadcast their patient's daily health regimes, typically keeping their minds and bodies healthy. Psychologists and psychiatrists can use videoconferencing that makes patients comfortable with their patients. This paper discusses cloud computing and its need for healthcare. Major key advantages, barriers, and challenges of Cloud computing for the healthcare industry are identified. Finally, it discusses the significant applications of cloud computing for healthcare. Today more and more healthcare suppliers are providing Internet of Things (IoT) enabled gadgets to patients, and patient data are instantly communicated to their doctors by linking such devices to the cloud system of hospitals. As a result, cloud computing, in conjunction with fast-expanding technologies such as Big Data analytics, artificial intelligence, and the internet of medical things, improves efficiencies and expands the number of ways to streamline healthcare delivery. It improves resource availability, improves interoperability, and reduces costs.}
}
@article{NABATI2017160,
title = {Data Driven Decision Making in Planning the Maintenance Activities of Off-shore Wind Energy},
journal = {Procedia CIRP},
volume = {59},
pages = {160-165},
year = {2017},
note = {Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630960X},
author = {Elaheh Gholamzadeh Nabati and Klaus-Dieter Thoben},
keywords = {Maintenance, (Big) data analysis, Off-shore wind turbines, Decision making},
abstract = {Planning and scheduling for wind farms play a critical role in the costs of maintenance. The use and analysis of field data or so-called Product Use Information (PUI) to improve maintenance activities and to reduce the costs has gained attention in the recent years. The product use data consist of sources such as measure of sensors on the turbines, the alarms information or signals from the condition monitoring, Supervisory Control and Data Acquisition (SCADA) systems, which are currently used in maintenance activities. However, those data have the potential to offer alternative solutions to improve processes and provide better decisions, by transforming them into actionable knowledge. In order to make the right decision it is important to understand, which PUI data source and which data analysis methods, are suitable for what kind of decision making task. The aim of this study is to discover, how analysis of PUI can help in the maintenance processes of off-shore wind power. The techniques from the field of big data analytics for analyzing the PUI are here addressed. The results of this study contain suggestions on the basis of algorithms of data analytics, suitable for each decision type.}
}
@article{ZARKOWSKY2021260,
title = {Artificial intelligence's role in vascular surgery decision-making},
journal = {Seminars in Vascular Surgery},
volume = {34},
number = {4},
pages = {260-267},
year = {2021},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0895796721000624},
author = {Devin S. Zarkowsky and David P. Stonko},
abstract = {ABSTRACT
Artificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on “big data” and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.}
}
@article{KHATRI2016673,
title = {Managerial work in the realm of the digital universe: The role of the data triad},
journal = {Business Horizons},
volume = {59},
number = {6},
pages = {673-688},
year = {2016},
note = {CYBERSECURITY IN 2016: PEOPLE, TECHNOLOGY, AND PROCESSES},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681316300519},
author = {Vijay Khatri},
keywords = {Analytics, Big data, Managerial decision making, Managerial work, Digital universe},
abstract = {With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.}
}
@article{AMUTHABALA2019233,
title = {Robust analysis and optimization of a novel efficient quality assurance model in data warehousing},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {233-244},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618318470},
author = {P. Amuthabala and R. Santhosh},
keywords = {Data warehouse, Distributed, Data complexity, Data quality, Quality assurance, Optimization, Machine learning},
abstract = {The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.}
}
@article{MICHAILIDOU2022101953,
title = {EQUALITY: Quality-aware intensive analytics on the edge},
journal = {Information Systems},
volume = {105},
pages = {101953},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101953},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001496},
author = {Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas},
keywords = {Fog computing, Optimization, Sensors, Data quality},
abstract = {Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.}
}
@incollection{LINSTEDT20161,
title = {Chapter 1 - Introduction to Data Warehousing},
editor = {Daniel Linstedt and Michael Olschimke},
booktitle = {Building a Scalable Data Warehouse with Data Vault 2.0},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-15},
year = {2016},
isbn = {978-0-12-802510-9},
doi = {https://doi.org/10.1016/B978-0-12-802510-9.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025109000015},
author = {Daniel Linstedt and Michael Olschimke},
keywords = {data, data warehouse, big data, decision support systems, scalability, business intelligence},
abstract = {This chapter introduces basic terminology of data warehousing, its applications, and the business context. It provides a brief description of its history and where it is heading. Basic data warehouse architectures that have been established in the industry are presented. Issues faced by data warehouse practitioners are explained, including topics such as big data, changing business requirements, performance issues, complexity, auditability, restart checkpoints, and fluctuation of team members.}
}
@incollection{SPANAKI2022147,
title = {Chapter 9 - Digital architectures: frameworks for supply chain data and information governance},
editor = {Bart L. MacCarthy and Dmitry Ivanov},
booktitle = {The Digital Supply Chain},
publisher = {Elsevier},
pages = {147-161},
year = {2022},
isbn = {978-0-323-91614-1},
doi = {https://doi.org/10.1016/B978-0-323-91614-1.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323916141000095},
author = {Konstantina Spanaki and Erisa Karafili and Stella Despoudi},
keywords = {Data access control, Data flows, Data quality, Data sharing, Information architecture, Information flows, Supply chain},
abstract = {Advances in digitalization present new and emerging Supply Chain (SC) Information Architectures that rely on data and information as vital resources. While the importance of data and information in SCs has long been understood, there is a dearth of research or understanding about the effective governance, control, or management of data ecosystems at the SC level. This chapter examines data architectures through a navigation of the background of database management and data quality research of previous decades. The chapter unfolds the critical architectural elements around data and information sharing in the SC regarding the context, systems, and infrastructure. A review of various frameworks and conceptual models is presented on data and information in SCs, as well as access control policies. The critical importance of data quality and the management of data in the cyber-physical systems are highlighted. Policies for data sharing agreements (DSAs) and access control are discussed and the importance of effective governance in the distributed environments of digitally enabled SCs is emphasized. We extend the concept of data sharing agreements to capture the interplay between the various SC stakeholders around data use. Research gaps and needs relevant to new and emerging SC data and information ecosystems are highlighted.}
}
@incollection{BOREK201423,
title = {Chapter 2 - Enterprise Information Management},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {23-38},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405547600002X},
author = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
keywords = {EIM Strategy, EIM Governance, EIM Components, Big Data and EIM, Challenges for EIM},
abstract = {This chapter gives an introduction to concept of enterprise information management, investigates the influence of Big Data on EIM, and discusses today's key challenges and pressures for EIM.}
}
@article{VILLAHENRIKSEN202060,
title = {Internet of Things in arable farming: Implementation, applications, challenges and potential},
journal = {Biosystems Engineering},
volume = {191},
pages = {60-84},
year = {2020},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2019.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1537511020300039},
author = {Andrés Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Grøn Sørensen},
keywords = {Smart farming, Internet of things, Wireless sensor network, Farm management information system, Big data, Machine learning},
abstract = {The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.}
}
@article{LIOUTAS2021103023,
title = {Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us},
journal = {Agricultural Systems},
volume = {187},
pages = {103023},
year = {2021},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2020.103023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X20308842},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari},
keywords = {Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience},
abstract = {The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.}
}
@article{MANTELERO2016238,
title = {Personal data for decisional purposes in the age of analytics: From an individual to a collective dimension of data protection},
journal = {Computer Law & Security Review},
volume = {32},
number = {2},
pages = {238-255},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2016.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0267364916300280},
author = {Alessandro Mantelero},
keywords = {Big data, Right to privacy, Data protection, Group privacy, Collective interests, Data protection authorities, Risk assessment},
abstract = {In the big data era, new technologies and powerful analytics make it possible to collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This is due to the fact that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the larger scale of data processing and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of data processing. On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interests.}
}
@article{ANDRADE2019102352,
title = {Cognitive security: A comprehensive study of cognitive science in cybersecurity},
journal = {Journal of Information Security and Applications},
volume = {48},
pages = {102352},
year = {2019},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2214212618307804},
author = {Roberto O Andrade and Sang Guun Yoo},
keywords = {Cognitive security, Cognitive science, Situation awareness, Cyber operations},
abstract = {Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.}
}
@article{OMRI202023,
title = {Industrial data management strategy towards an SME-oriented PHM},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {23-36},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300467},
author = {N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni},
keywords = {Small and medium-sized enterprises, Data-driven PHM, Industrial data management, Data quality metrics, PHM implementation strategy},
abstract = {The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.}
}
@article{SHARMA2018103,
title = {Big GIS analytics framework for agriculture supply chains: A literature review identifying the current trends and future perspectives},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {103-120},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918311311},
author = {Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran},
keywords = {Agriculture supply chain, GIS analytics, Big data analytics, Internet of things, Drones, Smart farming},
abstract = {The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.}
}
@article{TSENG2021108244,
title = {Smart product service system hierarchical model in banking industry under uncertainties},
journal = {International Journal of Production Economics},
volume = {240},
pages = {108244},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108244},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321002206},
author = {Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud},
keywords = {Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory},
abstract = {This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.}
}
@incollection{BRAVOMERODIO2021191,
title = {Chapter Four - Translational biomarkers in the era of precision medicine},
editor = {Gregory S. Makowski},
series = {Advances in Clinical Chemistry},
publisher = {Elsevier},
volume = {102},
pages = {191-232},
year = {2021},
issn = {0065-2423},
doi = {https://doi.org/10.1016/bs.acc.2020.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065242320300913},
author = {Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos},
keywords = {Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials},
abstract = {In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.}
}
@article{KRISTOFFERSEN2021120957,
title = {Towards a business analytics capability for the circular economy},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120957},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120957},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003899},
author = {Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li},
keywords = {Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews},
abstract = {Digital technologies are growing in importance for accelerating firms’ circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms’ circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.}
}
@article{ITO2022468,
title = {Improved root cause analysis supporting resilient production systems},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {468-478},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001273},
author = {Adriana Ito and Malin Hagström and Jon Bokrantz and Anders Skoogh and Mario Nawcki and Kanika Gandhi and Dag Bergsjö and Maja Bärring},
keywords = {Root cause analysis, Production disturbances, Production systems, Resilience},
abstract = {Manufacturing companies struggle to be efficient and effective when conducting root cause analyses of production disturbances; a fact which hinders them from creating and developing resilient production systems. This article aims to describe the challenges and enablers identified in current research relating to the different phases of root cause analysis. A systematic literature review was conducted, in which a total of 14 challenges and 17 enablers are identified and described. These correlate to the different phases of root cause analysis. Examples of challenges are “need for expertise”, “employee bias”, “poor data quality” and “lack of data integration”, among others. Examples of enablers are “visualisation tools”, “collaborative platforms”, “thesaurus” and “machine learning techniques”. Based on these findings, the authors also propose potential areas for further research and then design inputs for new solutions to improve root cause analysis. This article provides a theoretical contribution in that it describes the challenges and enablers of root cause analysis and their correlation to the creation of resilient production systems. The article also provides practical contributions, with an overview of current research to support practitioners in gaining insights into potential solutions to be implemented and further developed, with the aim of improving root cause analysis in production systems.}
}
@article{SHARMA2022107217,
title = {Technological revolutions in smart farming: Current trends, challenges & future directions},
journal = {Computers and Electronics in Agriculture},
volume = {201},
pages = {107217},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107217},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005324},
author = {Vivek Sharma and Ashish Kumar Tripathi and Himanshu Mittal},
keywords = {Smart farming, Current Trends in smart farming, Precision agriculture, Agriculture 4.0, Machine Learning},
abstract = {With increasing population, the demand for agricultural productivity is rising to meet the goal of “Zero Hunger”. Consequently, farmers have optimized the agricultural activities in a sustainable way with the modern technologies. This integration has boosted the agriculture production due to high potentiality in assisting the farmers. The impulse towards the technological advancement has revived the traditional agriculture methods and resulted in eco-friendly, sustainable, and efficient farming. This has revolutionized the era of smart farming which primarily alliance with modern technologies like, big data, machine learning, deep learning, swarm intelligence, internet-of-things, block chain, robotics and autonomous system, cloud-fog-edge computing, cyber physical systems, and generative adversarial networks (GAN). To cater the same, a detailed survey on ten hot-spots of smart farming is presented in this paper. The survey covers the technology-wise state-of-the-art methods along with their application domains. Moreover, the publicly available data sets with existing research challenges are investigated. Lastly, the paper concludes with suggestions to the identified problems and possible future research directions.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{LI2022,
title = {Artificial intelligence in radiotherapy},
journal = {Seminars in Cancer Biology},
year = {2022},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2022.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X22001912},
author = {Guangqi Li and Xin Wu and Xuelei Ma},
keywords = {Artificial intelligence, Radiotherapy, Auto-segmentation, Auto-planning, Quality assurance},
abstract = {Radiotherapy is a discipline closely integrated with computer science. Artificial intelligence (AI) has developed rapidly over the past few years. With the explosive growth of medical big data, AI promises to revolutionize the field of radiotherapy through highly automated workflow, enhanced quality assurance, improved regional balances of expert experiences, and individualized treatment guided by multi-omics. In addition to independent researchers, the increasing number of large databases, biobanks, and open challenges significantly facilitated AI studies on radiation oncology. This article reviews the latest research, clinical applications, and challenges of AI in each part of radiotherapy including image processing, contouring, planning, quality assurance, motion management, and outcome prediction. By summarizing cutting-edge findings and challenges, we aim to inspire researchers to explore more future possibilities and accelerate the arrival of AI radiotherapy.}
}
@article{ALASHHAB2021100059,
title = {Impact of coronavirus pandemic crisis on technologies and cloud computing applications},
journal = {Journal of Electronic Science and Technology},
volume = {19},
number = {1},
pages = {100059},
year = {2021},
note = {Special Section on In Silico Research on Microbiology and Public Health},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2020.100059},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X20300665},
author = {Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja’a}},
keywords = {Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home},
abstract = {In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.}
}
@article{FAN201575,
title = {Temporal knowledge discovery in big BAS data for building energy management},
journal = {Energy and Buildings},
volume = {109},
pages = {75-89},
year = {2015},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2015.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S0378778815302991},
author = {Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang},
keywords = {Temporal knowledge discovery, Time series data mining, Big data, Building automation system, Building energy management},
abstract = {With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.}
}
@article{SALIM2020106964,
title = {Modelling urban-scale occupant behaviour, mobility, and energy in buildings: A survey},
journal = {Building and Environment},
volume = {183},
pages = {106964},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106964},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320303231},
author = {Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kjærgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan},
keywords = {Big data, Occupant behaviour, Energy modelling, Mobility, Urban data, Sensors, Machine learning, Energy in buildings, Energy in cities},
abstract = {The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.}
}
@article{MAO2021103052,
title = {Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models},
journal = {iScience},
volume = {24},
number = {9},
pages = {103052},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.103052},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221010208},
author = {Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang},
keywords = {Data analysis in structural biology, Machine learning, Structural biology},
abstract = {Summary
Early quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.}
}
@incollection{OLANIYAN2023275,
title = {Chapter 17 - New trends in deep learning for neuroimaging analysis and disease prediction},
editor = {Ajith Abraham and Sujata Dash and Subhendu Kumar Pani and Laura García-Hernández},
booktitle = {Artificial Intelligence for Neurological Disorders},
publisher = {Academic Press},
pages = {275-287},
year = {2023},
isbn = {978-0-323-90277-9},
doi = {https://doi.org/10.1016/B978-0-323-90277-9.00012-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902779000122},
author = {Olugbemi T. Olaniyan and Charles O. Adetunji and Ayobami Dare and Olorunsola Adeyomoye and Mayowa J. Adeniyi and Alex Enoch},
keywords = {Deep learning, Diagnosis, Neuroimaging, Disease prediction, Cognitive functions},
abstract = {In the last few decades, deep learning techniques for diagnosing and predicting disease conditions from neuroimaging have attracted much attention and interest from the scientific community. Big data and artificial intelligence approaches and innovations are currently being utilized to generate large datasets from images, text, sounds, graphs, and signals. New trends in the utilization of deep learning for disease prediction in neurology, oncology, cardiology, and other areas entail converting patient electronic health records, biological system information, physiological signals, biomarkers, and biomedical images to cognitive functions. The current trends in deep learning techniques focus on utilizing neuroimaging analysis to evaluate alterations in local morphological topographies of different brain sub-regions and then predict novel disorder-linked brain patterns. Hence, this chapter presents a detailed overview of different approaches in deep learning for the prediction of major brain diseases such as mild cognitive impairment, Alzheimer's disease, brain tumors, depressive disorders, traumatic brain injury, schizophrenia, Parkinson's disease, autism spectrum disease, attention-deficit hyperactivity disorder, epilepsy, stroke, multiple sclerosis, and more. The chapter also discusses the current challenges of utilizing deep learning in assessing brain disorders in neuroimaging data.}
}
@article{HUSEIEN2022100116,
title = {A review on 5G technology for smart energy management and smart buildings in Singapore},
journal = {Energy and AI},
volume = {7},
pages = {100116},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100116},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000653},
author = {Ghasan Fahim Huseien and Kwok Wei Shah},
keywords = {5G technology, Sustainability, Smart building, Facilities management, Build environment},
abstract = {Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{BELLINI2014827,
title = {Km4City ontology building vs data harvesting and cleaning for smart-city services},
journal = {Journal of Visual Languages & Computing},
volume = {25},
number = {6},
pages = {827-839},
year = {2014},
note = {Distributed Multimedia Systems DMS2014 Part I},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2014.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X14001165},
author = {Pierfrancesco Bellini and Monica Benigni and Riccardo Billero and Paolo Nesi and Nadia Rauch},
keywords = {Smart city, Knowledge base construction, Reconciliation, Validation and verification of knowledge base, Smart city ontology, Linked open graph, Km4city},
abstract = {Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system for data ingestion and reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big data volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and the big data architecture for the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.}
}
@article{BURNS2022420,
title = {Real-World Evidence for Regulatory Decision-Making: Guidance From Around the World},
journal = {Clinical Therapeutics},
volume = {44},
number = {3},
pages = {420-437},
year = {2022},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2022.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0149291822000170},
author = {Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell},
keywords = {Efficiency, Product effectiveness, Real-world evidence, Regulatory decision making},
abstract = {Purpose
Interest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.
Methods
We assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.
Findings
This article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.
Implications
Cross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.}
}
@article{MINET2017126,
title = {Crowdsourcing for agricultural applications: A review of uses and opportunities for a farmsourcing approach},
journal = {Computers and Electronics in Agriculture},
volume = {142},
pages = {126-138},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917300479},
author = {Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and François Mélard and Bernard Tychon and Joost Wellens and Pierre Defourny},
keywords = {Crowdsourcing, Citizen science, Smart farming, Participatory approaches, Big data, ICT, Data collection},
abstract = {Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers’ participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.}
}
@article{HE2021102867,
title = {State-of-health estimation based on real data of electric vehicles concerning user behavior},
journal = {Journal of Energy Storage},
volume = {41},
pages = {102867},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.102867},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21005892},
author = {Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan},
keywords = {Electric vehicles, SOH, User behavior, LWLR, LSTM},
abstract = {State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.}
}
@article{JIANG2020101505,
title = {Ignorance is bliss? An empirical analysis of the determinants of PSS usefulness in practice},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101505},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101505},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302386},
author = {Huaxiong Jiang and Stan Geertman and Patrick Witte},
keywords = {Smart city, Implementation gap, Success and failure factors, Utility, Usability, Context},
abstract = {Planning support systems (PSS) enabled by smart city technologies (big data and information and communication technologies (ICTs)) are becoming more widespread in their availability, but have not yet been fully recognized as being useful in planning practice. Thus, a better understanding of the determinants of PSS usefulness in practice helps to improve the functional support of PSS for smart cities. This study is based on a recent international questionnaire (268 respondents) designed to evaluate the perceptions of scholars and practitioners in the smart city planning field. Based on the empirical evidence, this paper recommends that it is imperative for PSS developers and users to be more responsive to the fit for task-technology and user-technology (i.e., utility and usability, respectively) since they positively contribute to PSS usefulness in practice and to be more sensitive to the potential negative effects of contextual factors on PSS usefulness in smart cities. The empirical analyses further suggest that rather than merely striving for integrating smart city technologies into advancing PSS, the way that innovative PSS are integrated into the planning framework (i.e., how well PSS can satisfy the needs of planning tasks and users by considering context-specificities) is of great significance in promoting PSS's actual usefulness.}
}
@article{AMARATUNGA2020100027,
title = {Uses and opportunities for machine learning in hypertension research},
journal = {International Journal of Cardiology Hypertension},
volume = {5},
pages = {100027},
year = {2020},
issn = {2590-0862},
doi = {https://doi.org/10.1016/j.ijchy.2020.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2590086220300045},
author = {Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis},
keywords = {Machine learning, Deep neural networks, Hypertension, Disease management, Personalized disease network},
abstract = {Background
Artificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.
Methods and results
This review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%–57% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.
Conclusion
These examples describe the use of artificial intelligence methods in the field of hypertension.}
}
@article{GACUTAN2022150742,
title = {Continental patterns in marine debris revealed by a decade of citizen science},
journal = {Science of The Total Environment},
volume = {807},
pages = {150742},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.150742},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721058204},
author = {Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark},
keywords = {Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris},
abstract = {Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.}
}
@article{CHE2023513,
title = {Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data},
journal = {Journal of Environmental Sciences},
volume = {125},
pages = {513-523},
year = {2023},
issn = {1001-0742},
doi = {https://doi.org/10.1016/j.jes.2022.02.041},
url = {https://www.sciencedirect.com/science/article/pii/S1001074222001061},
author = {Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau},
keywords = {Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban area},
abstract = {Traditional air quality data have a spatial resolution of 1 km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1 km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.}
}
@article{YAO202154,
title = {Application of artificial intelligence in renal disease},
journal = {Clinical eHealth},
volume = {4},
pages = {54-61},
year = {2021},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2588914121000083},
author = {Lijing Yao and Hengyuan Zhang and Mengqin Zhang and Xing Chen and Jun Zhang and Jiyi Huang and Lu Zhang},
keywords = {Artificial intelligence (AI), Machine learning (ML), Artificial neural network (ANN), Convolution neural network (CNN), Nephrology},
abstract = {Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists’ medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.}
}
@article{LAKIND2019302,
title = {ExpoQual: Evaluating measured and modeled human exposure data},
journal = {Environmental Research},
volume = {171},
pages = {302-312},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119300465},
author = {Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman},
keywords = {ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument, Biomonitoring, Model uncertainty},
abstract = {Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).}
}
@article{SUN2022191,
title = {Advances in optical phenotyping of cereal crops},
journal = {Trends in Plant Science},
volume = {27},
number = {2},
pages = {191-208},
year = {2022},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2021.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S1360138521002028},
author = {Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen},
keywords = {cereal crops, high-throughput phenotyping, optical sensors, traits},
abstract = {Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories – morphological, biochemical, physiological, and performance traits – and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.}
}
@incollection{NETTLETON2014105,
title = {Chapter 7 - Data Sampling and Partitioning},
editor = {David Nettleton},
booktitle = {Commercial Data Mining},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-117},
year = {2014},
isbn = {978-0-12-416602-8},
doi = {https://doi.org/10.1016/B978-0-12-416602-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166028000078},
author = {David Nettleton},
keywords = {sampling, data reduction, partitioning, business criteria, train, test, Big Data},
abstract = {This chapter discusses various types of sampling such as random sampling and sampling based on business criteria (age of customer, time as client, etc.). It also discusses extracting train and test datasets for specific business objectives and considers the issue of Big Data, given that it is currently a hot topic.}
}
@article{YE2019936,
title = {Improved population mapping for China using remotely sensed and points-of-interest data within a random forests model},
journal = {Science of The Total Environment},
volume = {658},
pages = {936-946},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2018.12.276},
url = {https://www.sciencedirect.com/science/article/pii/S0048969718351489},
author = {Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia},
keywords = {Points of interest, Population, Random forests, Nighttime light, China},
abstract = {Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.}
}
@incollection{CINNAMON2020121,
title = {Humanitarian Mapping},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {121-128},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10559-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105591},
author = {Jonathan Cinnamon},
keywords = {Big data, Cartography, Crisis, Crowdsourcing, Disaster, Emergency, Geospatial web, Relief, Spatial data, Web mapping},
abstract = {Humanitarian mapping refers to the production of spatial data and cartographic products to improve situational awareness and decision-making around humanitarian issues from acute events such as natural disasters and public health emergencies to longer term events such as refugee crises and political unrest. Mapping is a key part of the broader area of humanitarian information management, which has traditionally been undertaken by governments and international humanitarian organizations. As a core aspect of the field of digital humanitarianism, mapping activities are now widely undertaken by smaller organizations and networks of volunteers who produce spatial data and maps on the ground and remotely via the use of Web mapping and mobile phone technologies. Big data based on location and behavioral attributes produced online and through interaction with digital systems and networks can also be exploited to enhance information environments. Together, these new developments signal new possibilities for improved risk and crisis management, based on up-to-date high resolution spatial and temporal evidence. Research in human geography, geographic information science, and related disciplines focuses on tracing benefits such as increased speed and low costs, as well as the risks of relying on distributed volunteers and new sources of data of questionable accuracy and validity.}
}
@article{MOSAVI2022503,
title = {Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities},
journal = {Procedia Computer Science},
volume = {201},
pages = {503-510},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.065},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004781},
author = {Nasim Sadat Mosavi and Francisco Freitas and Rogério Pires and César Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais},
keywords = {Energy consumption, Prediction, Optimization, Data Mining, Machine Learning, Forecasting, Industry 4.0},
abstract = {The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where “timestamp” was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.}
}
@incollection{LEUNG2021197,
title = {Chapter 13 - A support vector machine–based voice disorders detection using human voice signal},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {197-208},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000140},
author = {Pak Ho Leung and Kwok Tai Chui and Kenneth Lo and Patricia Ordóñez {de Pablos}},
keywords = {Artificial intelligence, big data, human voice, imbalanced classification, machine learning, medical screening, smart city, smart healthcare, support vector machine, voice disorders},
abstract = {Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel /a/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%–19.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel /a/ and serving as big data pool.}
}
@article{MATHERI2022103152,
title = {Sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant},
journal = {Physics and Chemistry of the Earth, Parts A/B/C},
volume = {126},
pages = {103152},
year = {2022},
issn = {1474-7065},
doi = {https://doi.org/10.1016/j.pce.2022.103152},
url = {https://www.sciencedirect.com/science/article/pii/S1474706522000468},
author = {Anthony Njuguna Matheri and Belaid Mohamed and Freeman Ntuli and Esther Nabadda and Jane Catherine Ngila},
keywords = {Circular bioeconomy, Data pipeline, Digital twin, Process design, Sensor, Wastewater treatment},
abstract = {Rapid urbanization, population increase, emerging contaminants and increasing water scarcity have put a major constraint on the wastewater treatment system. Scarcity of water is steering current way of water recycle, and the drive focus towards resource recovery. Zero waste pathway in circular bioeconomy can bring transformation of wastewater commercialization by adding value with resource recovery. The complex biological reactions, unforeseen microbial behaviours, lack of reliable on-line instrumentation, complex modelling, lack of visualize techniques, low-quality industrial measurements and highly time-varying intensive data-driven operations call for the intelligence techniques and operations. The study is a review of sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant. Water surveillance and monitoring, circular economy and sustainability, automation pyramid, digital transformation, artificial intelligence, data pipeline, digital twin, data mining, and data-driven visualization, cyber-physical systems and water-energy-health management were reviewed. The deployment of the digital systems has evidently proven to bridges the gap between the data-driven soft sensor, operation and control systems in WWTP. Accurate prediction of the WWTP variables can support process design and control, reduce operation cost, improve system reliability, predictive maintenance and troubleshooting, increase water quality, increase stakeholder's engagement and endorse optimization of the plant performance. This procures the best compliance with international standards and diversification. The inclusion of life cycle environmental or cost management technologies in optimization models is an interesting pathway towards sustainable water treatment in-line with sustainable development goals, circular bioeconomy and industry 4.0.}
}
@article{SHARMA2022101624,
title = {Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy},
journal = {Government Information Quarterly},
volume = {39},
number = {4},
pages = {101624},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000605},
author = {Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar},
keywords = {Artificial intelligence, Implementing challenges, Public manufacturing sector, AI enabled systems, Emerging economies},
abstract = {The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.}
}
@article{PRIYADARSHINI2015371,
title = {Semantic Retrieval of Relevant Sources for Large Scale Virtual Documents},
journal = {Procedia Computer Science},
volume = {54},
pages = {371-379},
year = {2015},
note = {Eleventh International Conference on Communication Networks, ICCN 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Data Mining and Warehousing, ICDMW 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Image and Signal Processing, ICISP 2015, August 21-23, 2015, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915013678},
author = {R. Priyadarshini and Latha Tamilselvan and T. Khuthbudin and S. Saravanan and S. Satish},
keywords = {Virtual documents (VD), Source document, Hadoop file System (HDFS), DW Ranking algorithm, Top  algorithm.},
abstract = {The term big data has come into use in recent years. It is used to refer to the ever-increasing amount of data that organizations are storing, processing and analyzing. An Interesting fact with bigdata is that it differ in Volume, Variety, Velocity characteristics which makes it difficult to process using the conventional Database Management System. Hence there is a need of schema less Management Systems even this will never be complete solution to bigdata analysis since the processing has no focus on the semantic information as they consider only the structural information. Content Management System like Wikipedia stores and links huge amount of documents and files. There is lack of semantic linking and analysis in such systems even though this kind of CMS uses clusters and distributed framework for storing big data. The retrieved references for a particular article are random and enormous. In order to reduce the number of references for a selected content there is a need for semantic matching. In this paper we propose framework which make use of the distributed parallel processing capability of Hadoop Distributed File System (HDFS) to perform semantic analysis over the volume of documents (bigdata) to find the best matched source document from the collection source documents for the same virtual document.}
}
@article{JIANG2020161,
title = {Clicking position and user posting behavior in online review systems: A data-driven agent-based modeling approach},
journal = {Information Sciences},
volume = {512},
pages = {161-174},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309089},
author = {Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu},
keywords = {Agent-based modeling, Big data, Online review systems, Clicking position, Posting behavior},
abstract = {In online review systems, a participant's level of knowledge impacts his/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.}
}