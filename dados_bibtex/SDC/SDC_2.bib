@article{VYDRA2019101383,
title = {Techno-optimism and policy-pessimism in the public sector big data debate},
journal = {Government Information Quarterly},
volume = {36},
number = {4},
pages = {101383},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302326},
author = {Simon Vydra and Bram Klievink},
keywords = {Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance},
abstract = {Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.}
}
@article{LI2021441,
title = {A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong},
journal = {Environmental Science & Policy},
volume = {124},
pages = {441-450},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121001714},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow},
keywords = {Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement},
abstract = {All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.}
}
@article{BELL2021453,
title = {Exploring future challenges for big data in the humanitarian domain},
journal = {Journal of Business Research},
volume = {131},
pages = {453-468},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306172},
author = {David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan},
keywords = {Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value},
abstract = {This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 “Partnerships for the Goals”. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous ‘exhaust trail’ of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality – that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.}
}
@incollection{GULERIA2022179,
title = {Chapter 15 - Predictions on diabetic patient datasets using big data analytics and machine learning techniques},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {179-199},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00018-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000182},
author = {Pratiyush Guleria},
keywords = {Analytics, Classification, Clustering, Decision, Diabetic, Healthcare},
abstract = {Big data analytics and machine learning are the promising fields of the present time and playing important role in the healthcare sector. Big data analytical techniques help in analyzing a huge volume of data which may be in structured, semistructured, or unstructured form, and extract meaningful information for effective decision-making. Machine learning techniques help in performing predictions with the trained models on the input datasets and perform classification, clustering of data. In this chapter, the author has performed data analysis on diabetic patients dataset categorical in nature using big data analytical techniques, i.e., MapReduce, Apache Pig, Apache Hive, Apache Spark, and their architectures are discussed. Apart from big data analytics, machine learning techniques, i.e., K-Nearest Neighbor, Decision Trees, Bagged Trees, are implemented on the female diabetic patient dataset which is categorical and numerical for performing predictions based on the attributes like Age, Body Mass Index, Glucose, Blood Pressure, etc. The sensitivity achieved by the decision tree is 61.2% which is higher compared to KNN and bagged tree, whereas the Specificity achieved by the KNN is 89.2% which is higher than the other two algorithms.}
}
@article{WANG202116,
title = {A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {16-32},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301990},
author = {Pei Wang and Ming Luo},
keywords = {Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing},
abstract = {Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.}
}
@article{LIU2020123,
title = {Urban big data fusion based on deep learning: An overview},
journal = {Information Fusion},
volume = {53},
pages = {123-133},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519301393},
author = {Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang},
keywords = {Urban computing, Big data, Data fusion, Deep learning},
abstract = {Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.}
}
@article{ELIA2020617,
title = {A multi-dimension framework for value creation through Big Data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {617-632},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120302212},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{MAJOR202056,
title = {Using big data in pediatric oncology: Current applications and future directions},
journal = {Seminars in Oncology},
volume = {47},
number = {1},
pages = {56-64},
year = {2020},
note = {Pediatric Oncology},
issn = {0093-7754},
doi = {https://doi.org/10.1053/j.seminoncol.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0093775420300063},
author = {Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum},
keywords = {Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics},
abstract = {Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.}
}
@article{CARRA2020300,
title = {Data-driven ICU management: Using Big Data and algorithms to improve outcomes},
journal = {Journal of Critical Care},
volume = {60},
pages = {300-304},
year = {2020},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0883944120306791},
author = {Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt},
keywords = {Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit},
abstract = {The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.}
}
@article{TAMYM2021102,
title = {A big data based architecture for collaborative networks: Supply chains mixed-network},
journal = {Computer Communications},
volume = {175},
pages = {102-111},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001924},
author = {Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri}},
keywords = {Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness},
abstract = {Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.}
}
@article{ZHAO2021101196,
title = {Prediction model of ecological environmental water demand based on big data analysis},
journal = {Environmental Technology & Innovation},
volume = {21},
pages = {101196},
year = {2021},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2020.101196},
url = {https://www.sciencedirect.com/science/article/pii/S2352186420314966},
author = {Lihong Zhao},
keywords = {Big data analysis, Ecological environment, Water demand, Prediction},
abstract = {The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.}
}
@article{VALENCIAPARRA2020101180,
title = {Unleashing Constraint Optimisation Problem solving in Big Data environments},
journal = {Journal of Computational Science},
volume = {45},
pages = {101180},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101180},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320304816},
author = {Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López},
keywords = {Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format},
abstract = {The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.}
}
@article{HUANG2021101712,
title = {Analytics of location-based big data for smart cities: Opportunities, challenges, and future directions},
journal = {Computers, Environment and Urban Systems},
volume = {90},
pages = {101712},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101712},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001198},
author = {Haosheng Huang and Xiaobai Angela Yao and Jukka M. Krisp and Bin Jiang},
keywords = {Location-based big data (LocBigData), Smart cities, Data analytics, State-of-the-art review, Research agenda, Geodata},
abstract = {The growing ubiquity of location/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from “what happened” and “why did it happen” to “what's likely to happen in the future” and “what to do next”. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume/velocity/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.}
}
@incollection{BISWAS202263,
title = {Chapter 6 - Big data analytics in precision medicine},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {63-72},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000054},
author = {Saurabh Biswas and Yasha Hasija},
keywords = {Big data, EHR, Healthcare, Omics, Precision medicine},
abstract = {Precision medicine is a medical model that recommends custom-tailored products, techniques, treatments, and decisions for a subgroup of patients having the same biological basis of diseases. Due to the huge size and complexity of omics data and dataset of patient features, they cannot be analyzed directly by doctors. Big data is a term used for complex or large datasets that cannot be accurately processed or stored by traditional management tools. Omics and electronic health record (EHR) data are essential big biomedical data having a strong association with precision medicine. In this chapter, we review the importance of analyzing EHR and omics data in precision medicine. Big data analytics has been applied to healthcare in biomarker discovery and disease subtyping, drug repurposing, and integrating omics data into EHR. This will provide the most appropriate and efficient treatment to every patient on the basis of their subtyping data.}
}
@article{CARPIOPINEDO2020102859,
title = {Consumption and symbolic capital in the metropolitan space: Integrating ‘old’ retail data sources with social big data},
journal = {Cities},
volume = {106},
pages = {102859},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102859},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120312075},
author = {José Carpio-Pinedo and Javier Gutiérrez},
keywords = {Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid},
abstract = {While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.}
}
@article{MIRACOLO2022S206,
title = {POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review},
journal = {Value in Health},
volume = {25},
number = {1, Supplement },
pages = {S206},
year = {2022},
note = {Emerging Frontiers and Opportunities},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2021.11.1002},
url = {https://www.sciencedirect.com/science/article/pii/S1098301521027972},
author = {A Miracolo and M Mills and P Kanavos}
}
@article{SBAI2020938,
title = {A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems},
journal = {Procedia Computer Science},
volume = {176},
pages = {938-947},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319876},
author = {Ines Sbai and Saoussen Krichen},
keywords = {Big data analytic, Decision Support System, DVRP, S-GA, Spark},
abstract = {Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark’s in-memory computing ability (as a master-slave distribution computing) and GA’s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.}
}
@article{SMALEC20215156,
title = {Big Data as a tool helpful in communication management},
journal = {Procedia Computer Science},
volume = {192},
pages = {5156-5165},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.293},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020329},
author = {Agnieszka Smalec},
keywords = {Big Data, marketing communication, management, data processing, collection, communication management},
abstract = {The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.}
}
@article{LI2021241,
title = {AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making},
journal = {Environmental Science & Policy},
volume = {125},
pages = {241-246},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121002471},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui},
abstract = {AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.}
}
@article{KHALAJZADEH2020100964,
title = {An end-to-end model-based approach to support big data analytics development},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100964},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300241},
author = {Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He},
keywords = {Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools},
abstract = {We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.}
}
@article{MISHRA20216864,
title = {An efficient approach for manufacturing process using Big data analytics},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {6864-6866},
year = {2021},
note = {International Conference on Advances in Design, Materials and Manufacturing},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.146},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321037226},
author = {Devendra Kumar Mishra and Arvind Kumar Upadhyay and Sanjiv Sharma},
keywords = {Manufacturing process, Bigdata, Structured data, Unstructured data},
abstract = {Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human’s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.}
}
@article{PAL2020100869,
title = {Big data in biology: The hope and present-day challenges in it},
journal = {Gene Reports},
volume = {21},
pages = {100869},
year = {2020},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2020.100869},
url = {https://www.sciencedirect.com/science/article/pii/S2452014420302831},
author = {Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh},
keywords = {Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning},
abstract = {The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.}
}
@article{CHEN2022157581,
title = {Quantifying on-road vehicle emissions during traffic congestion using updated emission factors of light-duty gasoline vehicles and real-world traffic monitoring big data},
journal = {Science of The Total Environment},
volume = {847},
pages = {157581},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2022.157581},
url = {https://www.sciencedirect.com/science/article/pii/S0048969722046794},
author = {Xue Chen and Linhui Jiang and Yan Xia and Lu Wang and Jianjie Ye and Tangyan Hou and Yibo Zhang and Mengying Li and Zhen Li and Zhe Song and Jiali Li and Yaping Jiang and Pengfei Li and Xiaoye Zhang and Yang Zhang and Daniel Rosenfeld and John H. Seinfeld and Shaocai Yu},
keywords = {On-road vehicle emissions, Traffic congestion, Light-duty gasoline vehicles, Real-world, Big data, Emission factors},
abstract = {Light-duty gasoline vehicles (LDGVs) have made up >90 % of vehicle fleets in China since 2019, moreover, with a high annual growth rate (> 10 %) since 2017. Hence, accurate estimates of air pollutant emissions of these fast-changing LDGVs are vital for air quality management, human healthcare, and ecological protection. However, this issue is poorly quantified due to insufficient reserves of timely updated LDGV emission factors, which are dependent on real-world activity levels. Here we constructed a big dataset of explicit emission profiles (e.g., emission factors and accumulated mileages) for 159,051 LDGVs based on an official I/M database by matching real-time traffic dynamics via real-world traffic monitoring (e.g., traffic volumes and speeds). Consequently, we provide robust evidence that the emission factors of these LDGVs follow a clear heavy-tailed distribution. The top 10 % emitters contributed >60 % to the total fleet emissions, while the bottom 50 % contributed <10 %. Such emission factors were effectively reduced by 75.7–86.2 % as official emission standards upgraded gradually (i.e., from China 2 to China 5) within 13 years from 2004 to 2017. Nevertheless, such achievements would be offset once traffic congestion occurred. In the real world, the typical traffic congestions (i.e., vehicle speed <5 km/h) can lead to emissions 5– 9 times higher than those on non-congested roads (i.e., vehicle speed >50 km/h). These empirical analyses enabled us to propose future traffic scenarios that could harmonize emission standards and traffic congestion. Practical approaches on vehicle emission controls under realistic conditions are proposed, which would provide new insights for future urban vehicle emission management.}
}
@incollection{LV2022247,
title = {Chapter 19 - Privacy security risks of big data processing in healthcare},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {247-263},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00020-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000200},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Cloud services, Healthcare, Privacy measures, Privacy security risk},
abstract = {This chapter aims to discuss healthcare's development in China and the privacy and security risk factors in medical data under big data. First, the development status of China's healthcare sector is analyzed. The questionnaire is made to analyze the privacy and security risk factors of healthcare big data (HBD) and protection measures are proposed according to the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions and medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all increase annually. In 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The questionnaire results reveal that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system are all greater than 0.8 when HBD is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to emphasize data privacy protection and grasp using digital medical data to provide decision support for subsequent medical data analysis.}
}
@article{ZHAO20201624,
title = {Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling},
journal = {Drug Discovery Today},
volume = {25},
number = {9},
pages = {1624-1638},
year = {2020},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620302646},
author = {Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu},
abstract = {Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.}
}
@article{FUGINI2021100192,
title = {A Big Data Analytics Architecture for Smart Cities and Smart Companies},
journal = {Big Data Research},
volume = {24},
pages = {100192},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000095},
author = {Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli},
keywords = {Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities},
abstract = {This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.}
}
@article{BALTI2020101136,
title = {A review of drought monitoring with big data: Issues, methods, challenges and research directions},
journal = {Ecological Informatics},
volume = {60},
pages = {101136},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101136},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300868},
author = {Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing},
abstract = {Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.}
}
@article{LV2020101937,
title = {Achieving secure big data collection based on trust evaluation and true data discovery},
journal = {Computers & Security},
volume = {96},
pages = {101937},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101937},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820302133},
author = {Denglong Lv and Shibing Zhu},
keywords = {Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network},
abstract = {Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.}
}
@article{WEN2021295,
title = {Big data driven Internet of Things for credit evaluation and early warning in finance},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {295-307},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001977},
author = {Chunhui Wen and Jinhai Yang and Liu Gan and Yang Pan},
keywords = {Internet of Things finance, Credit evaluation and early warning, Factor analysis, Particle swarm optimization, Big data driven},
abstract = {The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser–Meyer–Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.}
}
@article{MOHARM2019100945,
title = {State of the art in big data applications in microgrid: A review},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100945},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100945},
url = {https://www.sciencedirect.com/science/article/pii/S147403461830702X},
author = {Karim Moharm},
keywords = {Big data, Microgrid},
abstract = {The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.}
}
@article{PAIGE20211467,
title = {A Versatile Big Data Health System for Australia: Driving Improvements in Cardiovascular Health},
journal = {Heart, Lung and Circulation},
volume = {30},
number = {10},
pages = {1467-1476},
year = {2021},
issn = {1443-9506},
doi = {https://doi.org/10.1016/j.hlc.2021.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S1443950621005175},
author = {Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree},
keywords = {Big data, Datasets, Cardiovascular disease, National platform},
abstract = {Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.}
}
@article{VANAM2021,
title = {Analysis of twitter data through big data based sentiment analysis approaches},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.11.486},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320391501},
author = {Harika Vanam and Jeberson {Retna Raj R}},
keywords = {Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics, Machine learning algorithm},
abstract = {The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis …, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.}
}
@article{SU2020138984,
title = {Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis},
journal = {Science of The Total Environment},
volume = {733},
pages = {138984},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.138984},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720325018},
author = {Yuan Su and Yanni Yu and Ning Zhang},
keywords = {Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis},
abstract = {Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.}
}
@article{SOUIFI2021857,
title = {From Big Data to Smart Data: Application to performance management},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {857-862},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.100},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008491},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Big Data, Smart Data, Performance Management},
abstract = {In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.}
}
@article{SHAHOUD2021100432,
title = {An extended Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualizationz Technologies},
journal = {Internet of Things},
volume = {16},
pages = {100432},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100432},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000767},
author = {Shadi Shahoud and Moritz Winter and Hatem Khalloof and Clemens Duepmeier and Veit Hagenmeyer},
keywords = {Meta learning, Machine learning, Microservice, Web-based applications, Big data},
abstract = {For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.}
}
@article{NEILSON201935,
title = {Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications},
journal = {Big Data Research},
volume = {17},
pages = {35-44},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579617303866},
author = {Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra},
keywords = {Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero},
abstract = {Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.}
}
@article{YADEGARIDEHKORDI2018199,
title = {Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {199-210},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.07.043},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518304141},
author = {Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim},
keywords = {Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS},
abstract = {Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.}
}
@article{SHARMA20215515,
title = {A framework based on BWM for big data analytics (BDA) barriers in manufacturing supply chains},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {5515-5519},
year = {2021},
note = {3rd International e-Conference on Frontiers in Mechanical Engineering and nanoTechnology},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.03.374},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321024263},
author = {Vikrant Sharma and Atul Kumar and Mukesh Kumar},
keywords = {Big data analytics, Barriers, Manufacturing supply chains, Best worst method (BWM)},
abstract = {Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.}
}
@article{TSAI2019306,
title = {Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {306-310},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300347},
author = {Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez},
abstract = {In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.}
}
@incollection{FENG2021145,
title = {Chapter 10 - Spatiotemporal Big Data-Driven Vessel Traffic Risk Estimation for Promoting Maritime Healthcare: Lessons Learnt from Another Domain than Healthcare},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {145-160},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000061},
author = {Zikun Feng and Yan Li and Zhao Liu and Ryan Wen Liu},
keywords = {Data mining, maritime healthcare, maritime risk estimation, automatic identification system (AIS), maritime safety, artificial intelligence},
abstract = {With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data–driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.}
}
@article{ATITALLAH2020100303,
title = {Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions},
journal = {Computer Science Review},
volume = {38},
pages = {100303},
year = {2020},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100303},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304032},
author = {Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala},
keywords = {Internet of Things, Deep Learning, Smart city, Big data analytics, Review},
abstract = {The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.}
}
@incollection{WANG202135,
title = {Chapter 2 - Big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {35-49},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000179},
author = {Lidong Wang and Cheryl Alexander},
keywords = {Big data, Precision healthcare, Personalized healthcare, Big Data analytics, Telepsychiatry, Deep learning},
abstract = {Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.}
}
@article{YIN2021102514,
title = {Integrating remote sensing and geospatial big data for urban land use mapping: A review},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102514},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102514},
url = {https://www.sciencedirect.com/science/article/pii/S030324342100221X},
author = {Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu},
keywords = {Integration methods, Urban functional zone classification, Urban management, Land use},
abstract = {Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.}
}
@article{SHAMIM2020120315,
title = {Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120315},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120315},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311410},
author = {Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia},
keywords = {Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets},
abstract = {This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.}
}
@article{SALEM2022e55,
title = {Unaccounted Confounders Limit the Ability to Draw Conclusions From Big Data Analysis Comparing Radiotherapy Fractionation Regimens in NSCLC},
journal = {Journal of Thoracic Oncology},
volume = {17},
number = {6},
pages = {e55-e56},
year = {2022},
issn = {1556-0864},
doi = {https://doi.org/10.1016/j.jtho.2022.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556086422001381},
author = {Ahmed Salem and Kevin Franks and Alastair Greystoke and Gerard G. Hanna and Stephen Harrow and Matthew Hatton and Crispin Hiley and Fiona McDonald and Corinne Faivre-Finn}
}
@article{BAG2021120420,
title = {Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120420},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120420},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312464},
author = {Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi},
keywords = {Big data, Artificial intelligence, Industry 4.0, Circular economy, Sustainable manufacturing},
abstract = {ABSTRACT
The significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.}
}
@article{ROSADO2021102155,
title = {MARISMA-BiDa pattern: Integrated risk analysis for big data},
journal = {Computers & Security},
volume = {102},
pages = {102155},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820304284},
author = {David G. Rosado and Julio Moreno and Luis E. Sánchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fernández-Medina},
keywords = {Big data, Risk assessment, Risk analysis, Information security, Security standards},
abstract = {Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.}
}
@article{NARAYANAN20223121,
title = {A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part B},
pages = {3121-3135},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303700},
author = {Uma Narayanan and Varghese Paul and Shelbi Joseph},
keywords = {Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3},
abstract = {With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.}
}
@article{LI202149,
title = {The critical need to establish standards for data quality in intelligent medicine},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {49-50},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000073},
author = {Ruiyang Li and Yahan Yang and Haotian Lin},
keywords = {Artificial intelligence, Intelligent medicine, Big data, Data collection, Data storage, Data management},
abstract = {Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the “Belt and Road” International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.}
}
@article{SHAMIM2019103135,
title = {Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view},
journal = {Information & Management},
volume = {56},
number = {6},
pages = {103135},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618302854},
author = {Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan},
keywords = {Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China},
abstract = {This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.}
}
@article{SILVA2020111,
title = {Ion beam analysis and big data: How data science can support next-generation instrumentation},
journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
volume = {478},
pages = {111-115},
year = {2020},
issn = {0168-583X},
doi = {https://doi.org/10.1016/j.nimb.2020.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0168583X2030272X},
author = {Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães},
keywords = {Ion beam analysis, Big data, Data quality assurance, Artificial intelligence},
abstract = {With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.}
}
@article{LIN201949,
title = {Strategic orientations, developmental culture, and big data capability},
journal = {Journal of Business Research},
volume = {105},
pages = {49-60},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304333},
author = {Canchu Lin and Anand Kunnathur},
keywords = {Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture},
abstract = {Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.}
}
@article{MA2021107580,
title = {A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107580},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107580},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221004848},
author = {Qiuping Ma and Hongyan Li and Anders Thorstenson},
keywords = {Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network},
abstract = {Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.}
}
@article{GARCIAGIL2019135,
title = {Enabling Smart Data: Noise filtering in Big Data classification},
journal = {Information Sciences},
volume = {479},
pages = {135-152},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309460},
author = {Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera},
keywords = {Big Data, Smart Data, Classification, Class noise, Label noise.},
abstract = {In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.}
}
@article{COZZINI2022133422,
title = {Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family},
journal = {Chemosphere},
volume = {292},
pages = {133422},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2021.133422},
url = {https://www.sciencedirect.com/science/article/pii/S0045653521038960},
author = {Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani},
keywords = {Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology},
abstract = {According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.}
}
@article{SINGH2021157,
title = {Big data, industry 4.0 and cyber-physical systems integration: A smart industry context},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {157-162},
year = {2021},
note = {2nd International Conference on Manufacturing Material Science and Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.170},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320352639},
author = {Harpreet Singh},
keywords = {Agile management, Heterogeneity, Internet-of-things, Smart factory, Smart manufacturing},
abstract = {The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.}
}
@article{BAZZAZABKENAR2021101517,
title = {Big data analytics meets social media: A systematic review of techniques, open issues, and future directions},
journal = {Telematics and Informatics},
volume = {57},
pages = {101517},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101517},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301763},
author = {Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii},
keywords = {Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review},
abstract = {Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.}
}
@article{ARBEX2020102671,
title = {Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data},
journal = {Journal of Transport Geography},
volume = {85},
pages = {102671},
year = {2020},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2020.102671},
url = {https://www.sciencedirect.com/science/article/pii/S0966692319300092},
author = {Renato Arbex and Claudio B. Cunha},
keywords = {Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability},
abstract = {Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.}
}
@article{BAIG2019102095,
title = {Big data adoption: State of the art and research challenges},
journal = {Information Processing & Management},
volume = {56},
number = {6},
pages = {102095},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102095},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319301773},
author = {Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi},
keywords = {Big data adoption, Technology–Organization–Environment, Diffusion of Innovations},
abstract = {Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.}
}
@article{ALIC2019243,
title = {BIGSEA: A Big Data analytics platform for public transportation information},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {243-269},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304448},
author = {Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira},
abstract = {Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).}
}
@article{SUNDARAKANI2021102452,
title = {Big data driven supply chain design and applications for blockchain: An action research using case study approach},
journal = {Omega},
volume = {102},
pages = {102452},
year = {2021},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2021.102452},
url = {https://www.sciencedirect.com/science/article/pii/S030504832100061X},
author = {Balan Sundarakani and Aneesh Ajaykumar and Angappa Gunasekaran},
keywords = {Big data architecture, Action research, Case study research, Blockchain adoption, Supply chain management},
abstract = {Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.}
}
@article{ELIA2020508,
title = {A multi-dimension framework for value creation through big data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {508-522},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118307600},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{VIEIRA2020101985,
title = {On the use of simulation as a Big Data semantic validator for supply chain management},
journal = {Simulation Modelling Practice and Theory},
volume = {98},
pages = {101985},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.101985},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301182},
author = {António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira},
keywords = {Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0},
abstract = {Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.}
}
@article{JIN202024,
title = {Big Data in food safety- A review},
journal = {Current Opinion in Food Science},
volume = {36},
pages = {24-32},
year = {2020},
note = {Food Safety},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214799320301260},
author = {Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin},
abstract = {The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.}
}
@article{LOZADA2019e02541,
title = {Big data analytics capability and co-innovation: An empirical study},
journal = {Heliyon},
volume = {5},
number = {10},
pages = {e02541},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02541},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019362012},
author = {Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry},
keywords = {Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation},
abstract = {There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.}
}
@article{GOKALP2022103585,
title = {A process assessment model for big data analytics},
journal = {Computer Standards & Interfaces},
volume = {80},
pages = {103585},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103585},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000805},
author = {Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Selin Gökalp and Altan Koçyiğit and P. Erhan Eren},
keywords = {Big data, Data analytics, Software development, Software process improvement, Software process assessment},
abstract = {Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.}
}
@article{KEZUNOVIC2020106788,
title = {Big data analytics for future electricity grids},
journal = {Electric Power Systems Research},
volume = {189},
pages = {106788},
year = {2020},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2020.106788},
url = {https://www.sciencedirect.com/science/article/pii/S0378779620305915},
author = {Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa},
keywords = {Electricity grids, Analytics, Big data, Decision-making},
abstract = {This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.}
}
@incollection{KOLTAY202249,
title = {Chapter 3 - Data quality, the essential “ingredient”},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {49-75},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000047},
author = {Tibor Koltay},
keywords = {Research data quality, Stakeholders, Trust, Intrinsic and extrinsic data quality, Semiotic representation, Time-related dimensions, Data retrievability, Data reuse, Data governance},
abstract = {This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality’s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.}
}
@article{CUI2020101861,
title = {Manufacturing big data ecosystem: A systematic literature review},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {62},
pages = {101861},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101861},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300559},
author = {Yesheng Cui and Sami Kara and Ka C. Chan},
keywords = {Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL},
abstract = {Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.}
}
@article{LOPEZMARTINEZ2021263,
title = {A big data-centric architecture metamodel for Industry 4.0},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {263-284},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002156},
author = {Patricia {López Martínez} and Ricardo Dintén and José María Drake and Marta Zorrilla},
keywords = {Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel},
abstract = {The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.}
}
@article{NORTHCOTT202096,
title = {Big data and prediction: Four case studies},
journal = {Studies in History and Philosophy of Science Part A},
volume = {81},
pages = {96-104},
year = {2020},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368119300652},
author = {Robert Northcott},
keywords = {Big data, Prediction, Case studies, Explanation, Elections, Weather},
abstract = {Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.}
}
@article{TRIPATHI20201245,
title = {Big-data driven approaches in materials science: A survey},
journal = {Materials Today: Proceedings},
volume = {26},
pages = {1245-1249},
year = {2020},
note = {10th International Conference of Materials Processing and Characterization},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.02.249},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320310026},
author = {Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi},
keywords = {Material science, Big data, Machine learning, Data analytics, Predictive Algorithms},
abstract = {The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.}
}
@incollection{SEBASTIANCOLEMAN20223,
title = {Chapter 1 - The Importance of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {3-30},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000018},
author = {Laura Sebastian-Coleman},
keywords = {Quality management, process management, data quality management, data governance, costs of poor quality, big data, digital transformation, data privacy, data monetization},
abstract = {This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.}
}
@article{FAHEEM2021100236,
title = {CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0},
journal = {Journal of Industrial Information Integration},
volume = {24},
pages = {100236},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100236},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000364},
author = {Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor},
keywords = {Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network},
abstract = {Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.}
}
@article{KASTOUNI20222758,
title = {Big data analytics in telecommunications: Governance, architecture and use cases},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part A},
pages = {2758-2770},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S131915782030553X},
author = {Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}},
keywords = {Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases},
abstract = {With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.}
}
@article{BERGIER2021102864,
title = {Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?},
journal = {Autoimmunity Reviews},
volume = {20},
number = {8},
pages = {102864},
year = {2021},
issn = {1568-9972},
doi = {https://doi.org/10.1016/j.autrev.2021.102864},
url = {https://www.sciencedirect.com/science/article/pii/S1568997221001361},
author = {Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud},
keywords = {Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine},
abstract = {The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.}
}
@article{MIKALEF2020103361,
title = {The role of information governance in big data analytics driven innovation},
journal = {Information & Management},
volume = {57},
number = {7},
pages = {103361},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103361},
url = {https://www.sciencedirect.com/science/article/pii/S0378720620302998},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS},
abstract = {The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.}
}
@article{ABOELMAGED2020102234,
title = {Influencing models and determinants in big data analytics research: A bibliometric analysis},
journal = {Information Processing & Management},
volume = {57},
number = {4},
pages = {102234},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102234},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319313366},
author = {Mohamed Aboelmaged and Samar Mouakket},
keywords = {Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks},
abstract = {Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.}
}
@incollection{MORRA2021841,
title = {Fresh Outlook on Numerical Methods for Geodynamics. Part 2: Big Data, HPC, Education},
editor = {David Alderton and Scott A. Elias},
booktitle = {Encyclopedia of Geology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {841-855},
year = {2021},
isbn = {978-0-08-102909-1},
doi = {https://doi.org/10.1016/B978-0-08-102908-4.00111-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029084001119},
author = {Gabriele Morra and David A. Yuen and Henry M. Tufo and Matthew G. Knepley},
keywords = {Big Data, High performance computing, Education, Modeling, Geodynamics},
abstract = {Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.}
}
@incollection{MUHAMADIBRAHIM202233,
title = {Chapter 4 - Towards big data framework in government public open data (GPOD) for health},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {33-45},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00024-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000248},
author = {Najhan {Muhamad Ibrahim} and Nur Hidayah Ilham {Ahmad Azri} and Norbik Bashah Idris},
keywords = {Big data, Big data framework, Government public open data, Open data, Public data for health},
abstract = {Today, data is one of the most valuable assets on the planet. As a valuable resource, data may be used to develop a wide range of data applications, all of which are driven by creativity and innovation. In order to obtain information and provide services, data is also a critical component. In the recent years, big data has become a popular topic in global discussion. Big data is a new technology and knowledge generation phenomenon, that record, capture, and execute a significant amount of data for the usage in a variety of domains such as research, education, business, investing, health, and so on. The proliferation of data inspired by new methods of data gatherings such as via social media, wireless sensors, and data from government agencies which makes big data management an ultimate challenge. This study includes a thorough evaluation of existing theories and practical approach to address the public sector open data issues for determining the determinants of government public open data (GPOD) development of big data. To investigate the revolution of GPOD for health, the framework was dominantly used over architecture, infrastructures, followed by theoretical and conceptual framework, according to the review. This study revealed that most of the existing frameworks still lack consideration of the requirement for public open data in health. There is less number of existing research works that have sophisticated big data frameworks in GPOD for health. There also is still a lack of investment and adoption of big data in the public sector. The findings of this chapter will help academicians to empirically study the revealed requirement and provide decision-makers a better knowledge of how to leverage GPOD adoption in health by taking appropriate actions.}
}
@article{ARDAGNA2021107215,
title = {Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107215},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107215},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002081},
author = {Claudio A. Ardagna and Valerio Bellandi and Ernesto Damiani and Michele Bezzi and Cedric Hebert},
keywords = {Artificial intelligence, Big Data Analytics, Machine learning, Security and privacy},
abstract = {We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain’s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.}
}
@article{LI2020100608,
title = {Network analysis of big data research in tourism},
journal = {Tourism Management Perspectives},
volume = {33},
pages = {100608},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2019.100608},
url = {https://www.sciencedirect.com/science/article/pii/S2211973619301400},
author = {Xin Li and Rob Law},
keywords = {Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends},
abstract = {This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.}
}
@article{WANG2020120175,
title = {Tension in big data using machine learning: Analysis and applications},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120175},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120175},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520310015},
author = {Huamao Wang and Yumei Yao and Said Salhi},
keywords = {Big data, Machine learning, Data size, Prediction accuracy, Social media},
abstract = {The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.}
}
@article{RAUT2021102170,
title = {Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {145},
pages = {102170},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.102170},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520308139},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Manoj Dora and Mengqi Liu},
keywords = {Big data analytics, Manufacturing firms, Supply chain and logistics management, LARG, Business performance and innovation, Sustainability},
abstract = {The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of ‘Big Data Analytics’ (BDA) as a mediator between ‘sustainable supply chain business performance’ and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.}
}
@article{YILDIRIM2021114840,
title = {Big data analytics for default prediction using graph theory},
journal = {Expert Systems with Applications},
volume = {176},
pages = {114840},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114840},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002815},
author = {Mustafa Yıldırım and Feyza Yıldırım Okay and Suat Özdemir},
keywords = {Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value},
abstract = {With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.}
}
@article{RAKIPI2021100357,
title = {Correlates of the internal audit function’s use of data analytics in the big data era: Global evidence},
journal = {Journal of International Accounting, Auditing and Taxation},
volume = {42},
pages = {100357},
year = {2021},
issn = {1061-9518},
doi = {https://doi.org/10.1016/j.intaccaudtax.2020.100357},
url = {https://www.sciencedirect.com/science/article/pii/S1061951820300586},
author = {Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza},
keywords = {Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit},
abstract = {In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs’ ability to extract value from big data, helping IAFs to enhance their activities’ efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs’ DA usage. From the literature, we identify five main variables expected to be associated with IAFs’ DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs’ ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs’ soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs’ involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.}
}
@article{GAO2020668,
title = {Big data analytics for smart factories of the future},
journal = {CIRP Annals},
volume = {69},
number = {2},
pages = {668-692},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301359},
author = {Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti},
keywords = {Digital manufacturing system, Information, Learning},
abstract = {Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.}
}
@article{SHAH2020106970,
title = {Feature engineering in big data analytics for IoT-enabled smart manufacturing – Comparison between deep learning and statistical learning},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106970},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106970},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420300363},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning},
abstract = {As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.}
}
@article{SELLAMI2020102732,
title = {On the use of big data frameworks for big service composition},
journal = {Journal of Network and Computer Applications},
volume = {166},
pages = {102732},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102732},
url = {https://www.sciencedirect.com/science/article/pii/S108480452030206X},
author = {Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid},
keywords = {Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark},
abstract = {Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.}
}
@article{ZHANG2022103231,
title = {Orchestrating big data analytics capability for sustainability: A study of air pollution management in China},
journal = {Information & Management},
volume = {59},
number = {5},
pages = {103231},
year = {2022},
note = {Big Data Analytics for Sustainability},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103231},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619302010},
author = {Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu},
keywords = {Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration},
abstract = {Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.}
}
@article{JIA202255,
title = {Data Quality and Usability Assessment Methodology for Prognostics and Health Management: A Systematic Framework},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {55-60},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.183},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322013969},
author = {Xiaodong Jia and Da-Yan Ji and Takanobu Minami and Jay Lee},
keywords = {Intelligence Maintenance, Data Quality, Artificial Intelligence, Industry 4.0},
abstract = {Collecting useful and informative data play an essential role in ensuring the performance of data-driven solutions for intelligent maintenance. However, there is still a lack of methodology to systematically assess the data usefulness (or data suitability) for modeling. This lack of data suitability assessment becomes a more pressing issue in the big data environment where a large volume of machine data is generated at a high velocity. Therefore, there are imperative needs for standardized procedures and systematic solutions that can scan through a large amount of data to quantify the data suitability and locate the useful datasets for model development. To fill in this gap, this paper proposes a novel methodology to evaluate the data suitability for PHM modeling from the aspects of detectability assessment, diagnosability assessment, and prognosability assessment. In the discussion, new assessment procedures and algorithms are proposed by using a series of similarity metrics between data vectors or data distribution. Also, the proposed methods provide both visualization tools and quantitative metrics to assess the data suitability. The effectiveness of the methodology is demonstrated by using real-world examples about the ball screw degradation and boring tool degradation. The results successfully demonstrate the effectiveness and practicality of the proposed methodology and analytics.}
}
@article{LIU2020123646,
title = {Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123646},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123646},
url = {https://www.sciencedirect.com/science/article/pii/S095965262033691X},
author = {Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He},
keywords = {Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination},
abstract = {Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.}
}
@article{REN2021128154,
title = {Research on big data analysis model of multi energy power generation considering pollutant emission—Empirical analysis from Shanxi Province},
journal = {Journal of Cleaner Production},
volume = {316},
pages = {128154},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128154},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621023726},
author = {Dongfang Ren and Xiaopeng Guo and Cunbin Li},
keywords = {Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power},
abstract = {With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.}
}
@incollection{CHANG202221,
title = {Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning},
editor = {Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas G. Green and Gary Wills},
booktitle = {Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19},
publisher = {Academic Press},
pages = {21-66},
year = {2022},
isbn = {978-0-323-90054-6},
doi = {https://doi.org/10.1016/B978-0-323-90054-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900546000076},
author = {Victor Chang and Mohamed Aleem Ali and Alamgir Hossain},
keywords = {COVID-19 review, AU methods for COVID-19, Machine learning for COVID-19},
abstract = {This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.}
}
@article{RUSSELL2022108709,
title = {Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring},
journal = {Mechanical Systems and Signal Processing},
volume = {168},
pages = {108709},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021010293},
author = {Matthew Russell and Peng Wang},
keywords = {Physics-informed deep learning, Prognostics and health management, Data compression, Big data},
abstract = {The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.}
}
@article{SEDLMAYR202081,
title = {Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen für die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen},
journal = {Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen},
volume = {158-159},
pages = {81-91},
year = {2020},
issn = {1865-9217},
doi = {https://doi.org/10.1016/j.zefq.2020.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1865921720301744},
author = {Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr},
keywords = {Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases},
abstract = {Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.}
}
@article{BRESCIANI2021102347,
title = {Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda},
journal = {International Journal of Information Management},
volume = {60},
pages = {102347},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000402},
author = {Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris},
keywords = {Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review},
abstract = {This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.}
}
@article{YADEGARIDEHKORDI2020100921,
title = {The impact of big data on firm performance in hotel industry},
journal = {Electronic Commerce Research and Applications},
volume = {40},
pages = {100921},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2019.100921},
url = {https://www.sciencedirect.com/science/article/pii/S1567422319300985},
author = {Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}},
keywords = {Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling},
abstract = {Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.}
}
@article{JI2021108267,
title = {Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach},
journal = {Building and Environment},
volume = {205},
pages = {108267},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108267},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321006673},
author = {Sukwon Ji and Bumho Lee and Mun Yong Yi},
keywords = {Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network},
abstract = {Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72–4.6 and the coefficients of determination of 0.932–0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.}
}
@article{MCNUTT2019326,
title = {Use of Big Data for Quality Assurance in Radiation Therapy},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {326-332},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300384},
author = {Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright},
abstract = {The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.}
}