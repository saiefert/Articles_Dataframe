@article{GARCIABERNARDO2018164,
title = {The effects of data quality on the analysis of corporate board interlock networks},
journal = {Information Systems},
volume = {78},
pages = {164-172},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917302272},
author = {Javier Garcia-Bernardo and Frank W. Takes},
abstract = {Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API’s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms – shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.}
}
@article{DO2020100018,
title = {Data quality analysis of interregional travel demand: Extracting travel patterns using matrix decomposition},
journal = {Asian Transport Studies},
volume = {6},
pages = {100018},
year = {2020},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2020.100018},
url = {https://www.sciencedirect.com/science/article/pii/S2185556020300183},
author = {Canh Xuan Do and Makoto Tsukai and Akimasa Fujiwara},
keywords = {Interregional travel survey, Web-based survey, Mobile phone data, Data quality, Nonnegative matrix factorization},
abstract = {The Interregional Travel Survey in Japan (formerly the Net Passenger Transportation Survey [NPTS]) still has some limitations. New data sources have recently emerged, e.g., massive data from web-based surveys (WEB) or collecting passive mobile phone data (MOBI). Using or not using these data sources have been questioned for data integration or model estimation and validation. Therefore, as an initial step, the data quality of new data sources was evaluated to identify the potential for data integration with NPTS or new data collection methods to replace NPTS. This study focused on finding out the similarities in travel patterns extracted from these data sources using a nonnegative matrix factorization method. This study found that origin–destination pairs in the MOBI travel patterns were significantly different from those of NPTS and WEB, while there were some similarities between NPTS and WEB. However, some issues have been remaining and should be resolved in the future.}
}
@incollection{BERMAN2013183,
title = {Chapter 13 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {183-199},
year = {2013},
isbn = {978-0-12-404576-7},
doi = {https://doi.org/10.1016/B978-0-12-404576-7.00013-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045767000137},
author = {Jules J. Berman},
keywords = {Feist Publishing, Inc. v. Rural Telephone Service Co., Data Quality Act, Freedom of Information Act, limited data use agreements, tort, patents, intellectual property, informed consent, data ownership, copyright, infringement, fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of Big Data as a resource suitable for its intended purposes. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four problems never seem to go away.}
}
@incollection{HUGHES2016293,
title = {Chapter 13 - Surface Solutions Using Data Virtualization and Big Data},
editor = {Ralph Hughes},
booktitle = {Agile Data Warehousing for the Enterprise},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {293-327},
year = {2016},
isbn = {978-0-12-396464-9},
doi = {https://doi.org/10.1016/B978-0-12-396464-9.00013-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964649000138},
author = {Ralph Hughes},
keywords = {Agile enterprise data warehousing, surface solutions, backfilling the architecture, shadow IT, data virtualization, big data, Hadoop, HDFS, Map/Reduce, Hive},
abstract = {Without investing in exotic data modeling techniques, EDW teams can achieve fast delivery using “surface solutions.” Surface solutions allow developers to first solve business problems with data taken from landing areas and then steadily “backfill” the DW/BI reference architecture to provide progressively more complete and robust solutions. Teams can create surface solutions by leveraging shadow IT, using data virtualization, and tapping a big data platform. When leveraging shadow IT, the EDW team delivers progressively richer data sets to departmental staff members, who build their own temporary BI solutions using that information. The data virtualization strategy relies on a “superoptimizer” that can create views across databases and data types, even including semistructured data as needed. The big data strategy employs a new category of products such as Hadoop’s HDFS, MapReduce, and Hive to provide access to new data, whether it be very large, poorly structured, and/or just unfamiliar to IT and the business users.}
}
@article{FUCHS2014198,
title = {Big data analytics for knowledge generation in tourism destinations – A case from Sweden},
journal = {Journal of Destination Marketing & Management},
volume = {3},
number = {4},
pages = {198-209},
year = {2014},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X14000353},
author = {Matthias Fuchs and Wolfram Höpken and Maria Lexhagen},
keywords = {Big data analytics, Tourism destination, Destination management information system, Business intelligence, Data mining, Online Analytical Processing (OLAP)},
abstract = {This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, Åre. By applying a Business Intelligence approach, the Destination Management Information System Åre (DMIS-Åre) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington’s (2007) concept of the ‘Learning Tourism Destination’ and the ‘Knowledge Destination Framework’ introduced by Höpken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-Åre can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like “Web-Navigation”, “Booking” and “Feedback”. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists’ on-site behaviour at tourism destinations.}
}
@article{BJORNSDOTTIR20181195,
title = {Exhibiting caution with use of big data: The case of amphetamine in Iceland's prescription registry},
journal = {Research in Social and Administrative Pharmacy},
volume = {14},
number = {12},
pages = {1195-1202},
year = {2018},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S155174111830127X},
author = {Ingunn Björnsdottir and Guri Birgitte Verne},
abstract = {Background
Data from large electronic databases are increasingly used in epidemiological research, but golden standards for database validation remain elusive. The Prescription Registry (IPR) and the National Health Service (NHS) databases in Iceland have not undergone formal validation, and gross errors have repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new amphetamine tablets entered the Icelandic market, but were withdrawn half a year later due to being substandard. Return of unused stocks provided knowledge of the exact number of tablets used and hence a case where quality of the data could be assessed.
Objective
A case study of the quality of statistics in a national database on pharmaceuticals.
Methods
Data on the sales of the substandard amphetamine were obtained from the Prescription Registry and the pharmaceuticals statistics database. Upon the revelation of discrepancies, explanations were sought from the respective institutions, the producer, and dose dispensing companies.
Results
The substandard amphetamine was available from 1.9.2015 until 15.3.2016. According to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially stated 82860 tablets to have been sold, correcting to 74796 upon being notified about errors. The producer stated 72811 tablets to have been sold, and agreed with the dose dispensing companies on sales to those. The producer’s numbers were confirmed by the Medicines Agency.
Conclusion
Over-registration in the IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This case provided a unique opportunity for external validation of sales data for pharmaceuticals in Iceland, revealing enormous quality problems. The case has implications regarding database integrity beyond Iceland.}
}
@article{PONTORIERO2021106239,
title = {Automated Data Quality Control in FDOPA brain PET Imaging using Deep Learning},
journal = {Computer Methods and Programs in Biomedicine},
volume = {208},
pages = {106239},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106239},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003138},
author = {Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese},
keywords = {FDOPA, PET, quality control, QC, convolutional neural networks},
abstract = {ABSTRACT
Introduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 ± 0.01, accuracy for SNR: 0.69 ± 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.}
}
@article{ROMERO2015336,
title = {Tuning small analytics on Big Data: Data partitioning and secondary indexes in the Hadoop ecosystem},
journal = {Information Systems},
volume = {54},
pages = {336-356},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001458},
author = {Oscar Romero and Victor Herrero and Alberto Abelló and Jaume Ferrarons},
keywords = {Big Data, OLAP, Multidimensional model, Indexes, Partitioning, Cost estimation},
abstract = {In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I/O).}
}
@article{MAIER201797,
title = {Big data in large-scale systemic mouse phenotyping},
journal = {Current Opinion in Systems Biology},
volume = {4},
pages = {97-104},
year = {2017},
note = {Big data acquisition and analysis • Pharmacology and drug discovery},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300525},
author = {Holger Maier and Stefanie Leuchtenberger and Helmut Fuchs and Valerie Gailus-Durner and Martin {Hrabe de Angelis}},
abstract = {Systemic phenotyping of mutant mice has been established at large scale in the last decade as a new tool to uncover the relations between genotype, phenotype and environment. Recent advances in that field led to the generation of a valuable open access data resource that can be used to better understanding the underlying causes for human diseases. From an ethical perspective, systemic phenotyping significantly contributes to the reduction of experimental animals and the refinement of animal experiments by enforcing standardisation efforts. There are particular logistical, experimental and analytical challenges of systemic large-scale mouse phenotyping. On all levels, IT solutions are critical to implement and efficiently support breeding, phenotyping and data analysis processes that lead to the generation of high-quality systemic phenotyping data accessible for the scientific community.}
}
@article{HUARD201518,
title = {The data quality paradox},
journal = {Network Security},
volume = {2015},
number = {6},
pages = {18-20},
year = {2015},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(15)30051-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485815300519},
author = {Boris Huard},
abstract = {After its people, data is arguably an organisation's most valuable asset. According to recent figures by the Networked Systems and Services department at SINTEF, some 90% of all the data in the world has been created in the past two years. That shouldn't be too much of a surprise to us given the data-driven world we now live in; but what is promising is that companies are increasingly switching on to its strategic and commercial value. The challenge is how do we extract value from this data in a way that empowers people and organisations alike?}
}
@article{DANIEL2019104804,
title = {Initializing a hospital-wide data quality program. The AP-HP experience.},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104804},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306242},
author = {Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon},
keywords = {Data accuracy, Data quality, Electronic health records, Data warehousing, Observational Studies as Topic},
abstract = {Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.}
}
@article{ALVAREZSANCHEZ2019104824,
title = {TAQIH, a tool for tabular data quality assessment and improvement in the context of health data},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104824},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718304188},
author = {Roberto {Álvarez Sánchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin},
keywords = {Data quality, Exploratory data analysis, Data pre-processing},
abstract = {Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.}
}
@article{BUI2021109392,
title = {Advanced data analytics for ship performance monitoring under localized operational conditions},
journal = {Ocean Engineering},
volume = {235},
pages = {109392},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109392},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821008040},
author = {Khanh Q. Bui and Lokukaluge P. Perera},
keywords = {Big data analytics, Machine learning, Ship performance monitoring, Energy efficiency, Emission control, Data anomaly detection},
abstract = {Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship’s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship’s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship’s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.}
}
@article{RIVAS201794,
title = {Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {94-104},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916301192},
author = {Bibiano Rivas and Jorge Merino and Ismael Caballero and Manuel Serrano and Mario Piattini},
keywords = {Master data, Data quality, ISO 8000, Big data},
abstract = {During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasets The main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different parts of the standard like the development of a data dictionary with master data terms; a communication protocol; an API to manage the master data messages; and the algorithms in MapReduce to measure the data quality.}
}
@article{LIU2018191,
title = {Steering data quality with visual analytics: The complexity challenge},
journal = {Visual Informatics},
volume = {2},
number = {4},
pages = {191-197},
year = {2018},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X18300573},
author = {Shixia Liu and Gennady Andrienko and Yingcai Wu and Nan Cao and Liu Jiang and Conglei Shi and Yu-Shuen Wang and Seokhee Hong},
keywords = {Data quality management, Visual analytics, Data cleansing},
abstract = {Data quality management, especially data cleansing, has been extensively studied for many years in the areas of data management and visual analytics. In the paper, we first review and explore the relevant work from the research areas of data management, visual analytics and human-computer interaction. Then for different types of data such as multimedia data, textual data, trajectory data, and graph data, we summarize the common methods for improving data quality by leveraging data cleansing techniques at different analysis stages. Based on a thorough analysis, we propose a general visual analytics framework for interactively cleansing data. Finally, the challenges and opportunities are analyzed and discussed in the context of data and humans.}
}
@incollection{MAYER201667,
title = {Chapter 5 - Big Data For Health Through Social Media},
editor = {Shabbir Syed-Abdul and Elia Gabarron and Annie Y.S. Lau},
booktitle = {Participatory Health Through Social Media},
publisher = {Academic Press},
pages = {67-82},
year = {2016},
isbn = {978-0-12-809269-9},
doi = {https://doi.org/10.1016/B978-0-12-809269-9.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092699000050},
author = {M.A. Mayer and L. Fernández-Luque and A. Leis},
keywords = {Big Data, social media, data analysis, public health, Internet},
abstract = {Social Media (SM) can be a complementary channel of information to other official means for the health data collection such as the epidemiological surveillance activities and control carried out by health authorities. For this reason, more and more organizations, professionals, and scientific institutions are seeing the need to make the most of resources of health information based on SM platforms through the use of Big Data tools and analytics. Although there is a consensus on the potential benefits and opportunities that SM may provide when used for healthcare purposes, its use has brought unsuspected drawbacks and challenges related to the protection of personal data, it is essential to promote a wide reflection and that the authorities and governments establish, in collaboration with patients associations and professional institutions, specific ethical, legal guidelines, and use policies to the benefit of the current and future healthcare professional–patient relationship and general public.}
}
@article{KNEPPER201792,
title = {Forward Observer system for radar data workflows: Big data management in the field},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {92-97},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17310567},
author = {Richard Knepper and Matthew Standish},
keywords = {Microcomputers, Information storage, Physical sciences and engineering},
abstract = {There are unique challenges in managing data collection and management from instruments in the field in general. These issues become extreme when “in the field” means “in a plane over the Antarctic”. In this paper we present the design and function of the Forward Observer a computer cluster and data analysis system that flies in a plane in the Arctic and Antarctic to collect, analyze in real time, and store Synthetic Aperture Radar (SAR) data. SAR is used to analyze the thickness and structure of polar ice sheets. We also discuss the processing of data once it is returned to the continental US and made available via data grids. The needs for in-flight data analysis and storage in the Antarctic and Arctic are highly unusual, and we have developed a novel system to meet those needs. We describe the constraints and requirements that led to the creation of this system and the general functionality which it applies to any instrument. We discuss the main means for handling replication and creating checksum information to ensure that data collected in polar regions are returned safely to mainland US for analysis. So far, not a single byte of data collected in the field has failed to make it home to the US for analysis (although many particular data storage devices have failed or been damaged due to the challenges of the extreme environments in which this system is used). While the Forward Observer system is developed for the extreme situation of data management in the field in the Antarctic, the technology and solutions we have developed are applicable and potentially usable in many situations where researchers wish to do real time data management in the field in areas that are constrained in terms of electrical supply.}
}
@article{GUNTHER2019583,
title = {Data quality assessment for improved decision-making: a methodology for small and medium-sized enterprises},
journal = {Procedia Manufacturing},
volume = {29},
pages = {583-591},
year = {2019},
note = {“18th International Conference on Sheet Metal, SHEMET 2019”“New Trends and Developments in Sheet Metal Processing”},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.02.114},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919301477},
author = {Lisa C. Günther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer},
keywords = {Data quality assessment, Data quality control, Information quality, Benchmarking, Production planning, control},
abstract = {Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.}
}
@incollection{SHANG2022203,
title = {Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS)},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {203-228},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000087},
author = {Wen-Long Shang and Haoran Zhang and Yi Sui},
keywords = {MaaS, Big data, Data mining, Support vector machine, Linear regression, Decision tree, Clustering, Bike-sharing, COVID-19},
abstract = {This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.}
}
@incollection{PHAN2017253,
title = {9 - Big Data and Monitoring the Grid},
editor = {Brian W. D’Andrade},
booktitle = {The Power Grid},
publisher = {Academic Press},
pages = {253-285},
year = {2017},
isbn = {978-0-12-805321-8},
doi = {https://doi.org/10.1016/B978-0-12-805321-8.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053218000094},
author = {Sonal K. Phan and Cathy Chen},
keywords = {Big data, power quality disturbance detection, intrusion detection, islanding detection, feature extraction, classification, data analytics, forecasting, visualization, smart meters, demand response},
abstract = {A traditional power grid, also known as the legacy grid, collects data at a few locations on the grid to monitor grid performance and forecast energy requirements on a macro level. A smart grid is the next generation of the electric power grid; it includes technologies for real-time data acquisition from various sections of the grid and provides a means for two-way communication between energy suppliers and consumers. Compared to a legacy grid, the smart grid generates large volumes of data that can be exploited for power quality event monitoring, intrusion detection, islanding detection, price forecasting, and energy forecasting at a much more granular level. These large volumes of data have to be analyzed in real-time and with high accuracy in order assist in decision making for power system operations and optimal power flow. This poses a big data challenge, which to be implemented successfully requires changes in infrastructure and data analysis methods. This chapter describes the smart grid and its associated big data and discusses methods for informative feature extraction from raw data, event monitoring, and energy consumption forecasting using these features and visualization methods to assist with data interpretation and decision making.}
}
@article{MARSDEN2019113172,
title = {Perspectives on numerical data quality in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113172},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113172},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619302015},
author = {James R. Marsden and David E. Pingry and Jason B. Thatcher}
}
@article{SZYMANSKA20181,
title = {Modern data science for analytical chemical data – A comprehensive review},
journal = {Analytica Chimica Acta},
volume = {1028},
pages = {1-10},
year = {2018},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2018.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0003267018306421},
author = {Ewa Szymańska},
keywords = {Chemometrics, Data science, Big data, Chemical analytical data, Methodology},
abstract = {Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.}
}
@article{MORANFERNANDEZ2022365,
title = {How important is data quality? Best classifiers vs best features},
journal = {Neurocomputing},
volume = {470},
pages = {365-375},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.107},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221011127},
author = {Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos},
keywords = {Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis},
abstract = {The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.}
}
@article{CHALVATZIS2019381,
title = {Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {381-393},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315147},
author = {Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary},
keywords = {Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming},
abstract = {Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.}
}
@incollection{GUDIVADA2016169,
title = {Chapter 5 - Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {169-205},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300517},
author = {V.N. Gudivada and M.T. Irfan and E. Fathi and D.L. Rao},
keywords = {Cognitive analytics, Text analytics, Learning analytics, Educational data mining, Cognitive systems, Cognitive computing, Personalized learning, Data science, Machine learning, Big data analytics, Business analytics},
abstract = {This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage—cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.}
}
@article{FRANCIA2021299,
title = {Making data platforms smarter with MOSES},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {299-313},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002260},
author = {Matteo Francia and Enrico Gallinucci and Matteo Golfarelli and Anna Giulia Leoni and Stefano Rizzi and Nicola Santolini},
keywords = {Data lake, Metadata, Big data, Data platform},
abstract = {The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.}
}
@article{KARKOUCH201657,
title = {Data quality in internet of things: A state-of-the-art survey},
journal = {Journal of Network and Computer Applications},
volume = {73},
pages = {57-81},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301564},
author = {Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel},
keywords = {Internet of things, Data quality, Data cleaning, Outlier detection},
abstract = {In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.}
}
@article{WU2022129487,
title = {Machine learning in the identification, prediction and exploration of environmental toxicology: Challenges and perspectives},
journal = {Journal of Hazardous Materials},
volume = {438},
pages = {129487},
year = {2022},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2022.129487},
url = {https://www.sciencedirect.com/science/article/pii/S0304389422012808},
author = {Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu},
keywords = {Machine learning, Chemical, Toxicity, Environmental health, Big data},
abstract = {Over the past few decades, data-driven machine learning (ML) has distinguished itself from hypothesis-driven studies and has recently received much attention in environmental toxicology. However, the use of ML in environmental toxicology remains in the early stages, with knowledge gaps, technical bottlenecks in data quality, high-dimensional/heterogeneous/small-sample data analysis and model interpretability, and a lack of an in-depth understanding of environmental toxicology. Given the above problems, we review the recent progress in the literature and highlight state-of-the-art toxicological studies using ML (such as learning and predicting toxicity in complicated biosystems and multiple-factor environmental scenarios of long-term and large-scale pollution). Beyond predicting simple biological endpoints by integrating untargeted omics and adverse outcome pathways, ML development should focus on revealing toxicological mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis and adverse outcome pathway frameworks) endows ML with widely promising application in revealing toxicological mechanisms. High-quality databases and interpretable algorithms are urgently needed for toxicology and environmental science. Addressing the core issues and future challenges for ML in this review may narrow the knowledge gap between environmental toxicity and computational science and facilitate the control of environmental risk in the future.}
}
@incollection{ANYA201699,
title = {Chapter 5 - Leveraging Big Data Analytics for Personalized Elderly Care: Opportunities and Challenges},
editor = {Dhiya Al-Jumeily and Abir Hussain and Conor Mallucci and Carol Oliver},
booktitle = {Applied Computing in Medicine and Health},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {99-124},
year = {2016},
series = {Emerging Topics in Computer Science and Applied Computing},
isbn = {978-0-12-803468-2},
doi = {https://doi.org/10.1016/B978-0-12-803468-2.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034682000059},
author = {Obinna Anya and Hissam Tawfik},
keywords = {Elderly care, personalized care, independent living, ACTVAGE, Big Data analytics, CAPIM, lifestyle-oriented, context-awareness, framework},
abstract = {Owing to the growing increase in the world’s ageing population, research has focused on developing information and communication technology (ICT)–based services for personalized care, improved health, and quality social life for the elderly. Recent efforts explore Big Data in order to build mathematical models of personal behavior and lifestyle for analytics. Leveraging Big Data analytics holds enormous potential for solving some of the biggest and most intractable challenges in personalized elderly care through quantified modeling of a person’s lifestyle in a way that takes cognizance of their beliefs, values, and preferences, and connects to a history of events, things, and places around which they have progressively built their lives. However, the idea of discovering patterns to personalize care and inform critical health care decisions for the elderly is challenged as data grow exponentially in volume, become faster and increasingly unstructured, and are generated from sociodigital engagements that often may not accurately reflect the real-world entities and contexts they represent. As a result, the idea raises issues along several dimensions, including social, technical, and context-aware challenges. In this chapter, we present an overview of the state of the art in personalized elderly care, and explore the opportunities and inherent sociotechnical challenges in leveraging Big Data analytics to support elderly care and independent living. Based on this discussion, and arguing that analytics need to take account of the contexts that shape the generation and use of data, ACTVAGE, a context-aware lifestyle-oriented framework for personalized elderly care and independent living is proposed.}
}
@article{GUETA2016139,
title = {Quantifying the value of user-level data cleaning for big data: A case study using mammal distribution models},
journal = {Ecological Informatics},
volume = {34},
pages = {139-145},
year = {2016},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1574954116300577},
author = {Tomer Gueta and Yohay Carmel},
keywords = {Biodiversity informatics, Data-cleaning, SDM performance, MaxEnt, Australian mammals, Big-data},
abstract = {The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g–100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.}
}
@article{DONG2015278,
title = {Traffic zone division based on big data from mobile phone base stations},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {58},
pages = {278-291},
year = {2015},
note = {Big Data in Transportation and Traffic Engineering},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15002223},
author = {Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou},
keywords = {Mobile telephones, Call detail record (CDR) data, Traffic semantic analysis, Traffic zone division, Traffic zone attribute index, Travel patterns},
abstract = {Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters’ origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing’s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.}
}
@article{SALEM20225552,
title = {LODQuMa: A Free-ontology process for Linked (Open) Data quality management},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {5552-5563},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001348},
author = {Samah Salem and Fouzia Benchikha},
keywords = {Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia},
abstract = {For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.}
}
@article{EVANGELISTA2018112,
title = {Topological support and data quality can only be assessed through multiple tests in reviewing Blattodea phylogeny},
journal = {Molecular Phylogenetics and Evolution},
volume = {128},
pages = {112-122},
year = {2018},
issn = {1055-7903},
doi = {https://doi.org/10.1016/j.ympev.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1055790318300186},
author = {Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre},
keywords = {Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long branch attraction, Signal analysis},
abstract = {Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.}
}
@article{RADONJIC2022,
title = {Artificial intelligence and HRM: HR managers’ perspective on decisiveness and challenges},
journal = {European Management Journal},
year = {2022},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0263237322000883},
author = {Aleksandar Radonjić and Henrique Duarte and Nádia Pereira},
keywords = {Human resources, HR, HRM, e-HRM, Decision-making, Big data (BD), Big data maturity models (BDMM), Artificial intelligence (AI)},
abstract = {Focus
The transformative power of today's big data (BD) has allowed many companies, i.e., decision-makers, to evolve at an unprecedented pace. With regard to decision-making, artificial intelligence (AI) takes task delegation to a new level, and by employing AI-assisted tools, companies can provide their HR departments with the means to manage the existing data and HR altogether.
Objectives
To determine how HR managers assess whether BD management is facilitated by AI, and how they frame the changes necessary to meet the trends related to AI and its implementation, namely their willingness to master its implementation and to meet the possible challenges.
Methodology
Content analysis was conducted on interviews held with a sample of 16 HR practitioners from a spectrum of areas, and the findings were analysed using the big data maturity model (BDMM) framework. Domains covered by this model allow the study of decision-making trends, in terms of preparedness and willingness to tackle disruptive technology with the aim of improving and gaining the competitive edge in decision-making.
Findings
The central potential of AI lies in faster data storage and processing power, thereby leading to more insightful and effective decision-making. This article contains closer insights into the challenges underlying the implementation of AI in decision-making processes, specifically in terms of strategic alignment, governance, and implementation. The results reflect the notions regarding the nature of AI – in assisting HR – and lay out the path that precedes the extraction of BD, through the delivery of advantageous intelligence, to augment decision-making in HR.}
}
@article{KOLOSSA2018775,
title = {Data quality over data quantity in computational cognitive neuroscience},
journal = {NeuroImage},
volume = {172},
pages = {775-785},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918300053},
author = {Antonio Kolossa and Bruno Kopp},
keywords = {Computational modeling, Functional brain imaging, Signal-to-noise ratio, Reliability, Replicability},
abstract = {We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.}
}
@article{DAVIS2017224,
title = {Residential land values in the Washington, DC metro area: New insights from big data},
journal = {Regional Science and Urban Economics},
volume = {66},
pages = {224-246},
year = {2017},
issn = {0166-0462},
doi = {https://doi.org/10.1016/j.regsciurbeco.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0166046216301508},
author = {Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka},
keywords = {Land, Housing, House prices, Housing boom and bust, Financial crisis},
abstract = {We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.}
}
@incollection{TALBURT2015191,
title = {Chapter 11 - ISO Data Quality Standards for Master Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {191-205},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000119},
author = {John R. Talburt and Yinle Zhou},
keywords = {ISO, ANSI, ISO 8000, ISO 22745, Semantic Encoding},
abstract = {This chapter provides a discussion of the new International Organization for Standardization (ISO) standards related to the exchange of master data. It includes an in-depth look at the ISO 8000 family of standards, including ISO 8000-110, -120, -130, and -140, and their relationship to the ISO 22745-10, -30, and -40 standards. Also an explanation is given of simple versus strong ISO 8000-110 compliance, and the value proposition for ISO 8000 compliance is discussed.}
}
@article{HEINRICH201895,
title = {Assessing data quality – A probability-based metric for semantic consistency},
journal = {Decision Support Systems},
volume = {110},
pages = {95-106},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300599},
author = {Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner},
keywords = {Data quality, Data quality assessment, Data quality metric, Data consistency},
abstract = {We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.}
}
@article{STOGER2021105587,
title = {Legal aspects of data cleansing in medical AI},
journal = {Computer Law & Security Review},
volume = {42},
pages = {105587},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105587},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000601},
author = {Karl Stöger and David Schneeberger and Peter Kieseberg and Andreas Holzinger},
keywords = {Data cleansing, Data quality, Medical AI, Medical devices},
abstract = {Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to "dirty data" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.}
}
@article{BAVARESCO2022112197,
title = {Are years-long field studies about window operation efficient? a data-driven approach based on information theory and deep learning},
journal = {Energy and Buildings},
volume = {268},
pages = {112197},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112197},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822003681},
author = {Mateus Bavaresco and Ioannis Kousis and Ilaria Pigliautile and Anna {Laura Pisello} and Cristina Piselli and Enedir Ghisi},
keywords = {Information theory, Occupant behaviour, Energy efficiency, Machine learning, Data length, Big data, Indoor environmental quality, Continuous monitoring},
abstract = {Scientific literature about building occupants’ behaviour and the related energy performance analyses document about several strategies to monitor window operation, including different sensors and data series lengths. In this framework, the primary goal of this study is to propose effective guidelines for minimum experiment durations and their reliability. A six-year-long database from a living laboratory was used as a benchmark; and a recursive strategy enabled to split it into more than 2,500 subsets, supporting two main steps. First, information theory concepts were used to calculate uncertainty and subsets’ divergence were compared to the full database. Second, the subsets were used to train deep neural networks and evaluate the influence of monitoring lengths combined with different kinds of environmental data (i.e. indoor or outdoor). From the information-theoretic metrics, the results support that indoor-related variables can reduce most of the uncertainty related to window operation. Besides, subsets influenced by autumn and winter diverge the most compared to the full database. Considering the modelling approach, the results demonstrated that by including indoor-related variables, higher shares of reliably-performing models were achieved, and smaller subsets were needed. Seasonality has also played a major role along these lines. As a consequence, the conclusions supported the feasibility of nine-month-long field studies, starting in summer or spring, when indoor and outdoor variables are monitored.}
}
@article{MADHIKERMI2016145,
title = {Data quality assessment of maintenance reporting procedures},
journal = {Expert Systems with Applications},
volume = {63},
pages = {145-164},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S095741741630330X},
author = {Manik Madhikermi and Sylvain Kubler and Jérémy Robert and Andrea Buda and Kary Främling},
keywords = {Data quality, Information quality, Multi-criteria decision making, Analytic hierarchy process, Decision support systems, Maintenance},
abstract = {Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.}
}
@article{BELLINI2019521,
title = {Data quality and blockchain technology},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {5},
pages = {521-522},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818305368},
author = {Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami},
keywords = {Machine learning, Artificial intelligence, Blockchain technology}
}
@article{LIU2022105219,
title = {Investigation of VRF system cooling operation and performance in residential buildings based on large-scale dataset},
journal = {Journal of Building Engineering},
volume = {61},
pages = {105219},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.105219},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222012256},
author = {Hua Liu and Yi Wu and Da Yan and Shan Hu and Mingyang Qian},
keywords = {Big data, VRF systems, Operational performance, Statistical analysis},
abstract = {With the increase in the energy consumption of air-conditioning (AC) systems in Chinese residential buildings, the realization of energy savings in AC systems has attracted increasing attention. The variable refrigerant flow (VRF) system is a common AC system for residential buildings in China. In most previous studies on VRF systems, onsite measurements or surveys were conducted to collect operational data. These traditional methods may face various data issues, such as limited sample sizes and invalid data, making them unable to capture the spatial and temporal performance features of VRF systems in residential buildings on a large scale. To fill this gap, with advances in data storage and transmission technology, Big Data methods have been widely used for data collection. In the present study, researchers adopted 16,985 sets of VRF system operation data from China as the database and conducted data analysis for both the spatial and temporal dimensions. Several key indicators were proposed from the two perspectives (spatial and temporal), including the part-space index (PSI), load ratio (LR), use duration (UD), and cooling energy consumption. The main findings were as follows: (1) The “part-time part-space” operation mode of residential VRF systems can be analyzed according to the statistical results of the UD and PSI. (2) An LR of <30% is the main operating condition for VRF systems in residential buildings. (3) Extracted typical LR patterns can reflect different user behavior. The statistical results obtained in this study provide a basis for VRF engineering projects.}
}
@article{HOSEINZADEH2020101518,
title = {Quality of location-based crowdsourced speed data on surface streets: A case study of Waze and Bluetooth speed data in Sevierville, TN},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101518},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101518},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302519},
author = {Nima Hoseinzadeh and Yuandong Liu and Lee D. Han and Candace Brakewood and Amin Mohammadnazar},
keywords = {Location-based data, Crowdsourced data, Waze, Bluetooth, Big data, Smart cities, Surface streets},
abstract = {Obtaining accurate speed and travel time information is a challenge for researchers, geographers, and transportation agencies. In the past, traffic data were usually acquired and disseminated by government agencies through fixed-location sensors. High costs, infrastructure demands, and low coverage levels of these sensor devices require agencies and researchers to look beyond the traditional approaches. With the emergence of smartphones and navigation apps, location-based and crowdsourced Big Data are receiving increased attention. In this regard, location-based big data (LocBigData) collected from probe vehicles and road users can be used to provide speed and travel time information in different locations. Examining the quality of crowdsourced data is essential for researchers and agencies before using them. This study assessed the quality of Waze speed data from surface streets and conducted a case study in Sevierville, Tennessee. Typically, examining the quality of these data in surface streets and arterials is more challenging than freeways data. This research used Bluetooth speed data as the ground truth, which is independent of Waze data. In this study, three steps of methodology were used. In the first step, Waze speed data was compared to Bluetooth data in terms of accuracy, mean difference, and distribution similarity. In the second step, a k-means algorithm was used to categorize Waze data quality, and a multinomial logistics regression model was performed to explore the significant factors that impact data quality. Finally, in the third step, machine learning techniques were conducted to predict the data quality in different conditions. The result of the comparison showed a similar pattern and a slight difference between datasets, which verified the quality of Waze speed data. The statistical model indicates that that Waze speed data are more accurate in peak hours than in night hours. Also, the traffic speed, traffic volume, and segment length have a significant association on the accuracy of Waze data on surface streets. Finally, the result of machine learning prediction showed that a KNN method performed the highest prediction accuracy of 84.5% and 82.9% of the time for training and test datasets, respectively. Overall, the study results suggest that Waze speed data is a promising data source for surface streets.}
}
@article{SINGH2018652,
title = {Real world big data for clinical research and drug development},
journal = {Drug Discovery Today},
volume = {23},
number = {3},
pages = {652-660},
year = {2018},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617305950},
author = {Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra},
abstract = {The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this ‘big denominator’ capability, especially given the needs of precision medicine research.}
}
@article{ALWIS2022103624,
title = {A survey on smart farming data, applications and techniques},
journal = {Computers in Industry},
volume = {138},
pages = {103624},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103624},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000197},
author = {Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar},
keywords = {Smart farming, Data analysis, Big data, Machine learning, Digital farming, Predictive farming, Farming industry},
abstract = {The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.}
}
@incollection{SEBASTIANCOLEMAN2013173,
title = {Chapter 13 - Directives for Data Quality Strategy},
editor = {Laura Sebastian-Coleman},
booktitle = {Measuring Data Quality for Ongoing Improvement},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {173-192},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397033-6},
doi = {https://doi.org/10.1016/B978-0-12-397033-6.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970336000146},
author = {Laura Sebastian-Coleman}
}
@article{YANG2021,
title = {Standardization of collection, storage, annotation, and management of data related to medical artificial intelligence},
journal = {Intelligent Medicine},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621001200},
author = {Yahan Yang and Ruiyang Li and Yifan Xiang and Duoru Lin and Anqi Yan and Wenben Chen and Zhongwen Li and Weiyi Lai and Xiaohang Wu and Cheng Wan and Wei Bai and Xiucheng Huang and Qiang Li and Wenrui Deng and Xiyang Liu and Yucong Lin and Pisong Yan and Haotian Lin},
keywords = {Artificial intelligence, Big data, Intelligent medicine, Data collection, Data storage, Data annotation, Data management},
abstract = {Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.}
}
@incollection{TONG2020107,
title = {Chapter 5 - Machine learning for spatiotemporal big data in air pollution},
editor = {Lixin Li and Xiaolu Zhou and Weitian Tong},
booktitle = {Spatiotemporal Analysis of Air Pollution and Its Application in Public Health},
publisher = {Elsevier},
pages = {107-134},
year = {2020},
isbn = {978-0-12-815822-7},
doi = {https://doi.org/10.1016/B978-0-12-815822-7.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158227000054},
author = {Weitian Tong},
keywords = {Air pollution, Fine particulate matter, Spatiotemporal interpolation, Machine learning, Deep learning},
abstract = {An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.}
}
@article{VIDAURRE2018646,
title = {Discovering dynamic brain networks from big data in rest and task},
journal = {NeuroImage},
volume = {180},
pages = {646-656},
year = {2018},
note = {Brain Connectivity Dynamics},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.06.077},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917305487},
author = {Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich},
abstract = {Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.}
}
@article{SADIQ2017150,
title = {Open data: Quality over quantity},
journal = {International Journal of Information Management},
volume = {37},
number = {3},
pages = {150-154},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216309021},
author = {Shazia Sadiq and Marta Indulska},
keywords = {Open data, Data quality},
abstract = {Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.}
}
@incollection{TALBURT2015161,
title = {Chapter 10 - CSRUD for Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {161-190},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000107},
author = {John R. Talburt and Yinle Zhou},
keywords = {Big Data, Hadoop Map/Reduce, Transitive Closure, Graph Component},
abstract = {This chapter describes how a distributed processing environment such as Hadoop Map/Reduce can be used to support the CSRUD Life Cycle for Big Data. The examples shown in this chapter use the match key blocking described in Chapter 9 as a data partitioning strategy to perform ER on large datasets. The chapter includes an algorithm for finding the transitive closure of multiple match keys in a distributed processing environment using an iterative algorithm that minimizes the amount of local memory required for each processor. It also outlines a structure for an identity knowledge base in a distributed key-value data store, and describes strategies and distributed processing workflows for capture and update phases of the CSRUD life cycle using both record-based and attribute-based cluster-to-cluster structure projections.}
}
@article{WANG2022e97,
title = {Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care},
journal = {Clinical Oncology},
volume = {34},
number = {2},
pages = {e97-e103},
year = {2022},
note = {Artificial Intelligence in Radiation Therapy},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2021.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0936655521004593},
author = {J.W. Wang and M. Williams},
keywords = {Artificial intelligence, Big Data, database, deep learning, registries, repository},
abstract = {Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.}
}
@article{LOOTEN2019104825,
title = {What can millions of laboratory test results tell us about the temporal aspect of data quality? Study of data spanning 17 years in a clinical data warehouse},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104825},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718307089},
author = {Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and Laëtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance},
keywords = {Quality control, Computational biology/methods*, Information storage and retrieval, Humans, Clinical laboratory information systems},
abstract = {Objective
To identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).
Materials and Methods
We leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000 + patients, and 131 million laboratory test results).
Results
We identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.
Discussion and conclusion
our method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).}
}
@article{BUFFAT2017277,
title = {Big data GIS analysis for novel approaches in building stock modelling},
journal = {Applied Energy},
volume = {208},
pages = {277-290},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S030626191731454X},
author = {René Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg},
keywords = {Building heat demand, Big data, Large scale modelling, Bottom-up modelling, GIS, Climate data, Spatio-temporal modelling},
abstract = {Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.}
}
@article{BRONSELAER201895,
title = {An incremental approach for data quality measurement with insufficient information},
journal = {International Journal of Approximate Reasoning},
volume = {96},
pages = {95-111},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X17307478},
author = {A. Bronselaer and J. Nielandt and G. {De Tré}},
keywords = {Data quality measurement, Uncertainty modelling, Insufficient information, Possibility theory},
abstract = {Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.}
}
@article{WIBISONO201633,
title = {Traffic big data prediction and visualization using Fast Incremental Model Trees-Drift Detection (FIMT-DD)},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {33-46},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004165},
author = {Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto},
keywords = {Intelligent traffic systems, Data stream, Traffic prediction, Traffic visualization},
abstract = {Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24 h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.}
}
@incollection{KRISHNAN2013257,
title = {Chapter 14 - Implementing the Big Data – Data Warehouse – Real-Life Situations},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {257-265},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000143},
author = {Krish Krishnan},
keywords = {Hadoop, RDBMS, NoSQL, transformation, architecture},
abstract = {This chapter discusses the real-life implementation of the next-generation platform by three different companies and the direction they each have chosen from a technology and architecture perspective.}
}
@article{ALGHAMDI2021462,
title = {Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {462-479},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2033079X},
author = {Ibrahim Alghamdi and Christos Anagnostopoulos and Dimitrios P. Pezaros},
keywords = {Mobile edge computing, Tasks offloading, Data quality, Optimal stopping theory, Sequential decision making},
abstract = {An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.}
}
@article{SU201722,
title = {A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning},
journal = {Habitat International},
volume = {64},
pages = {22-40},
year = {2017},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0197397517300498},
author = {Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng},
keywords = {Food geography, Healthy food access, Accessibility, Social inequalities, Transport mode, Multilevel regression},
abstract = {Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.}
}
@article{DUVIER2018358,
title = {Data quality and governance in a UK social housing initiative: Implications for smart sustainable cities},
journal = {Sustainable Cities and Society},
volume = {39},
pages = {358-365},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717312520},
author = {Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava},
keywords = {Data quality, Data interoperability, Social housing, Smart sustainable cities, Business intelligence},
abstract = {Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to ‘talk to’ each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.}
}
@incollection{REEVE2013141,
title = {Chapter 21 - Big Data Integration},
editor = {April Reeve},
booktitle = {Managing Data in Motion},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {141-156},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397167-8},
doi = {https://doi.org/10.1016/B978-0-12-397167-8.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971678000212},
author = {April Reeve}
}
@article{LOPEZ20162128,
title = {Data Quality Control for St. Petersburg Flood Warning System},
journal = {Procedia Computer Science},
volume = {80},
pages = {2128-2140},
year = {2016},
note = {International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.532},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916310225},
author = {Jose Luis Araya Lopez and Anna V. Kalyuzhnaya and Sergey S. Kosukhin and Sergey V. Ivanov},
keywords = {outliers, quality-control, principal components, gap filling},
abstract = {This paper focuses on techniques for dealing with imperfect data in a frame of early warning system (EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing values, met-ocean simulation systems have to deal with them to provide data transaction between models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control of simulated fields, statistical control and restoration of measurements and control using alternative models. Domain specific quality control was presented as two types of procedures based on theoretically proved methods were applied. The first procedure is based on probabilistic model of dynamical system, where processes are spatially interrelated and could be implemented in a form of multivariate regression (MRM). The second procedure is based on principal component analysis extended for taking into account temporal relations in data set (ePCA).}
}
@incollection{MCKNIGHT201432,
title = {Chapter Four - Data Quality: Passing the Standard},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {32-43},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000047},
author = {William McKnight},
keywords = {data quality, referential integrity, system of origination, data profiling},
abstract = {We might as well not do any data storage if we are not storing and passing high quality data. This chapter defines data quality and a program to maintain high standards throughout the enterprise.}
}
@article{DEVI20224980,
title = {Traffic management in smart cities using support vector machine for predicting the accuracy during peak traffic conditions},
journal = {Materials Today: Proceedings},
volume = {62},
pages = {4980-4984},
year = {2022},
note = {International Conference on Innovative Technology for Sustainable Development},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.03.722},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322022131},
author = {T. Devi and K. Alice and N. Deepa},
keywords = {Big data, Mobility, Linear and logistic regression, Support vector machine (SVM), Traffic management},
abstract = {Mobility is the main key for smart living, where navigation and automatic suggestions are also a strategy for a successful life in smart cities. Big Data analytics are behind urban changes in the mobility of smart cities to bring sustainable life. By the year 2025 all over Indian states can reach the expected lifestyle by providing high security and mobility which can grow the opportunities also high. As the population is rapidly increasing, the needs of people are also increasing such that necessitating real-time apps for daily needs, communication devices, and so on. We focus our idea on the benefits of traffic and safety measures which are becoming a huge challenge nowadays. Many are preferred with sophistication when traveling for short distances. In such a way the big data analytics tools R studio and weka are used on the dataset smart city from the Kaggle website for traffic patterns during the high traffic duration. Using the dataset, the data are classified using a Support vector machine (SVM) and applied with regression such as linear, logistic regression to find the accuracy of traffic peak situations. The proposed work aims to compare the efficiency of big data technologies which can be applied using various classification and regression that can be shown on various tools such as R, Weka, map-reduce which can produce accurate results to visualize the smart cities and their traffic analysis.}
}
@incollection{DHAESE2018137,
title = {Chapter 13 - Big Data and Deep Brain Stimulation},
editor = {Elliot S. Krames and P. Hunter Peckham and Ali R. Rezai},
booktitle = {Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {137-145},
year = {2018},
isbn = {978-0-12-805353-9},
doi = {https://doi.org/10.1016/B978-0-12-805353-9.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053539000139},
author = {Pierre-Francois D’Haese and Peter E. Konrad and Benoit M. Dawant},
keywords = {Atlas, Big data, Collaborative, CranialCloud, DBS, Normalization},
abstract = {Surgeons, neurologists, researchers, and patients have lacked the technology-based tools to facilitate sharing the tremendously valuable data about patients’ treatment and research in regard to what is working and what is not. Today, only 9% of patients who could benefit from complex therapies to address neurologic conditions actually receive them, and the medical information for each patient who does is hidden away in disconnected databases. To optimize and accelerate our understanding of the brain, we need to gather intelligence around every case, every research subject, every study while connecting that information through a unified, Health Insurance Portability and Accountability Act of 1996 (HIPAA)-compliant network that leverages technology and harnesses the Internet to drive advancements and better connect patients to their care teams. In this chapter, we highlight the key aspects needed to fulfill the requirements of a robust, HIPAA-compliant archive for brain data and highlight the impact of normalization on the accuracy of statistical analyses.}
}
@article{GUO20221792,
title = {Measuring and evaluating SDG indicators with Big Earth Data},
journal = {Science Bulletin},
volume = {67},
number = {17},
pages = {1792-1801},
year = {2022},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2022.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S2095927322002997},
author = {Huadong Guo and Dong Liang and Zhongchang Sun and Fang Chen and Xinyuan Wang and Junsheng Li and Li Zhu and Jinhu Bian and Yanqiang Wei and Lei Huang and Yu Chen and Dailiang Peng and Xiaosong Li and Shanlong Lu and Jie Liu and Zeeshan Shirazi},
keywords = {Big Earth Data, Big data, Sustainable Development Goals (SDGs), Decision support, CASEarth, Digital Earth},
abstract = {The United Nations 2030 Agenda for Sustainable Development provides an important framework for economic, social, and environmental action. A comprehensive indicator system to aid in the systematic implementation and monitoring of progress toward the Sustainable Development Goals (SDGs) is unfortunately limited in many countries due to lack of data. The availability of a growing amount of multi-source data and rapid advancements in big data methods and infrastructure provide unique opportunities to mitigate these data shortages and develop innovative methodologies for comparatively monitoring SDGs. Big Earth Data, a special class of big data with spatial attributes, holds tremendous potential to facilitate science, technology, and innovation toward implementing SDGs around the world. Several programs and initiatives in China have invested in Big Earth Data infrastructure and capabilities, and have successfully carried out case studies to demonstrate their utility in sustainability science. This paper presents implementations of Big Earth Data in evaluating SDG indicators, including the development of new algorithms, indicator expansion (for SDG 11.4.1) and indicator extension (for SDG 11.3.1), introduction of a biodiversity risk index as a more effective analysis method for SDG 15.5.1, and several new high-quality data products, such as global net ecosystem productivity, high-resolution global mountain green cover index, and endangered species richness. These innovations are used to present a comprehensive analysis of SDGs 2, 6, 11, 13, 14, and 15 from 2010 to 2020 in China utilizing Big Earth Data, concluding that all six SDGs are on schedule to be achieved by 2030.}
}
@article{SONG201734,
title = {Data quality management for service-oriented manufacturing cyber-physical systems},
journal = {Computers & Electrical Engineering},
volume = {64},
pages = {34-44},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302099},
author = {Zhiting Song and Yanming Sun and Jiafu Wan and Peipei Liang},
keywords = {Data quality, Cyber-physical systems, Service-oriented manufacturing, Workflow nets},
abstract = {Service-oriented manufacturing (SOM) is a new worldwide manufacturing paradigm, and a cyber-physical system (CPS) is accepted as a strategic choice of SOM enterprises looking to provide bundles of satisfying products and services to customers. The issue of data quality is common in any CPS and poses great challenges to its efficient operation. This paper focuses on defective data generated by the improper operation of physical and cyber components of a service-oriented manufacturing CPS (SMCPS), and develops effective managerial policies to deal with such data. First, formal semantics of workflow nets (WF-nets) are employed to construct process-oriented ontology for the SMCPS. Second, a two-stage optimization model together with algorithms is designed to find optimal policies that balance local and global management objectives. Finally, our model is illustrated through a case. Results show that the proposed control strategy outperforms one-stage control and random control in guaranteeing data quality and saving control costs.}
}
@article{MATHEW201585,
title = {Big-data for building energy performance: Lessons from assembling a very large national database of building energy use},
journal = {Applied Energy},
volume = {140},
pages = {85-93},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914012112},
author = {Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter},
keywords = {Buildings Performance Database, Building performance, Big data, Building data collection, Data-driven decision support},
abstract = {Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.}
}
@incollection{LEVIN2016317,
title = {Chapter 11 - From Databases to Big Data},
editor = {Elaine Holmes and Jeremy K. Nicholson and Ara W. Darzi and John C. Lindon},
booktitle = {Metabolic Phenotyping in Personalized and Public Healthcare},
publisher = {Academic Press},
address = {Boston},
pages = {317-331},
year = {2016},
isbn = {978-0-12-800344-2},
doi = {https://doi.org/10.1016/B978-0-12-800344-2.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128003442000112},
author = {Nadine Levin and Reza M. Salek and Christoph Steinbeck},
keywords = {Databases, big data, metabolomics, metabolic phenotyping, data exchange, data warehousing},
abstract = {This chapter explains the concept of big data and shows that the analytical data and related meta-data of metabolic phenotyping experiments fall into this category. The various databases that can be used to aid interpretation of such data, comprising general chemical and biochemical data, biochemical pathway specific data, analytical chemistry data to aid metabolite identification, and finally metabolic results, are discussed. The concept of data warehousing is explored in the context of metabolic data sets. Finally, the challenges for successful data storage, data exchange, and data interpretation are discussed.}
}
@incollection{CHAKRABORTY202273,
title = {Chapter 7 - Recent advances in processing, interpreting, and managing biological data for therapeutic intervention of human infectious disease},
editor = {Pantea Keikhosrokiani},
booktitle = {Big Data Analytics for Healthcare},
publisher = {Academic Press},
pages = {73-82},
year = {2022},
isbn = {978-0-323-91907-4},
doi = {https://doi.org/10.1016/B978-0-323-91907-4.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323919074000091},
author = {Pritha Chakraborty and Parth Sarthi {Sen Gupta} and Shankar Dey and Nabarun {Chandra Das} and Ritwik Patra and Suprabhat Mukherjee},
keywords = {Big Data, Biomedical, Electronic health records, Epidemiology, Surveillance},
abstract = {Big data has been a buzzword for academics and the healthcare industry, describing data accumulation, processing, and interpretation using analytical tools and techniques. Hitherto, identification of state of the art in big data has become a question of research for academicians due to lack of addressable studies in this regard. The potential of big data is lagging, as compared to other fields. Herein, in this chapter, we have described the potential of big data in combating infectious diseases involving personalized, participatory, predictive, and preventive models based on biomedical data also called the “omics data” and electronic health records from different authenticated sources. Our study indicates the use of various tools and techniques for data accumulation and management, thus providing an insight toward the revolution of the healthcare industry as well as the research community. Though the application of big data is still in the preliminary stage, growing Research and Development investment with its successful implementation will show enormous potential of growth in coming years. Considering all together, there is a need for the development of advanced technologies with inclusion of transparent ethical values to provide acceptance and with a moral foundation.}
}
@article{VISSER2021623,
title = {Imprecision farming? Examining the (in)accuracy and risks of digital agriculture},
journal = {Journal of Rural Studies},
volume = {86},
pages = {623-632},
year = {2021},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2021.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0743016721002217},
author = {Oane Visser and Sarah Ruth Sippel and Louis Thiemann},
keywords = {Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data},
abstract = {The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.}
}
@article{CLARKE2018467,
title = {Guidelines for the responsible application of data analytics},
journal = {Computer Law & Security Review},
volume = {34},
number = {3},
pages = {467-476},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917303643},
author = {Roger Clarke},
keywords = {Big data, Data science, Data quality, Decision quality, Regulation},
abstract = {The vague but vogue notion of ‘big data’ is enjoying a prolonged honeymoon. Well-funded, ambitious projects are reaching fruition, and inferences are being drawn from inadequate data processed by inadequately understood and often inappropriate data analytic techniques. As decisions are made and actions taken on the basis of those inferences, harm will arise to external stakeholders, and, over time, to internal stakeholders as well. A set of Guidelines is presented, whose purpose is to intercept ill-advised uses of data and analytical tools, prevent harm to important values, and assist organisations to extract the achievable benefits from data, rather than dreaming dangerous dreams.}
}
@article{SALVETAT2020101602,
title = {Data determinants of the activity of SMEs automobile dealers},
journal = {Journal of Engineering and Technology Management},
volume = {58},
pages = {101602},
year = {2020},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2020.101602},
url = {https://www.sciencedirect.com/science/article/pii/S0923474820300503},
author = {David Salvetat and Jean-Sébastien Lacam},
keywords = {Big data, Smart data, Development, Automobile, SME},
abstract = {Many SMEs still seem reluctant to accept the management of large datasets, which still appear to be too complex for them. However, our study reveals that the majority of small French car dealers are developing Big data and Smart data policies to improve the quality of their offers, the dynamism of their sales and their access to new opportunities. However, not every policy has the same effects on the development of their business. Whereas Big data improves all the components of SME development in a global, short-term and operational way, Smart data presents itself as a more targeted, prospective and strategic approach.}
}
@article{SHARMA2020538,
title = {MR-I MaxMin-scalable two-phase border based knowledge hiding technique using MapReduce},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {538-550},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.05.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17322173},
author = {Shivani Sharma and Durga Toshniwal},
keywords = {Big data, Computational cost, Knowledge hiding techniques, MapReduce, Privacy preservation, Scalability},
abstract = {Border based Knowledge hiding techniques (BB-KHT) are widely adopted form of privacy preservation techniques of data mining. These approaches are used to hide sensitive knowledge (confidential information) present in a dataset before sharing or analyzing it. BB-KHT primarily rely on border theory and maximum criterion method for preserving privacy and perpetuating good data quality of sanitized dataset but costs high computational complexity. Further, due to sequential nature, these approaches are particularly felicitous for small datasets and become infeasible while dealing with large scale datasets. Therefore, to subjugate the identified challenges of infeasibility and high computational complexity, a scalable two-phase improved MaxMin BB-KHT using MapReduce framework (MR-I MaxMin) is proposed. The proposed scheme requires only two database scans throughout the hiding process and hence, is computationally inexpensive. Moreover, the scheme also commits to preserve good data quality of sanitized dataset. The MapReduce version of proposed approach helps in achieving the feasibility by processing large voluminous data in a parallel fashion. Quantitative experiments and evaluations have been performed over a number of real and synthetically generated large-scale datasets. It is shown that the proposed MR-I MaxMin technique outperforms the similar existing approaches and vanquishes the identified challenges along with much-needed privacy preservation.}
}
@article{DUVIER2018196,
title = {Data quality challenges in the UK social housing sector},
journal = {International Journal of Information Management},
volume = {38},
number = {1},
pages = {196-200},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216308222},
author = {Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens},
keywords = {Social housing, Data quality},
abstract = {The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.}
}
@article{GHORBANI2022101089,
title = {Framework components for data-centric dry laboratories in the minerals industry: A path to science-and-technology-led innovation},
journal = {The Extractive Industries and Society},
volume = {10},
pages = {101089},
year = {2022},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2022.101089},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X22000508},
author = {Yousef Ghorbani and Steven E. Zhang and Glen T. Nwaila and Julie E. Bourdeau},
keywords = {Dry laboratory, Data analytics, Process simulation, Mining industry, Data-centric, Data-driven},
abstract = {ABSTRACT
The world continues to experience a surge in data generation and digital transformation. Historic data is increasingly being replaced by modernized data, such as big data, which is regarded as data that exhibits the 5Vs: volume, variety, velocity, veracity and value. The capacity to optimally use and comprehend value from big data has become an indispensable aptitude for modern companies. In contrast to commercial and technology firms, usage, management and governance of data, including big data is a novel and evolving trend for mining and mineral industries. Although the mining industry can be unenthusiastic to change, embracing modernized data and big data is evolutionarily unavoidable, given many industry-wide challenges (i.e., fluctuation in commodity prices, geotechnical and harsh ground conditions, and ore grade), which corrode revenues and increase business risks, including the possibility of regulatory non-compliance. The minerals industry holds a genuine gold mine of data that were collected for scientific, engineering, operational and other purposes. Data and data-centric workspaces that are targeted towards innovation and experimentation, which if combined with in-discipline expertise are two harmonious ingredients that can provide many practical solutions for the mining and mineral industries. In this paper, the concept, the opportunity and the necessity for a move towards a technology- and innovation-based, data-centric ‘dry laboratories’ (common workspaces that facilitates data-centric experimentation and innovation) in the minerals industry are assessed. We contend that the dry laboratory environment maximizes the value of data for the minerals industry. Toward the establishment of dry laboratories, we propose several essential components of a framework that would enable the functionality of dry laboratories in the minerals industry, while concomitantly examining the components from both academia and industry perspectives.}
}
@incollection{HOLLIN201514,
title = {Chapter 2 - Drilling into the Big Data Gold Mine: Data Fusion and High-Performance Analytics for Intelligence Professionals},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {14-20},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019672000021},
author = {Rupert Hollin},
keywords = {Big Data, Fusion, High-performance analytics, Visualization},
abstract = {Threats to local, national, and global public security are continually evolving, and for those tasked with preventing and responding to these threats, the amount of potentially useful data can often seem overwhelming. What compounds this Big Data issue is the fact that we are living in a time of global economic austerity in which national security and law enforcement agencies need to become better at exploiting information while managing the demands of ever-shrinking budgets. This chapter looks at how, by using the latest software tools and techniques for data fusion and high-performance analytics, agencies can automate traditional labor-intensive tasks, gain a holistic view of information that originates from multiple sources, and extract valuable intelligence in a timely and more efficient manner.}
}
@article{WONG201944,
title = {Artificial Intelligence for infectious disease Big Data Analytics},
journal = {Infection, Disease & Health},
volume = {24},
number = {1},
pages = {44-48},
year = {2019},
issn = {2468-0451},
doi = {https://doi.org/10.1016/j.idh.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468045118301445},
author = {Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang},
keywords = {Infectious diseases modelling, Emergency response, Artificial Intelligence, Machine learning},
abstract = {Background
Since the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.
Methods
This paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.
Results and Conclusion
It is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.}
}
@article{EZERINS2022105569,
title = {Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health},
journal = {Safety Science},
volume = {146},
pages = {105569},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105569},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004112},
author = {Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz},
keywords = {Safety analytics, Data analytics, Readiness assessment, Occupational health},
abstract = {Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.}
}
@incollection{BERMAN2018395,
title = {19 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {395-417},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00019-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000194},
author = {Jules J. Berman},
keywords = {Data Quality Act, Freedom of Information Act, FOIA, Limited Data Use Agreements, and Madey v. Duke, Tort, Patents, Intellectual property, Informed consent, Data ownership, Copyright, Infringement, Fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly: copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of a Big Data resource. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) obtaining the rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four issues will never go away.}
}
@article{AZEROUAL201850,
title = {Analyzing data quality issues in research information systems via data profiling},
journal = {International Journal of Information Management},
volume = {41},
pages = {50-56},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218300975},
author = {Otmane Azeroual and Gunter Saake and Eike Schallehn},
keywords = {Current research information systems, CRIS, Research information systems, RIS, Research information, Data sources, Data quality, Extraction transformation load, ETL, Data analysis, Data profiling, Science system, Standardization},
abstract = {The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.}
}
@article{KLEES202214,
title = {Building a smart database for predictive maintenance in already implemented manufacturing systems},
journal = {Procedia Computer Science},
volume = {204},
pages = {14-21},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007402},
author = {Marina Klees and Safa Evirgen},
keywords = {predictive maintenance, smart maintenance, big data analytics, Sensor Data Analytics},
abstract = {Predictive analytics methods have become increasingly important in Manufacturing Organization in the context of Smart Maintenance. Standardized process models for data mining already known to search existing data stocks for patterns, trends and correlations. Sensors are progressively implemented in production machines to create a database for data mining processes. But the risk of Big Data, thus the risk of low quality data is probably high. For an economic consideration, the amount of investment in new measurement technology and infrastructure should be assessed. Organizations are confronted with the challenge of how much they have to invest to obtain a meaningful database. For this reason, it is important to research which existing approaches support the development of a sufficient database for predictive maintenance in manufacturing systems and provide a methodical framework.}
}
@article{ELKASSAR2019483,
title = {Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {483-498},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315226},
author = {Abdul-Nasser El-Kassar and Sanjay Kumar Singh},
keywords = {Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance},
abstract = {Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.}
}
@incollection{VAIDYA2022409,
title = {Chapter 24 - Exploring performance and predictive analytics of agriculture data},
editor = {Ajith Abraham and Sujata Dash and Joel J.P.C. Rodrigues and Biswaranjan Acharya and Subhendu Kumar Pani},
booktitle = {AI, Edge and IoT-based Smart Agriculture},
publisher = {Academic Press},
pages = {409-436},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-823694-9},
doi = {https://doi.org/10.1016/B978-0-12-823694-9.00030-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823694900030X},
author = {Madhavi Vaidya and Shweta Katkar},
keywords = {Agriculture, Big data, Weka, J48, Talend, Crops, Analytics, Fertilizers, Prediction},
abstract = {The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.}
}
@article{SIRGO2018166,
title = {Validation of the ICU-DaMa tool for automatically extracting variables for minimum dataset and quality indicators: The importance of data quality assessment},
journal = {International Journal of Medical Informatics},
volume = {112},
pages = {166-172},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618300443},
author = {Gonzalo Sirgo and Federico Esteban and Josep Gómez and Gerard Moreno and Alejandro Rodríguez and Lluis Blanch and Juan José Guardiola and Rafael Gracia and Lluis {De Haro} and María Bodí},
keywords = {Electronic medical record, Quality indicators, Critical care, Information processing, Data quality, Verification},
abstract = {Background
Big data analytics promise insights into healthcare processes and management, improving outcomes while reducing costs. However, data quality is a major challenge for reliable results. Business process discovery techniques and an associated data model were used to develop data management tool, ICU-DaMa, for extracting variables essential for overseeing the quality of care in the intensive care unit (ICU).
Objective
To determine the feasibility of using ICU-DaMa to automatically extract variables for the minimum dataset and ICU quality indicators from the clinical information system (CIS).
Methods
The Wilcoxon signed-rank test and Fisher’s exact test were used to compare the values extracted from the CIS with ICU-DaMa for 25 variables from all patients attended in a polyvalent ICU during a two-month period against the gold standard of values manually extracted by two trained physicians. Discrepancies with the gold standard were classified into plausibility, conformance, and completeness errors.
Results
Data from 149 patients were included. Although there were no significant differences between the automatic method and the manual method, we detected differences in values for five variables, including one plausibility error and two conformance and completeness errors. Plausibility: 1) Sex, ICU-DaMa incorrectly classified one male patient as female (error generated by the Hospital’s Admissions Department). Conformance: 2) Reason for isolation, ICU-DaMa failed to detect a human error in which a professional misclassified a patient’s isolation. 3) Brain death, ICU-DaMa failed to detect another human error in which a professional likely entered two mutually exclusive values related to the death of the patient (brain death and controlled donation after circulatory death). Completeness: 4) Destination at ICU discharge, ICU-DaMa incorrectly classified two patients due to a professional failing to fill out the patient discharge form when thepatients died. 5) Length of continuous renal replacement therapy, data were missing for one patient because the CRRT device was not connected to the CIS.
Conclusions
Automatic generation of minimum dataset and ICU quality indicators using ICU-DaMa is feasible. The discrepancies were identified and can be corrected by improving CIS ergonomics, training healthcare professionals in the culture of the quality of information, and using tools for detecting and correcting data errors.}
}
@article{OLEARY2019113139,
title = {Technology life cycle and data quality: Action and triangulation},
journal = {Decision Support Systems},
volume = {126},
pages = {113139},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113139},
url = {https://www.sciencedirect.com/science/article/pii/S016792361930168X},
author = {Daniel E. O'Leary},
keywords = {Technology life cycle, Data quality, Non-stationary data, Hype cycle, Data biases, Data phase triangulation},
abstract = {Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.}
}
@article{KOTECHA2022e757,
title = {CODE-EHR best-practice framework for the use of structured electronic health-care records in clinical research},
journal = {The Lancet Digital Health},
volume = {4},
number = {10},
pages = {e757-e764},
year = {2022},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(22)00151-0},
url = {https://www.sciencedirect.com/science/article/pii/S2589750022001510},
author = {Dipak Kotecha and Folkert W Asselbergs and Stephan Achenbach and Stefan D Anker and Dan Atar and Colin Baigent and Amitava Banerjee and Birgit Beger and Gunnar Brobert and Barbara Casadei and Cinzia Ceccarelli and Martin R Cowie and Filippo Crea and Maureen Cronin and Spiros Denaxas and Andrea Derix and Donna Fitzsimons and Martin Fredriksson and Chris P Gale and Georgios V Gkoutos and Wim Goettsch and Harry Hemingway and Martin Ingvar and Adrian Jonas and Robert Kazmierski and Susanne Løgstrup and R Thomas Lumbers and Thomas F Lüscher and Paul McGreavy and Ileana L Piña and Lothar Roessig and Carl Steinbeisser and Mats Sundgren and Benoît Tyl and Ghislaine van Thiel and Kees van Bochove and Panos E Vardas and Tiago Villanueva and Marilena Vrana and Wim Weber and Franz Weidinger and Stephan Windecker and Angela Wood and Diederick E Grobbee},
abstract = {Summary
Big data is important to new developments in global clinical science that aim to improve the lives of patients. Technological advances have led to the regular use of structured electronic health-care records with the potential to address key deficits in clinical evidence that could improve patient care. The COVID-19 pandemic has shown this potential in big data and related analytics but has also revealed important limitations. Data verification, data validation, data privacy, and a mandate from the public to conduct research are important challenges to effective use of routine health-care data. The European Society of Cardiology and the BigData@Heart consortium have brought together a range of international stakeholders, including representation from patients, clinicians, scientists, regulators, journal editors, and industry members. In this Review, we propose the CODE-EHR minimum standards framework to be used by researchers and clinicians to improve the design of studies and enhance transparency of study methods. The CODE-EHR framework aims to develop robust and effective utilisation of health-care data for research purposes.}
}
@article{JEFFREYKUO2018120,
title = {Analyze the energy consumption characteristics and affecting factors of Taiwan's convenience stores-using the big data mining approach},
journal = {Energy and Buildings},
volume = {168},
pages = {120-136},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817334345},
author = {Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee},
keywords = {Convenience store, Data mining, Machine learning, Energy consumption characteristics, Energy consumption affecting factor},
abstract = {This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.}
}
@article{MARSDEN2018A1,
title = {Numerical data quality in IS research and the implications for replication},
journal = {Decision Support Systems},
volume = {115},
pages = {A1-A7},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618301647},
author = {James R. Marsden and David E. Pingry},
abstract = {We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research – a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.}
}
@article{EYOB201927,
title = {Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives},
journal = {International Journal of Surgery},
volume = {72},
pages = {27-32},
year = {2019},
note = {Endoscopic Surgery},
issn = {1743-9191},
doi = {https://doi.org/10.1016/j.ijsu.2019.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S174391911930189X},
author = {Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger},
keywords = {Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data},
abstract = {Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.}
}
@article{MACHADO2022263,
title = {Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures},
journal = {Procedia Computer Science},
volume = {196},
pages = {263-271},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022365},
author = {Inês Araújo Machado and Carlos Costa and Maribel Yasmina Santos},
keywords = {Big Data, Data Mesh, Data Architectures, Data Lake},
abstract = {Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.}
}
@article{OBRIEN2015442,
title = {‘Accounting’ for Data Quality in Enterprise Systems},
journal = {Procedia Computer Science},
volume = {64},
pages = {442-449},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.539},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915026745},
author = {Tony O’Brien},
keywords = {Data Quality, Enterprise Systems, Accounting Information Systems, ERP, SCM, CRM, Big Data},
abstract = {Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.}
}
@article{CHANG201656,
title = {A model to compare cloud and non-cloud storage of Big Data},
journal = {Future Generation Computer Systems},
volume = {57},
pages = {56-76},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003167},
author = {Victor Chang and Gary Wills},
keywords = {Organizational sustainability modeling (OSM), Comparison between Cloud and non-Cloud storage platforms, Real Cloud case studies, Data analysis and visualization},
abstract = {When comparing Cloud and non-Cloud Storage it can be difficult to ensure that the comparison is fair. In this paper we examine the process of setting up such a comparison and the metric used. Performance comparisons on Cloud and non-Cloud systems, deployed for biomedical scientists, have been conducted to identify improvements of efficiency and performance. Prior to the experiments, network latency, file size and job failures were identified as factors which degrade performance and experiments were conducted to understand their impacts. Organizational Sustainability Modeling (OSM) is used before, during and after the experiments to ensure fair comparisons are achieved. OSM defines the actual and expected execution time, risk control rates and is used to understand key outputs related to both Cloud and non-Cloud experiments. Forty experiments on both Cloud and non-Cloud systems were undertaken with two case studies. The first case study was focused on transferring and backing up 10,000 files of 1 GB each and the second case study was focused on transferring and backing up 1000 files 10 GB each. Results showed that first, the actual and expected execution time on the Cloud was lower than on the non-Cloud system. Second, there was more than 99% consistency between the actual and expected execution time on the Cloud while no comparable consistency was found on the non-Cloud system. Third, the improvement in efficiency was higher on the Cloud than the non-Cloud. OSM is the metric used to analyze the collected data and provided synthesis and insights to the data analysis and visualization of the two case studies.}
}
@incollection{REIS2020179,
title = {3.10 - Data Quality and Denoising: A Review☆},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {179-204},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.14874-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472148747},
author = {M.S. Reis and P.M. Saraiva and B.R. Bakshi},
keywords = {Bayesian estimation, Data quality, Data rectification, Filtering, Fourier analysis, Gaussian and non-Gaussian noise, Kalman filtering, Model-based denoising, Multiscale analysis, Off-line and online denoising, Outliers, Smoothing, Wavelet analysis, Wavelet thresholding, Windowed Fourier analysis},
abstract = {This article introduces the methods of Fourier and wavelet analysis for enhancing data quality in typical chemometric and process analytics applications. Fourier analysis has been popular for many decades but is best suited for enhancing signals where most features are localized in frequency. In contrast, wavelet analysis is appropriate for signals that contain features localized in both time and frequency domains. It also retains the benefits of Fourier analysis such as orthonormality and computational efficiency. Practical algorithms for off-line and on-line denoising are described and compared via simple examples. These algorithms can be used for off-line or on-line applications in order to mitigate the impact of additive Gaussian as well as non-Gaussian noise.}
}
@article{SAEZ2019104954,
title = {Guest editorial: Special issue in biomedical data quality assessment methods},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104954},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719309174},
author = {Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez}
}
@article{GANT2015S36,
title = {The importance of data quality to enhance the impact of omics sciences},
journal = {Toxicology Letters},
volume = {238},
number = {2, Supplement },
pages = {S36},
year = {2015},
note = {ABSTRACTS OF THE 51st Congress of the European Societies of Toxicology (EUROTOX)},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2015.08.097},
url = {https://www.sciencedirect.com/science/article/pii/S0378427415020378},
author = {T. Gant}
}
@article{STECKLER20151803,
title = {The preclinical data forum network: A new ECNP initiative to improve data quality and robustness for (preclinical) neuroscience},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {10},
pages = {1803-1807},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15001674},
author = {Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov},
keywords = {Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience, Pre-clinical},
abstract = {Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.}
}
@incollection{CELKO2014119,
title = {Chapter 9 - Big Data and Cloud Computing},
editor = {Joe Celko},
booktitle = {Joe Celko’s Complete Guide to NoSQL},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {119-128},
year = {2014},
isbn = {978-0-12-407192-6},
doi = {https://doi.org/10.1016/B978-0-12-407192-6.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124071926000091},
author = {Joe Celko},
keywords = {Forrester Research, V-list, cloud computing, Big Data, data mining},
abstract = {Big Data is largely a buzzword in IT right now. It was coined by Forrester Research to put a wrapper around existing database mining, data management, and other extensions of existing technology to the current hardware. The goal is to use mixed tools with larger volumes of several different forms of data being brought together under one roof. Along with this approach to data, we are also concerned with cloud computing, which is a public or private Internet network that replaces the tradition hardwired landlines within a company.}
}
@article{DEKHTIAR2018227,
title = {Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study},
journal = {Computers in Industry},
volume = {100},
pages = {227-243},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517305560},
author = {Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis},
keywords = {Deep learning, Machine learning, Computer vision, Product Lifecycle Management, Digital mock-up, Shape retrieval},
abstract = {With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.}
}