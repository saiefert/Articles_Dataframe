{"columns":["abstract","author","doi","year","title"],"index":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],"data":[["Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API\u2019s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms \u2013 shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.","Javier Garcia-Bernardo and Frank W. Takes","https:\/\/doi.org\/10.1016\/j.is.2017.10.005","2018","The effects of data quality on the analysis of corporate board interlock networks"],["The Interregional Travel Survey in Japan (formerly the Net Passenger Transportation Survey [NPTS]) still has some limitations. New data sources have recently emerged, e.g., massive data from web-based surveys (WEB) or collecting passive mobile phone data (MOBI). Using or not using these data sources have been questioned for data integration or model estimation and validation. Therefore, as an initial step, the data quality of new data sources was evaluated to identify the potential for data integration with NPTS or new data collection methods to replace NPTS. This study focused on finding out the similarities in travel patterns extracted from these data sources using a nonnegative matrix factorization method. This study found that origin\u2013destination pairs in the MOBI travel patterns were significantly different from those of NPTS and WEB, while there were some similarities between NPTS and WEB. However, some issues have been remaining and should be resolved in the future.","Canh Xuan Do and Makoto Tsukai and Akimasa Fujiwara","https:\/\/doi.org\/10.1016\/j.eastsj.2020.100018","2020","Data quality analysis of interregional travel demand: Extracting travel patterns using matrix decomposition"],["Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of Big Data as a resource suitable for its intended purposes. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four problems never seem to go away.","Jules J. Berman","https:\/\/doi.org\/10.1016\/B978-0-12-404576-7.00013-7","2013","Chapter 13 - Legalities"],["Without investing in exotic data modeling techniques, EDW teams can achieve fast delivery using \u201csurface solutions.\u201d Surface solutions allow developers to first solve business problems with data taken from landing areas and then steadily \u201cbackfill\u201d the DW\/BI reference architecture to provide progressively more complete and robust solutions. Teams can create surface solutions by leveraging shadow IT, using data virtualization, and tapping a big data platform. When leveraging shadow IT, the EDW team delivers progressively richer data sets to departmental staff members, who build their own temporary BI solutions using that information. The data virtualization strategy relies on a \u201csuperoptimizer\u201d that can create views across databases and data types, even including semistructured data as needed. The big data strategy employs a new category of products such as Hadoop\u2019s HDFS, MapReduce, and Hive to provide access to new data, whether it be very large, poorly structured, and\/or just unfamiliar to IT and the business users.","Ralph Hughes","https:\/\/doi.org\/10.1016\/B978-0-12-396464-9.00013-8","2016","Chapter 13 - Surface Solutions Using Data Virtualization and Big Data"],["This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, \u00c5re. By applying a Business Intelligence approach, the Destination Management Information System \u00c5re (DMIS-\u00c5re) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington\u2019s (2007) concept of the \u2018Learning Tourism Destination\u2019 and the \u2018Knowledge Destination Framework\u2019 introduced by H\u00f6pken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-\u00c5re can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like \u201cWeb-Navigation\u201d, \u201cBooking\u201d and \u201cFeedback\u201d. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists\u2019 on-site behaviour at tourism destinations.","Matthias Fuchs and Wolfram H\u00f6pken and Maria Lexhagen","https:\/\/doi.org\/10.1016\/j.jdmm.2014.08.002","2014","Big data analytics for knowledge generation in tourism destinations \u2013 A case from Sweden"],["Background\nData from large electronic databases are increasingly used in epidemiological research, but golden standards for database validation remain elusive. The Prescription Registry (IPR) and the National Health Service (NHS) databases in Iceland have not undergone formal validation, and gross errors have repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new amphetamine tablets entered the Icelandic market, but were withdrawn half a year later due to being substandard. Return of unused stocks provided knowledge of the exact number of tablets used and hence a case where quality of the data could be assessed.\nObjective\nA case study of the quality of statistics in a national database on pharmaceuticals.\nMethods\nData on the sales of the substandard amphetamine were obtained from the Prescription Registry and the pharmaceuticals statistics database. Upon the revelation of discrepancies, explanations were sought from the respective institutions, the producer, and dose dispensing companies.\nResults\nThe substandard amphetamine was available from 1.9.2015 until 15.3.2016. According to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially stated 82860 tablets to have been sold, correcting to 74796 upon being notified about errors. The producer stated 72811 tablets to have been sold, and agreed with the dose dispensing companies on sales to those. The producer\u2019s numbers were confirmed by the Medicines Agency.\nConclusion\nOver-registration in the IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This case provided a unique opportunity for external validation of sales data for pharmaceuticals in Iceland, revealing enormous quality problems. The case has implications regarding database integrity beyond Iceland.","Ingunn Bj\u00f6rnsdottir and Guri Birgitte Verne","https:\/\/doi.org\/10.1016\/j.sapharm.2018.02.009","2018","Exhibiting caution with use of big data: The case of amphetamine in Iceland's prescription registry"],["ABSTRACT\nIntroduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET\/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 \u00b1 0.01, accuracy for SNR: 0.69 \u00b1 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.","Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese","https:\/\/doi.org\/10.1016\/j.cmpb.2021.106239","2021","Automated Data Quality Control in FDOPA brain PET Imaging using Deep Learning"],["In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I\/O).","Oscar Romero and Victor Herrero and Alberto Abell\u00f3 and Jaume Ferrarons","https:\/\/doi.org\/10.1016\/j.is.2014.09.005","2015","Tuning small analytics on Big Data: Data partitioning and secondary indexes in the Hadoop ecosystem"],["Systemic phenotyping of mutant mice has been established at large scale in the last decade as a new tool to uncover the relations between genotype, phenotype and environment. Recent advances in that field led to the generation of a valuable open access data resource that can be used to better understanding the underlying causes for human diseases. From an ethical perspective, systemic phenotyping significantly contributes to the reduction of experimental animals and the refinement of animal experiments by enforcing standardisation efforts. There are particular logistical, experimental and analytical challenges of systemic large-scale mouse phenotyping. On all levels, IT solutions are critical to implement and efficiently support breeding, phenotyping and data analysis processes that lead to the generation of high-quality systemic phenotyping data accessible for the scientific community.","Holger Maier and Stefanie Leuchtenberger and Helmut Fuchs and Valerie Gailus-Durner and Martin {Hrabe de Angelis}","https:\/\/doi.org\/10.1016\/j.coisb.2017.07.012","2017","Big data in large-scale systemic mouse phenotyping"],["After its people, data is arguably an organisation's most valuable asset. According to recent figures by the Networked Systems and Services department at SINTEF, some 90% of all the data in the world has been created in the past two years. That shouldn't be too much of a surprise to us given the data-driven world we now live in; but what is promising is that companies are increasingly switching on to its strategic and commercial value. The challenge is how do we extract value from this data in a way that empowers people and organisations alike?","Boris Huard","https:\/\/doi.org\/10.1016\/S1353-4858(15)30051-9","2015","The data quality paradox"],["Background and Objectives\nData Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.\nMaterials and Methods\nAs part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets \u2013 the PI data set containing 8.8\u202fM patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.\nResults\nEleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n\u202f=\u202f6), conformance (n\u202f=\u202f3) and plausibility (n\u202f=\u202f2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.\nConclusion\nDQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.","Christel Daniel and Patricia Serre and Nina Orlova and St\u00e9phane Br\u00e9ant and Nicolas Paris and Nicolas Griffon","https:\/\/doi.org\/10.1016\/j.cmpb.2018.10.016","2019","Initializing a hospital-wide data quality program. The AP-HP experience."],["Background and Objectives\nData curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.\nMethods\nA web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.\nResults\nIt has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with\u202f>\u202f75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (\u223c\u202f+\u202f1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (>\u202f70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.\nConclusions\nTAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.","Roberto {\u00c1lvarez S\u00e1nchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin","https:\/\/doi.org\/10.1016\/j.cmpb.2018.12.029","2019","TAQIH, a tool for tabular data quality assessment and improvement in the context of health data"],["Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship\u2019s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship\u2019s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship\u2019s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.","Khanh Q. Bui and Lokukaluge P. Perera","https:\/\/doi.org\/10.1016\/j.oceaneng.2021.109392","2021","Advanced data analytics for ship performance monitoring under localized operational conditions"],["During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasets The main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different parts of the standard like the development of a data dictionary with master data terms; a communication protocol; an API to manage the master data messages; and the algorithms in MapReduce to measure the data quality.","Bibiano Rivas and Jorge Merino and Ismael Caballero and Manuel Serrano and Mario Piattini","https:\/\/doi.org\/10.1016\/j.csi.2016.10.004","2017","Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets"],["Data quality management, especially data cleansing, has been extensively studied for many years in the areas of data management and visual analytics. In the paper, we first review and explore the relevant work from the research areas of data management, visual analytics and human-computer interaction. Then for different types of data such as multimedia data, textual data, trajectory data, and graph data, we summarize the common methods for improving data quality by leveraging data cleansing techniques at different analysis stages. Based on a thorough analysis, we propose a general visual analytics framework for interactively cleansing data. Finally, the challenges and opportunities are analyzed and discussed in the context of data and humans.","Shixia Liu and Gennady Andrienko and Yingcai Wu and Nan Cao and Liu Jiang and Conglei Shi and Yu-Shuen Wang and Seokhee Hong","https:\/\/doi.org\/10.1016\/j.visinf.2018.12.001","2018","Steering data quality with visual analytics: The complexity challenge"],["Social Media (SM) can be a complementary channel of information to other official means for the health data collection such as the epidemiological surveillance activities and control carried out by health authorities. For this reason, more and more organizations, professionals, and scientific institutions are seeing the need to make the most of resources of health information based on SM platforms through the use of Big Data tools and analytics. Although there is a consensus on the potential benefits and opportunities that SM may provide when used for healthcare purposes, its use has brought unsuspected drawbacks and challenges related to the protection of personal data, it is essential to promote a wide reflection and that the authorities and governments establish, in collaboration with patients associations and professional institutions, specific ethical, legal guidelines, and use policies to the benefit of the current and future healthcare professional\u2013patient relationship and general public.","M.A. Mayer and L. Fern\u00e1ndez-Luque and A. Leis","https:\/\/doi.org\/10.1016\/B978-0-12-809269-9.00005-0","2016","Chapter 5 - Big Data For Health Through Social Media"],["There are unique challenges in managing data collection and management from instruments in the field in general. These issues become extreme when \u201cin the field\u201d means \u201cin a plane over the Antarctic\u201d. In this paper we present the design and function of the Forward Observer a computer cluster and data analysis system that flies in a plane in the Arctic and Antarctic to collect, analyze in real time, and store Synthetic Aperture Radar (SAR) data. SAR is used to analyze the thickness and structure of polar ice sheets. We also discuss the processing of data once it is returned to the continental US and made available via data grids. The needs for in-flight data analysis and storage in the Antarctic and Arctic are highly unusual, and we have developed a novel system to meet those needs. We describe the constraints and requirements that led to the creation of this system and the general functionality which it applies to any instrument. We discuss the main means for handling replication and creating checksum information to ensure that data collected in polar regions are returned safely to mainland US for analysis. So far, not a single byte of data collected in the field has failed to make it home to the US for analysis (although many particular data storage devices have failed or been damaged due to the challenges of the extreme environments in which this system is used). While the Forward Observer system is developed for the extreme situation of data management in the field in the Antarctic, the technology and solutions we have developed are applicable and potentially usable in many situations where researchers wish to do real time data management in the field in areas that are constrained in terms of electrical supply.","Richard Knepper and Matthew Standish","https:\/\/doi.org\/10.1016\/j.future.2017.05.031","2017","Forward Observer system for radar data workflows: Big data management in the field"],["Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.","Lisa C. G\u00fcnther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer","https:\/\/doi.org\/10.1016\/j.promfg.2019.02.114","2019","Data quality assessment for improved decision-making: a methodology for small and medium-sized enterprises"],["This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.","Wen-Long Shang and Haoran Zhang and Yi Sui","https:\/\/doi.org\/10.1016\/B978-0-323-90169-7.00008-7","2022","Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS)"],["A traditional power grid, also known as the legacy grid, collects data at a few locations on the grid to monitor grid performance and forecast energy requirements on a macro level. A smart grid is the next generation of the electric power grid; it includes technologies for real-time data acquisition from various sections of the grid and provides a means for two-way communication between energy suppliers and consumers. Compared to a legacy grid, the smart grid generates large volumes of data that can be exploited for power quality event monitoring, intrusion detection, islanding detection, price forecasting, and energy forecasting at a much more granular level. These large volumes of data have to be analyzed in real-time and with high accuracy in order assist in decision making for power system operations and optimal power flow. This poses a big data challenge, which to be implemented successfully requires changes in infrastructure and data analysis methods. This chapter describes the smart grid and its associated big data and discusses methods for informative feature extraction from raw data, event monitoring, and energy consumption forecasting using these features and visualization methods to assist with data interpretation and decision making.","Sonal K. Phan and Cathy Chen","https:\/\/doi.org\/10.1016\/B978-0-12-805321-8.00009-4","2017","9 - Big Data and Monitoring the Grid"],[null,"James R. Marsden and David E. Pingry and Jason B. Thatcher","https:\/\/doi.org\/10.1016\/j.dss.2019.113172","2019","Perspectives on numerical data quality in IS research"],["Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.","Ewa Szyma\u0144ska","https:\/\/doi.org\/10.1016\/j.aca.2018.05.038","2018","Modern data science for analytical chemical data \u2013 A comprehensive review"],["The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and\/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.","Laura Mor\u00e1n-Fern\u00e1ndez and Ver\u00f3nica B\u00f3lon-Canedo and Amparo Alonso-Betanzos","https:\/\/doi.org\/10.1016\/j.neucom.2021.05.107","2022","How important is data quality? Best classifiers vs best features"],["Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5\u202fmin of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.","Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary","https:\/\/doi.org\/10.1016\/j.techfore.2018.04.031","2019","Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation"],["This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage\u2014cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.","V.N. Gudivada and M.T. Irfan and E. Fathi and D.L. Rao","https:\/\/doi.org\/10.1016\/bs.host.2016.07.010","2016","Chapter 5 - Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning"],["The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.","Matteo Francia and Enrico Gallinucci and Matteo Golfarelli and Anna Giulia Leoni and Stefano Rizzi and Nicola Santolini","https:\/\/doi.org\/10.1016\/j.future.2021.06.031","2021","Making data platforms smarter with MOSES"],["In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.","Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel","https:\/\/doi.org\/10.1016\/j.jnca.2016.08.002","2016","Data quality in internet of things: A state-of-the-art survey"],["Over the past few decades, data-driven machine learning (ML) has distinguished itself from hypothesis-driven studies and has recently received much attention in environmental toxicology. However, the use of ML in environmental toxicology remains in the early stages, with knowledge gaps, technical bottlenecks in data quality, high-dimensional\/heterogeneous\/small-sample data analysis and model interpretability, and a lack of an in-depth understanding of environmental toxicology. Given the above problems, we review the recent progress in the literature and highlight state-of-the-art toxicological studies using ML (such as learning and predicting toxicity in complicated biosystems and multiple-factor environmental scenarios of long-term and large-scale pollution). Beyond predicting simple biological endpoints by integrating untargeted omics and adverse outcome pathways, ML development should focus on revealing toxicological mechanisms. The integration of data-driven ML with other methods (e.g., omics analysis and adverse outcome pathway frameworks) endows ML with widely promising application in revealing toxicological mechanisms. High-quality databases and interpretable algorithms are urgently needed for toxicology and environmental science. Addressing the core issues and future challenges for ML in this review may narrow the knowledge gap between environmental toxicity and computational science and facilitate the control of environmental risk in the future.","Xiaotong Wu and Qixing Zhou and Li Mu and Xiangang Hu","https:\/\/doi.org\/10.1016\/j.jhazmat.2022.129487","2022","Machine learning in the identification, prediction and exploration of environmental toxicology: Challenges and perspectives"],["Owing to the growing increase in the world\u2019s ageing population, research has focused on developing information and communication technology (ICT)\u2013based services for personalized care, improved health, and quality social life for the elderly. Recent efforts explore Big Data in order to build mathematical models of personal behavior and lifestyle for analytics. Leveraging Big Data analytics holds enormous potential for solving some of the biggest and most intractable challenges in personalized elderly care through quantified modeling of a person\u2019s lifestyle in a way that takes cognizance of their beliefs, values, and preferences, and connects to a history of events, things, and places around which they have progressively built their lives. However, the idea of discovering patterns to personalize care and inform critical health care decisions for the elderly is challenged as data grow exponentially in volume, become faster and increasingly unstructured, and are generated from sociodigital engagements that often may not accurately reflect the real-world entities and contexts they represent. As a result, the idea raises issues along several dimensions, including social, technical, and context-aware challenges. In this chapter, we present an overview of the state of the art in personalized elderly care, and explore the opportunities and inherent sociotechnical challenges in leveraging Big Data analytics to support elderly care and independent living. Based on this discussion, and arguing that analytics need to take account of the contexts that shape the generation and use of data, ACTVAGE, a context-aware lifestyle-oriented framework for personalized elderly care and independent living is proposed.","Obinna Anya and Hissam Tawfik","https:\/\/doi.org\/10.1016\/B978-0-12-803468-2.00005-9","2016","Chapter 5 - Leveraging Big Data Analytics for Personalized Elderly Care: Opportunities and\u00a0Challenges"],["The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g\u2013100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.","Tomer Gueta and Yohay Carmel","https:\/\/doi.org\/10.1016\/j.ecoinf.2016.06.001","2016","Quantifying the value of user-level data cleaning for big data: A case study using mammal distribution models"],["Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters\u2019 origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing\u2019s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.","Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou","https:\/\/doi.org\/10.1016\/j.trc.2015.06.007","2015","Traffic zone division based on big data from mobile phone base stations"],["For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.","Samah Salem and Fouzia Benchikha","https:\/\/doi.org\/10.1016\/j.jksuci.2021.06.001","2022","LODQuMa: A Free-ontology process for Linked (Open) Data quality management"],["Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree\u2019s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.","Dominic Evangelista and France Thouz\u00e9 and Manpreet Kaur Kohli and Philippe Lopez and Fr\u00e9d\u00e9ric Legendre","https:\/\/doi.org\/10.1016\/j.ympev.2018.05.007","2018","Topological support and data quality can only be assessed through multiple tests in reviewing Blattodea phylogeny"],["Focus\nThe transformative power of today's big data (BD) has allowed many companies, i.e., decision-makers, to evolve at an unprecedented pace. With regard to decision-making, artificial intelligence (AI) takes task delegation to a new level, and by employing AI-assisted tools, companies can provide their HR departments with the means to manage the existing data and HR altogether.\nObjectives\nTo determine how HR managers assess whether BD management is facilitated by AI, and how they frame the changes necessary to meet the trends related to AI and its implementation, namely their willingness to master its implementation and to meet the possible challenges.\nMethodology\nContent analysis was conducted on interviews held with a sample of 16 HR practitioners from a spectrum of areas, and the findings were analysed using the big data maturity model (BDMM) framework. Domains covered by this model allow the study of decision-making trends, in terms of preparedness and willingness to tackle disruptive technology with the aim of improving and gaining the competitive edge in decision-making.\nFindings\nThe central potential of AI lies in faster data storage and processing power, thereby leading to more insightful and effective decision-making. This article contains closer insights into the challenges underlying the implementation of AI in decision-making processes, specifically in terms of strategic alignment, governance, and implementation. The results reflect the notions regarding the nature of AI \u2013 in assisting HR \u2013 and lay out the path that precedes the extraction of BD, through the delivery of advantageous intelligence, to augment decision-making in HR.","Aleksandar Radonji\u0107 and Henrique Duarte and N\u00e1dia Pereira","https:\/\/doi.org\/10.1016\/j.emj.2022.07.001","2022","Artificial intelligence and HRM: HR managers\u2019 perspective on decisiveness and challenges"],["We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.","Antonio Kolossa and Bruno Kopp","https:\/\/doi.org\/10.1016\/j.neuroimage.2018.01.005","2018","Data quality over data quantity in computational cognitive neuroscience"],["We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.","Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka","https:\/\/doi.org\/10.1016\/j.regsciurbeco.2017.06.006","2017","Residential land values in the Washington, DC metro area: New insights from big data"],["This chapter provides a discussion of the new International Organization for Standardization (ISO) standards related to the exchange of master data. It includes an in-depth look at the ISO 8000 family of standards, including ISO 8000-110, -120, -130, and -140, and their relationship to the ISO 22745-10, -30, and -40 standards. Also an explanation is given of simple versus strong ISO 8000-110 compliance, and the value proposition for ISO 8000 compliance is discussed.","John R. Talburt and Yinle Zhou","https:\/\/doi.org\/10.1016\/B978-0-12-800537-8.00011-9","2015","Chapter 11 - ISO Data Quality Standards for Master Data"],["We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.","Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner","https:\/\/doi.org\/10.1016\/j.dss.2018.03.011","2018","Assessing data quality \u2013 A probability-based metric for semantic consistency"],["Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to \"dirty data\" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.","Karl St\u00f6ger and David Schneeberger and Peter Kieseberg and Andreas Holzinger","https:\/\/doi.org\/10.1016\/j.clsr.2021.105587","2021","Legal aspects of data cleansing in medical AI"],["Scientific literature about building occupants\u2019 behaviour and the related energy performance analyses document about several strategies to monitor window operation, including different sensors and data series lengths. In this framework, the primary goal of this study is to propose effective guidelines for minimum experiment durations and their reliability. A six-year-long database from a living laboratory was used as a benchmark; and a recursive strategy enabled to split it into more than 2,500 subsets, supporting two main steps. First, information theory concepts were used to calculate uncertainty and subsets\u2019 divergence were compared to the full database. Second, the subsets were used to train deep neural networks and evaluate the influence of monitoring lengths combined with different kinds of environmental data (i.e. indoor or outdoor). From the information-theoretic metrics, the results support that indoor-related variables can reduce most of the uncertainty related to window operation. Besides, subsets influenced by autumn and winter diverge the most compared to the full database. Considering the modelling approach, the results demonstrated that by including indoor-related variables, higher shares of reliably-performing models were achieved, and smaller subsets were needed. Seasonality has also played a major role along these lines. As a consequence, the conclusions supported the feasibility of nine-month-long field studies, starting in summer or spring, when indoor and outdoor variables are monitored.","Mateus Bavaresco and Ioannis Kousis and Ilaria Pigliautile and Anna {Laura Pisello} and Cristina Piselli and Enedir Ghisi","https:\/\/doi.org\/10.1016\/j.enbuild.2022.112197","2022","Are years-long field studies about window operation efficient? a data-driven approach based on information theory and deep learning"],["Today\u2019s largest and fastest growing companies\u2019 assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises\u2019 daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily \u2013 and in real-time \u2013 assess\/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.","Manik Madhikermi and Sylvain Kubler and J\u00e9r\u00e9my Robert and Andrea Buda and Kary Fr\u00e4mling","https:\/\/doi.org\/10.1016\/j.eswa.2016.06.043","2016","Data quality assessment of maintenance reporting procedures"],[null,"Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami","https:\/\/doi.org\/10.1016\/j.accpm.2018.12.015","2019","Data quality and blockchain technology"],["With the increase in the energy consumption of air-conditioning (AC) systems in Chinese residential buildings, the realization of energy savings in AC systems has attracted increasing attention. The variable refrigerant flow (VRF) system is a common AC system for residential buildings in China. In most previous studies on VRF systems, onsite measurements or surveys were conducted to collect operational data. These traditional methods may face various data issues, such as limited sample sizes and invalid data, making them unable to capture the spatial and temporal performance features of VRF systems in residential buildings on a large scale. To fill this gap, with advances in data storage and transmission technology, Big Data methods have been widely used for data collection. In the present study, researchers adopted 16,985 sets of VRF system operation data from China as the database and conducted data analysis for both the spatial and temporal dimensions. Several key indicators were proposed from the two perspectives (spatial and temporal), including the part-space index (PSI), load ratio (LR), use duration (UD), and cooling energy consumption. The main findings were as follows: (1) The \u201cpart-time part-space\u201d operation mode of residential VRF systems can be analyzed according to the statistical results of the UD and PSI. (2) An LR of <30% is the main operating condition for VRF systems in residential buildings. (3) Extracted typical LR patterns can reflect different user behavior. The statistical results obtained in this study provide a basis for VRF engineering projects.","Hua Liu and Yi Wu and Da Yan and Shan Hu and Mingyang Qian","https:\/\/doi.org\/10.1016\/j.jobe.2022.105219","2022","Investigation of VRF system cooling operation and performance in residential buildings based on large-scale dataset"],["Obtaining accurate speed and travel time information is a challenge for researchers, geographers, and transportation agencies. In the past, traffic data were usually acquired and disseminated by government agencies through fixed-location sensors. High costs, infrastructure demands, and low coverage levels of these sensor devices require agencies and researchers to look beyond the traditional approaches. With the emergence of smartphones and navigation apps, location-based and crowdsourced Big Data are receiving increased attention. In this regard, location-based big data (LocBigData) collected from probe vehicles and road users can be used to provide speed and travel time information in different locations. Examining the quality of crowdsourced data is essential for researchers and agencies before using them. This study assessed the quality of Waze speed data from surface streets and conducted a case study in Sevierville, Tennessee. Typically, examining the quality of these data in surface streets and arterials is more challenging than freeways data. This research used Bluetooth speed data as the ground truth, which is independent of Waze data. In this study, three steps of methodology were used. In the first step, Waze speed data was compared to Bluetooth data in terms of accuracy, mean difference, and distribution similarity. In the second step, a k-means algorithm was used to categorize Waze data quality, and a multinomial logistics regression model was performed to explore the significant factors that impact data quality. Finally, in the third step, machine learning techniques were conducted to predict the data quality in different conditions. The result of the comparison showed a similar pattern and a slight difference between datasets, which verified the quality of Waze speed data. The statistical model indicates that that Waze speed data are more accurate in peak hours than in night hours. Also, the traffic speed, traffic volume, and segment length have a significant association on the accuracy of Waze data on surface streets. Finally, the result of machine learning prediction showed that a KNN method performed the highest prediction accuracy of 84.5% and 82.9% of the time for training and test datasets, respectively. Overall, the study results suggest that Waze speed data is a promising data source for surface streets.","Nima Hoseinzadeh and Yuandong Liu and Lee D. Han and Candace Brakewood and Amin Mohammadnazar","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2020.101518","2020","Quality of location-based crowdsourced speed data on surface streets: A case study of Waze and Bluetooth speed data in Sevierville, TN"],["The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this \u2018big denominator\u2019 capability, especially given the needs of precision medicine research.","Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra","https:\/\/doi.org\/10.1016\/j.drudis.2017.12.002","2018","Real world big data for clinical research and drug development"],["The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.","Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar","https:\/\/doi.org\/10.1016\/j.compind.2022.103624","2022","A survey on smart farming data, applications and techniques"],[null,"Laura Sebastian-Coleman","https:\/\/doi.org\/10.1016\/B978-0-12-397033-6.00014-6","2013","Chapter 13 - Directives for Data Quality Strategy"],["Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.","Yahan Yang and Ruiyang Li and Yifan Xiang and Duoru Lin and Anqi Yan and Wenben Chen and Zhongwen Li and Weiyi Lai and Xiaohang Wu and Cheng Wan and Wei Bai and Xiucheng Huang and Qiang Li and Wenrui Deng and Xiyang Liu and Yucong Lin and Pisong Yan and Haotian Lin","https:\/\/doi.org\/10.1016\/j.imed.2021.11.002","2021","Standardization of collection, storage, annotation, and management of data related to medical artificial intelligence"],["An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.","Weitian Tong","https:\/\/doi.org\/10.1016\/B978-0-12-815822-7.00005-4","2020","Chapter 5 - Machine learning for spatiotemporal big data in air pollution"],["Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.","Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich","https:\/\/doi.org\/10.1016\/j.neuroimage.2017.06.077","2018","Discovering dynamic brain networks from big data in rest and task"],["Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.","Shazia Sadiq and Marta Indulska","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2017.01.003","2017","Open data: Quality over quantity"],["This chapter describes how a distributed processing environment such as Hadoop Map\/Reduce can be used to support the CSRUD Life Cycle for Big Data. The examples shown in this chapter use the match key blocking described in Chapter 9 as a data partitioning strategy to perform ER on large datasets. The chapter includes an algorithm for finding the transitive closure of multiple match keys in a distributed processing environment using an iterative algorithm that minimizes the amount of local memory required for each processor. It also outlines a structure for an identity knowledge base in a distributed key-value data store, and describes strategies and distributed processing workflows for capture and update phases of the CSRUD life cycle using both record-based and attribute-based cluster-to-cluster structure projections.","John R. Talburt and Yinle Zhou","https:\/\/doi.org\/10.1016\/B978-0-12-800537-8.00010-7","2015","Chapter 10 - CSRUD for Big Data"],["Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image\/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.","J.W. Wang and M. Williams","https:\/\/doi.org\/10.1016\/j.clon.2021.11.040","2022","Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care"],["Objective\nTo identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).\nMaterials and Methods\nWe leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000\u202f+\u202fpatients, and 131 million laboratory test results).\nResults\nWe identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.\nDiscussion and conclusion\nour method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).","Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and La\u00ebtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance","https:\/\/doi.org\/10.1016\/j.cmpb.2018.12.030","2019","What can millions of laboratory test results tell us about the temporal aspect of data quality? Study of data spanning 17 years in a clinical data warehouse"],["Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.","Ren\u00e9 Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg","https:\/\/doi.org\/10.1016\/j.apenergy.2017.10.041","2017","Big data GIS analysis for novel approaches in building stock modelling"],["Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.","A. Bronselaer and J. Nielandt and G. {De Tr\u00e9}","https:\/\/doi.org\/10.1016\/j.ijar.2018.03.007","2018","An incremental approach for data quality measurement with insufficient information"],["Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24\u00a0h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.","Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto","https:\/\/doi.org\/10.1016\/j.knosys.2015.10.028","2016","Traffic big data prediction and visualization using Fast Incremental Model Trees-Drift Detection (FIMT-DD)"],["This chapter discusses the real-life implementation of the next-generation platform by three different companies and the direction they each have chosen from a technology and architecture perspective.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-405891-0.00014-3","2013","Chapter 14 - Implementing the Big Data \u2013 Data Warehouse \u2013 Real-Life Situations"],["An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.","Ibrahim Alghamdi and Christos Anagnostopoulos and Dimitrios P. Pezaros","https:\/\/doi.org\/10.1016\/j.future.2020.12.017","2021","Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach"],["Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30\u201320:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.","Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng","https:\/\/doi.org\/10.1016\/j.habitatint.2017.04.007","2017","A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning"],["Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to \u2018talk to\u2019 each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.","Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava","https:\/\/doi.org\/10.1016\/j.scs.2018.02.015","2018","Data quality and governance in a UK social housing initiative: Implications for smart sustainable cities"],[null,"April Reeve","https:\/\/doi.org\/10.1016\/B978-0-12-397167-8.00021-2","2013","Chapter 21 - Big Data Integration"],["This paper focuses on techniques for dealing with imperfect data in a frame of early warning system (EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing values, met-ocean simulation systems have to deal with them to provide data transaction between models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control of simulated fields, statistical control and restoration of measurements and control using alternative models. Domain specific quality control was presented as two types of procedures based on theoretically proved methods were applied. The first procedure is based on probabilistic model of dynamical system, where processes are spatially interrelated and could be implemented in a form of multivariate regression (MRM). The second procedure is based on principal component analysis extended for taking into account temporal relations in data set (ePCA).","Jose Luis Araya Lopez and Anna V. Kalyuzhnaya and Sergey S. Kosukhin and Sergey V. Ivanov","https:\/\/doi.org\/10.1016\/j.procs.2016.05.532","2016","Data Quality Control for St. Petersburg Flood Warning System"],["We might as well not do any data storage if we are not storing and passing high quality data. This chapter defines data quality and a program to maintain high standards throughout the enterprise.","William McKnight","https:\/\/doi.org\/10.1016\/B978-0-12-408056-0.00004-7","2014","Chapter Four - Data Quality: Passing the Standard"],["Mobility is the main key for smart living, where navigation and automatic suggestions are also a strategy for a successful life in smart cities. Big Data analytics are behind urban changes in the mobility of smart cities to bring sustainable life. By the year 2025 all over Indian states can reach the expected lifestyle by providing high security and mobility which can grow the opportunities also high. As the population is rapidly increasing, the needs of people are also increasing such that necessitating real-time apps for daily needs, communication devices, and so on. We focus our idea on the benefits of traffic and safety measures which are becoming a huge challenge nowadays. Many are preferred with sophistication when traveling for short distances. In such a way the big data analytics tools R studio and weka are used on the dataset smart city from the Kaggle website for traffic patterns during the high traffic duration. Using the dataset, the data are classified using a Support vector machine (SVM) and applied with regression such as linear, logistic regression to find the accuracy of traffic peak situations. The proposed work aims to compare the efficiency of big data technologies which can be applied using various classification and regression that can be shown on various tools such as R, Weka, map-reduce which can produce accurate results to visualize the smart cities and their traffic analysis.","T. Devi and K. Alice and N. Deepa","https:\/\/doi.org\/10.1016\/j.matpr.2022.03.722","2022","Traffic management in smart cities using support vector machine for predicting the accuracy during peak traffic conditions"],["Surgeons, neurologists, researchers, and patients have lacked the technology-based tools to facilitate sharing the tremendously valuable data about patients\u2019 treatment and research in regard to what is working and what is not. Today, only 9% of patients who could benefit from complex therapies to address neurologic conditions actually receive them, and the medical information for each patient who does is hidden away in disconnected databases. To optimize and accelerate our understanding of the brain, we need to gather intelligence around every case, every research subject, every study while connecting that information through a unified, Health Insurance Portability and Accountability Act of 1996 (HIPAA)-compliant network that leverages technology and harnesses the Internet to drive advancements and better connect patients to their care teams. In this chapter, we highlight the key aspects needed to fulfill the requirements of a robust, HIPAA-compliant archive for brain data and highlight the impact of normalization on the accuracy of statistical analyses.","Pierre-Francois D\u2019Haese and Peter E. Konrad and Benoit M. Dawant","https:\/\/doi.org\/10.1016\/B978-0-12-805353-9.00013-9","2018","Chapter 13 - Big Data and Deep Brain Stimulation"],["The United Nations 2030 Agenda for Sustainable Development provides an important framework for economic, social, and environmental action. A comprehensive indicator system to aid in the systematic implementation and monitoring of progress toward the Sustainable Development Goals (SDGs) is unfortunately limited in many countries due to lack of data. The availability of a growing amount of multi-source data and rapid advancements in big data methods and infrastructure provide unique opportunities to mitigate these data shortages and develop innovative methodologies for comparatively monitoring SDGs. Big Earth Data, a special class of big data with spatial attributes, holds tremendous potential to facilitate science, technology, and innovation toward implementing SDGs around the world. Several programs and initiatives in China have invested in Big Earth Data infrastructure and capabilities, and have successfully carried out case studies to demonstrate their utility in sustainability science. This paper presents implementations of Big Earth Data in evaluating SDG indicators, including the development of new algorithms, indicator expansion (for SDG 11.4.1) and indicator extension (for SDG 11.3.1), introduction of a biodiversity risk index as a more effective analysis method for SDG 15.5.1, and several new high-quality data products, such as global net ecosystem productivity, high-resolution global mountain green cover index, and endangered species richness. These innovations are used to present a comprehensive analysis of SDGs 2, 6, 11, 13, 14, and 15 from 2010 to 2020 in China utilizing Big Earth Data, concluding that all six SDGs are on schedule to be achieved by 2030.","Huadong Guo and Dong Liang and Zhongchang Sun and Fang Chen and Xinyuan Wang and Junsheng Li and Li Zhu and Jinhu Bian and Yanqiang Wei and Lei Huang and Yu Chen and Dailiang Peng and Xiaosong Li and Shanlong Lu and Jie Liu and Zeeshan Shirazi","https:\/\/doi.org\/10.1016\/j.scib.2022.07.015","2022","Measuring and evaluating SDG indicators with Big Earth Data"],["Service-oriented manufacturing (SOM) is a new worldwide manufacturing paradigm, and a cyber-physical system (CPS) is accepted as a strategic choice of SOM enterprises looking to provide bundles of satisfying products and services to customers. The issue of data quality is common in any CPS and poses great challenges to its efficient operation. This paper focuses on defective data generated by the improper operation of physical and cyber components of a service-oriented manufacturing CPS (SMCPS), and develops effective managerial policies to deal with such data. First, formal semantics of workflow nets (WF-nets) are employed to construct process-oriented ontology for the SMCPS. Second, a two-stage optimization model together with algorithms is designed to find optimal policies that balance local and global management objectives. Finally, our model is illustrated through a case. Results show that the proposed control strategy outperforms one-stage control and random control in guaranteeing data quality and saving control costs.","Zhiting Song and Yanming Sun and Jiafu Wan and Peipei Liang","https:\/\/doi.org\/10.1016\/j.compeleceng.2016.08.010","2017","Data quality management for service-oriented manufacturing cyber-physical systems"],["Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.","Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter","https:\/\/doi.org\/10.1016\/j.apenergy.2014.11.042","2015","Big-data for building energy performance: Lessons from assembling a very large national database of building energy use"],["This chapter explains the concept of big data and shows that the analytical data and related meta-data of metabolic phenotyping experiments fall into this category. The various databases that can be used to aid interpretation of such data, comprising general chemical and biochemical data, biochemical pathway specific data, analytical chemistry data to aid metabolite identification, and finally metabolic results, are discussed. The concept of data warehousing is explored in the context of metabolic data sets. Finally, the challenges for successful data storage, data exchange, and data interpretation are discussed.","Nadine Levin and Reza M. Salek and Christoph Steinbeck","https:\/\/doi.org\/10.1016\/B978-0-12-800344-2.00011-2","2016","Chapter 11 - From Databases to Big Data"],["Big data has been a buzzword for academics and the healthcare industry, describing data accumulation, processing, and interpretation using analytical tools and techniques. Hitherto, identification of state of the art in big data has become a question of research for academicians due to lack of addressable studies in this regard. The potential of big data is lagging, as compared to other fields. Herein, in this chapter, we have described the potential of big data in combating infectious diseases involving personalized, participatory, predictive, and preventive models based on biomedical data also called the \u201comics data\u201d and electronic health records from different authenticated sources. Our study indicates the use of various tools and techniques for data accumulation and management, thus providing an insight toward the revolution of the healthcare industry as well as the research community. Though the application of big data is still in the preliminary stage, growing Research and Development investment with its successful implementation will show enormous potential of growth in coming years. Considering all together, there is a need for the development of advanced technologies with inclusion of transparent ethical values to provide acceptance and with a moral foundation.","Pritha Chakraborty and Parth Sarthi {Sen Gupta} and Shankar Dey and Nabarun {Chandra Das} and Ritwik Patra and Suprabhat Mukherjee","https:\/\/doi.org\/10.1016\/B978-0-323-91907-4.00009-1","2022","Chapter 7 - Recent advances in processing, interpreting, and managing biological data for therapeutic intervention of human infectious disease"],["The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows \u2018doing more with less\u2019 through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often \u2018precisely inaccurate\u2019, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of \u2018ultra-precise\u2019 digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a \u2018precision trap\u2019. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of \u2018precision traps\u2019 increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging \u2018precision divide\u2019: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like \u2018imprecision farming\u2019.","Oane Visser and Sarah Ruth Sippel and Louis Thiemann","https:\/\/doi.org\/10.1016\/j.jrurstud.2021.07.024","2021","Imprecision farming? Examining the (in)accuracy and risks of digital agriculture"],["The vague but vogue notion of \u2018big data\u2019 is enjoying a prolonged honeymoon. Well-funded, ambitious projects are reaching fruition, and inferences are being drawn from inadequate data processed by inadequately understood and often inappropriate data analytic techniques. As decisions are made and actions taken on the basis of those inferences, harm will arise to external stakeholders, and, over time, to internal stakeholders as well. A set of Guidelines is presented, whose purpose is to intercept ill-advised uses of data and analytical tools, prevent harm to important values, and assist organisations to extract the achievable benefits from data, rather than dreaming dangerous dreams.","Roger Clarke","https:\/\/doi.org\/10.1016\/j.clsr.2017.11.002","2018","Guidelines for the responsible application of data analytics"],["Many SMEs still seem reluctant to accept the management of large datasets, which still appear to be too complex for them. However, our study reveals that the majority of small French car dealers are developing Big data and Smart data policies to improve the quality of their offers, the dynamism of their sales and their access to new opportunities. However, not every policy has the same effects on the development of their business. Whereas Big data improves all the components of SME development in a global, short-term and operational way, Smart data presents itself as a more targeted, prospective and strategic approach.","David Salvetat and Jean-S\u00e9bastien Lacam","https:\/\/doi.org\/10.1016\/j.jengtecman.2020.101602","2020","Data determinants of the activity of SMEs automobile dealers"],["Border based Knowledge hiding techniques (BB-KHT) are widely adopted form of privacy preservation techniques of data mining. These approaches are used to hide sensitive knowledge (confidential information) present in a dataset before sharing or analyzing it. BB-KHT primarily rely on border theory and maximum criterion method for preserving privacy and perpetuating good data quality of sanitized dataset but costs high computational complexity. Further, due to sequential nature, these approaches are particularly felicitous for small datasets and become infeasible while dealing with large scale datasets. Therefore, to subjugate the identified challenges of infeasibility and high computational complexity, a scalable two-phase improved MaxMin BB-KHT using MapReduce framework (MR-I MaxMin) is proposed. The proposed scheme requires only two database scans throughout the hiding process and hence, is computationally inexpensive. Moreover, the scheme also commits to preserve good data quality of sanitized dataset. The MapReduce version of proposed approach helps in achieving the feasibility by processing large voluminous data in a parallel fashion. Quantitative experiments and evaluations have been performed over a number of real and synthetically generated large-scale datasets. It is shown that the proposed MR-I MaxMin technique outperforms the similar existing approaches and vanquishes the identified challenges along with much-needed privacy preservation.","Shivani Sharma and Durga Toshniwal","https:\/\/doi.org\/10.1016\/j.future.2018.05.063","2020","MR-I MaxMin-scalable two-phase border based knowledge hiding technique using MapReduce"],["The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.","Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2017.09.008","2018","Data quality challenges in the UK social housing sector"],["ABSTRACT\nThe world continues to experience a surge in data generation and digital transformation. Historic data is increasingly being replaced by modernized data, such as big data, which is regarded as data that exhibits the 5Vs: volume, variety, velocity, veracity and value. The capacity to optimally use and comprehend value from big data has become an indispensable aptitude for modern companies. In contrast to commercial and technology firms, usage, management and governance of data, including big data is a novel and evolving trend for mining and mineral industries. Although the mining industry can be unenthusiastic to change, embracing modernized data and big data is evolutionarily unavoidable, given many industry-wide challenges (i.e., fluctuation in commodity prices, geotechnical and harsh ground conditions, and ore grade), which corrode revenues and increase business risks, including the possibility of regulatory non-compliance. The minerals industry holds a genuine gold mine of data that were collected for scientific, engineering, operational and other purposes. Data and data-centric workspaces that are targeted towards innovation and experimentation, which if combined with in-discipline expertise are two harmonious ingredients that can provide many practical solutions for the mining and mineral industries. In this paper, the concept, the opportunity and the necessity for a move towards a technology- and innovation-based, data-centric \u2018dry laboratories\u2019 (common workspaces that facilitates data-centric experimentation and innovation) in the minerals industry are assessed. We contend that the dry laboratory environment maximizes the value of data for the minerals industry. Toward the establishment of dry laboratories, we propose several essential components of a framework that would enable the functionality of dry laboratories in the minerals industry, while concomitantly examining the components from both academia and industry perspectives.","Yousef Ghorbani and Steven E. Zhang and Glen T. Nwaila and Julie E. Bourdeau","https:\/\/doi.org\/10.1016\/j.exis.2022.101089","2022","Framework components for data-centric dry laboratories in the minerals industry: A path to science-and-technology-led innovation"],["Threats to local, national, and global public security are continually evolving, and for those tasked with preventing and responding to these threats, the amount of potentially useful data can often seem overwhelming. What compounds this Big Data issue is the fact that we are living in a time of global economic austerity in which national security and law enforcement agencies need to become better at exploiting information while managing the demands of ever-shrinking budgets. This chapter looks at how, by using the latest software tools and techniques for data fusion and high-performance analytics, agencies can automate traditional labor-intensive tasks, gain a holistic view of information that originates from multiple sources, and extract valuable intelligence in a timely and more efficient manner.","Rupert Hollin","https:\/\/doi.org\/10.1016\/B978-0-12-801967-2.00002-1","2015","Chapter 2 - Drilling into the Big Data Gold Mine: Data Fusion and High-Performance Analytics for Intelligence Professionals"],["Background\nSince the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.\nMethods\nThis paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.\nResults and Conclusion\nIt is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.","Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang","https:\/\/doi.org\/10.1016\/j.idh.2018.10.002","2019","Artificial Intelligence for infectious disease Big Data Analytics"],["Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.","Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yal\u00e7\u0131n A\u00e7\u0131kg\u00f6z","https:\/\/doi.org\/10.1016\/j.ssci.2021.105569","2022","Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health"],["Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly: copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of a Big Data resource. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) obtaining the rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four issues will never go away.","Jules J. Berman","https:\/\/doi.org\/10.1016\/B978-0-12-815609-4.00019-4","2018","19 - Legalities"],["The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.","Otmane Azeroual and Gunter Saake and Eike Schallehn","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.02.007","2018","Analyzing data quality issues in research information systems via data profiling"],["Predictive analytics methods have become increasingly important in Manufacturing Organization in the context of Smart Maintenance. Standardized process models for data mining already known to search existing data stocks for patterns, trends and correlations. Sensors are progressively implemented in production machines to create a database for data mining processes. But the risk of Big Data, thus the risk of low quality data is probably high. For an economic consideration, the amount of investment in new measurement technology and infrastructure should be assessed. Organizations are confronted with the challenge of how much they have to invest to obtain a meaningful database. For this reason, it is important to research which existing approaches support the development of a sufficient database for predictive maintenance in manufacturing systems and provide a methodical framework.","Marina Klees and Safa Evirgen","https:\/\/doi.org\/10.1016\/j.procs.2022.08.002","2022","Building a smart database for predictive maintenance in already implemented manufacturing systems"],["Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption\/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.","Abdul-Nasser El-Kassar and Sanjay Kumar Singh","https:\/\/doi.org\/10.1016\/j.techfore.2017.12.016","2019","Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices"],["The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.","Madhavi Vaidya and Shweta Katkar","https:\/\/doi.org\/10.1016\/B978-0-12-823694-9.00030-X","2022","Chapter 24 - Exploring performance and predictive analytics of agriculture data"],["Background\nBig data analytics promise insights into healthcare processes and management, improving outcomes while reducing costs. However, data quality is a major challenge for reliable results. Business process discovery techniques and an associated data model were used to develop data management tool, ICU-DaMa, for extracting variables essential for overseeing the quality of care in the intensive care unit (ICU).\nObjective\nTo determine the feasibility of using ICU-DaMa to automatically extract variables for the minimum dataset and ICU quality indicators from the clinical information system (CIS).\nMethods\nThe Wilcoxon signed-rank test and Fisher\u2019s exact test were used to compare the values extracted from the CIS with ICU-DaMa for 25 variables from all patients attended in a polyvalent ICU during a two-month period against the gold standard of values manually extracted by two trained physicians. Discrepancies with the gold standard were classified into plausibility, conformance, and completeness errors.\nResults\nData from 149 patients were included. Although there were no significant differences between the automatic method and the manual method, we detected differences in values for five variables, including one plausibility error and two conformance and completeness errors. Plausibility: 1) Sex, ICU-DaMa incorrectly classified one male patient as female (error generated by the Hospital\u2019s Admissions Department). Conformance: 2) Reason for isolation, ICU-DaMa failed to detect a human error in which a professional misclassified a patient\u2019s isolation. 3) Brain death, ICU-DaMa failed to detect another human error in which a professional likely entered two mutually exclusive values related to the death of the patient (brain death and controlled donation after circulatory death). Completeness: 4) Destination at ICU discharge, ICU-DaMa incorrectly classified two patients due to a professional failing to fill out the patient discharge form when thepatients died. 5) Length of continuous renal replacement therapy, data were missing for one patient because the CRRT device was not connected to the CIS.\nConclusions\nAutomatic generation of minimum dataset and ICU quality indicators using ICU-DaMa is feasible. The discrepancies were identified and can be corrected by improving CIS ergonomics, training healthcare professionals in the culture of the quality of information, and using tools for detecting and correcting data errors.","Gonzalo Sirgo and Federico Esteban and Josep G\u00f3mez and Gerard Moreno and Alejandro Rodr\u00edguez and Lluis Blanch and Juan Jos\u00e9 Guardiola and Rafael Gracia and Lluis {De Haro} and Mar\u00eda Bod\u00ed","https:\/\/doi.org\/10.1016\/j.ijmedinf.2018.02.007","2018","Validation of the ICU-DaMa tool for automatically extracting variables for minimum dataset and quality indicators: The importance of data quality assessment"],["Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of \u201cdata phase triangulation,\u201d which is contrasted with \u201cmethodology triangulation\u201d and \u201cdata (collection) triangulation.\u201d In addition, we discuss the importance of being able to use the findings from life cycle-based research to \u201cpush\u201d a technology from one phase to another phase.","Daniel E. O'Leary","https:\/\/doi.org\/10.1016\/j.dss.2019.113139","2019","Technology life cycle and data quality: Action and triangulation"],["Summary\nBig data is important to new developments in global clinical science that aim to improve the lives of patients. Technological advances have led to the regular use of structured electronic health-care records with the potential to address key deficits in clinical evidence that could improve patient care. The COVID-19 pandemic has shown this potential in big data and related analytics but has also revealed important limitations. Data verification, data validation, data privacy, and a mandate from the public to conduct research are important challenges to effective use of routine health-care data. The European Society of Cardiology and the BigData@Heart consortium have brought together a range of international stakeholders, including representation from patients, clinicians, scientists, regulators, journal editors, and industry members. In this Review, we propose the CODE-EHR minimum standards framework to be used by researchers and clinicians to improve the design of studies and enhance transparency of study methods. The CODE-EHR framework aims to develop robust and effective utilisation of health-care data for research purposes.","Dipak Kotecha and Folkert W Asselbergs and Stephan Achenbach and Stefan D Anker and Dan Atar and Colin Baigent and Amitava Banerjee and Birgit Beger and Gunnar Brobert and Barbara Casadei and Cinzia Ceccarelli and Martin R Cowie and Filippo Crea and Maureen Cronin and Spiros Denaxas and Andrea Derix and Donna Fitzsimons and Martin Fredriksson and Chris P Gale and Georgios V Gkoutos and Wim Goettsch and Harry Hemingway and Martin Ingvar and Adrian Jonas and Robert Kazmierski and Susanne L\u00f8gstrup and R Thomas Lumbers and Thomas F L\u00fcscher and Paul McGreavy and Ileana L Pi\u00f1a and Lothar Roessig and Carl Steinbeisser and Mats Sundgren and Beno\u00eet Tyl and Ghislaine van Thiel and Kees van Bochove and Panos E Vardas and Tiago Villanueva and Marilena Vrana and Wim Weber and Franz Weidinger and Stephan Windecker and Angela Wood and Diederick E Grobbee","https:\/\/doi.org\/10.1016\/S2589-7500(22)00151-0","2022","CODE-EHR best-practice framework for the use of structured electronic health-care records in clinical research"],["This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing\/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C\/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.","Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee","https:\/\/doi.org\/10.1016\/j.enbuild.2018.03.021","2018","Analyze the energy consumption characteristics and affecting factors of Taiwan's convenience stores-using the big data mining approach"],["We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research \u2013 a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and\/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.","James R. Marsden and David E. Pingry","https:\/\/doi.org\/10.1016\/j.dss.2018.10.007","2018","Numerical data quality in IS research and the implications for replication"],["Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.","Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger","https:\/\/doi.org\/10.1016\/j.ijsu.2019.07.036","2019","Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives"],["Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.","In\u00eas Ara\u00fajo Machado and Carlos Costa and Maribel Yasmina Santos","https:\/\/doi.org\/10.1016\/j.procs.2021.12.013","2022","Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures"],["Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and\/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.","Tony O\u2019Brien","https:\/\/doi.org\/10.1016\/j.procs.2015.08.539","2015","\u2018Accounting\u2019 for Data Quality in Enterprise Systems"],["When comparing Cloud and non-Cloud Storage it can be difficult to ensure that the comparison is fair. In this paper we examine the process of setting up such a comparison and the metric used. Performance comparisons on Cloud and non-Cloud systems, deployed for biomedical scientists, have been conducted to identify improvements of efficiency and performance. Prior to the experiments, network latency, file size and job failures were identified as factors which degrade performance and experiments were conducted to understand their impacts. Organizational Sustainability Modeling (OSM) is used before, during and after the experiments to ensure fair comparisons are achieved. OSM defines the actual and expected execution time, risk control rates and is used to understand key outputs related to both Cloud and non-Cloud experiments. Forty experiments on both Cloud and non-Cloud systems were undertaken with two case studies. The first case study was focused on transferring and backing up 10,000 files of 1 GB each and the second case study was focused on transferring and backing up 1000 files 10 GB each. Results showed that first, the actual and expected execution time on the Cloud was lower than on the non-Cloud system. Second, there was more than 99% consistency between the actual and expected execution time on the Cloud while no comparable consistency was found on the non-Cloud system. Third, the improvement in efficiency was higher on the Cloud than the non-Cloud. OSM is the metric used to analyze the collected data and provided synthesis and insights to the data analysis and visualization of the two case studies.","Victor Chang and Gary Wills","https:\/\/doi.org\/10.1016\/j.future.2015.10.003","2016","A model to compare cloud and non-cloud storage of Big Data"],["This article introduces the methods of Fourier and wavelet analysis for enhancing data quality in typical chemometric and process analytics applications. Fourier analysis has been popular for many decades but is best suited for enhancing signals where most features are localized in frequency. In contrast, wavelet analysis is appropriate for signals that contain features localized in both time and frequency domains. It also retains the benefits of Fourier analysis such as orthonormality and computational efficiency. Practical algorithms for off-line and on-line denoising are described and compared via simple examples. These algorithms can be used for off-line or on-line applications in order to mitigate the impact of additive Gaussian as well as non-Gaussian noise.","M.S. Reis and P.M. Saraiva and B.R. Bakshi","https:\/\/doi.org\/10.1016\/B978-0-12-409547-2.14874-7","2020","3.10 - Data Quality and Denoising: A Review\u2606"],[null,"Carlos S\u00e1ez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez","https:\/\/doi.org\/10.1016\/j.cmpb.2019.06.013","2019","Guest editorial: Special issue in biomedical data quality assessment methods"],[null,"T. Gant","https:\/\/doi.org\/10.1016\/j.toxlet.2015.08.097","2015","The importance of data quality to enhance the impact of omics sciences"],["Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public\/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.","Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov","https:\/\/doi.org\/10.1016\/j.euroneuro.2015.05.011","2015","The preclinical data forum network: A new ECNP initiative to improve data quality and robustness for (preclinical) neuroscience"],["Big Data is largely a buzzword in IT right now. It was coined by Forrester Research to put a wrapper around existing database mining, data management, and other extensions of existing technology to the current hardware. The goal is to use mixed tools with larger volumes of several different forms of data being brought together under one roof. Along with this approach to data, we are also concerned with cloud computing, which is a public or private Internet network that replaces the tradition hardwired landlines within a company.","Joe Celko","https:\/\/doi.org\/10.1016\/B978-0-12-407192-6.00009-1","2014","Chapter 9 - Big Data and Cloud Computing"],["With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project \u2013 DMU Imagery Comparison Engine \u2013 is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https:\/\/www.dmu-net.org.","Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis","https:\/\/doi.org\/10.1016\/j.compind.2018.04.005","2018","Deep learning for big data applications in CAD and PLM \u2013 Research review, opportunities and case study"],["The Big Data era is creating seismic shifts in how we approach enterprise business intelligence and data warehousing. This final chapter discusses considerations related to technology and architecture, analytics-oriented requirements collection, and organizational structure.","Alan Simon","https:\/\/doi.org\/10.1016\/B978-0-12-801540-7.00008-1","2015","Chapter 8 - Considerations for the Big Data Era"],["With the increasing penetration of traditional and emerging information technologies in the electric power industry, together with the rapid development of electricity market reform, the electric power industry has accumulated a large amount of data. Data quality issues have become increasingly prominent, which affect the accuracy and effectiveness of electricity data mining and energy big data analytics. It is also closely related to the safety and reliability of the power system operation and management based on data-driven decision support. In this paper, we study the data quality of electricity consumption data in a smart grid environment. First, we analyze the significance of data quality. Also, the definition and classification of data quality issues are explained. Then we analyze the data quality of electricity consumption data and introduce the characteristics of electricity consumption data in a smart grid environment. The data quality issues of electricity consumption data are divided into three types, namely noise data, incomplete data and outlier data. We make a detailed discussion on these three types of data quality issues. In view of that outlier data is one of the most prominent issues in electricity consumption data, so we mainly focus on the outlier detection of electricity consumption data. This paper introduces the causes of electricity consumption outlier data and illustrates the significance of the electricity consumption outlier data from the negative and positive aspects respectively. Finally, the focus of this paper is to provide a review on the detection methods of electricity consumption outlier data. The methods are mainly divided into two categories, namely the data mining-based and the state estimation-based methods.","Wen Chen and Kaile Zhou and Shanlin Yang and Cheng Wu","https:\/\/doi.org\/10.1016\/j.rser.2016.10.054","2017","Data quality of electricity consumption data in a smart grid environment"],["Privacy trust directly affects the personal willingness to share data and thus influences the quality and size of the data, thus affecting the development of big data technology and industry. As China is probably the largest personal data pool and vastest application market of big data, the situation of Chinese privacy trust plays a significant role. Based on the 17 most common data collection scenarios, the following aspects have been observed through 508 questionnaires and interviews of 20 samples. To start with, there is a severe privacy trust crisis in China, both in the field of enterprise services such as online shopping and social networks, etc. and in some public services like medical care and education, etc. Besides, there are also doubts about data collected by the government since individuals refuse to offer personal information or give false information as much as possible. Some people even buy two phone numbers, one is in use, while the other is not carried around or used by them, which is only bought to be offered to data collectors. Secondly, in terms of gender, females have lower trust in enterprises and social associations than males, especially in the fields of social networks and personal consumption. However, there is no obvious difference in fields of government and public services. Females possess stronger awareness but less skilled in precautions than males. Thirdly, people between the ages of 18 and 50 are more suspicious of data collected by enterprises, while age exerts little obvious influence on the credibility of data collected by the government, social associations and public services. Older people are less aware of precautions than people at other ages. In addition, from the perspective of education background, people with higher degrees possess stronger awareness of precautions and thus lower degree of trust. Therefore, it is suggested that more education on privacy consciousness should be given, and relative laws as well as regulations need improving. Besides, innovation in privacy protection technologies should be encouraged. What is more, we need to reinforce the management of the internet industry and strictly regulate personal data collection of the government.","Zhong Wang and Qian Yu","https:\/\/doi.org\/10.1016\/j.clsr.2015.08.006","2015","Privacy trust crisis of personal data in China in the era of Big Data: The survey and countermeasures"],["We introduce the Forward Observer system, which is designed to provide data assurance in field data acquisition while receiving significant amounts (several terabytes per flight) of Synthetic Aperture Radar data during flights over the polar regions, which provide unique requirements for developing data collection and processing systems. Under polar conditions in the field and given the difficulty and expense of collecting data, data retention is absolutely critical. Our system provides a storage and analysis cluster with software that connects to field instruments via standard protocols, replicates data to multiple stores automatically as soon as it is written, and provides pre-processing of data so that initial visualizations are available immediately after collection, where they can provide feedback to researchers in the aircraft during the flight.","Richard Knepper and Matthew Standish and Matthew Link","https:\/\/doi.org\/10.1016\/j.procs.2015.05.340","2015","Big Data on Ice: The Forward Observer System for In-flight Synthetic Aperture Radar Processing"],["This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.","Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang","https:\/\/doi.org\/10.1016\/j.jclepro.2016.03.006","2017","An optimization model for green supply chain management by using a big data analytic approach"],["The focus of this chapter is to discuss the integration of Big Data and the data warehouse, the possible techniques and pitfalls, and where we leverage a technology. How do we deal with complexity and heterogeneity of technologies? What are the performance and scalabilities of each technology, and how can we sustain performance for the new environment?","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-405891-0.00010-6","2013","Chapter 10 - Integration of Big Data and Data Warehousing"],["Big data and predictive analytics (BDPA) tools and methodologies are leveraged by businesses in many ways to improve operational and strategic capabilities, and ultimately, to positively impact corporate financial performance. BDPA has become crucial for managing supply chain functions, where data intensive processes can be vastly improved through its effective use. BDPA has also become a competitive necessity for the management of supply chains, with practitioners and scholars focused almost entirely on how BDPA is used to increase economic measures of performance. There is limited understanding, however, as to how BDPA can impact other aspects of the triple bottom-line, namely environmental and social sustainability outcomes. Indeed, this area is in immediate need of attention from scholars in many fields including industrial engineering, supply chain management, information systems, business analytics, as well as other business and engineering disciplines. The purpose of this article is to motivate such research by proposing an agenda based in well-established theory. This article reviews eight theories that can be used by researchers to examine and clarify the nature of BDPA\u2019s impact on supply chain sustainability, and presents research questions based upon this review. Scholars can leverage this article as the basis for future research activity, and practitioners can use this article as a means to understand how company-wide BDPA initiatives might impact measures of supply chain sustainability.","Benjamin T. Hazen and Joseph B. Skipper and Jeremy D. Ezell and Christopher A. Boone","https:\/\/doi.org\/10.1016\/j.cie.2016.06.030","2016","Big data and predictive analytics for supply chain sustainability: A theory-driven research agenda"],["Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure \u201cPhenoMeNal,\u201d which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.","Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen","https:\/\/doi.org\/10.1016\/B978-0-12-812293-8.00011-6","2019","Chapter 11 - Big Data and Databases for Metabolic Phenotyping"],["Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized\/proposed\/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 \u2013 What are the different types of BD challenges theorized\/proposed\/confronted by organizations? and Q2 \u2013 What are the different types of BDA methods theorized\/proposed\/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns\/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes\/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.","Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody","https:\/\/doi.org\/10.1016\/j.jbusres.2016.08.001","2017","Critical analysis of Big Data challenges and analytical methods"],["Abstract:\nThe approaches used by epidemiologists are diverse: they range from \u201cfield studies\u201d for modeling and healthcare monitoring, to methods developed for researching and combating the emergence of diseases. Their analytical tools focus on the bio-statistics used as a tool to objectify phenomena studied in well-defined populations.","Marius Fieschi","https:\/\/doi.org\/10.1016\/B978-1-78548-287-8.50016-X","2018","16 - Data for Epidemiology and Public Health, and Big Data11The questions posed by data processing for epidemiology and public health are often similar to those discussed in the chapters on clinical research (Chapter 18) and bioinformatics data (Chapter 17). For the sake of clarity, we address these questions in different chapters, although the problems are of the same nature and the solutions are isomorphic. In order to avoid too much repetition, the issue of big data is discussed here without going into the content of the other chapters."],["Mobile Sensing Networks have been widely applied to many fields for big data perception such as intelligent transportation, medical health and environment sensing. However, in some complex environments and unreachable regions of inconvenience for human, the establishment of the mobile sensing networks, the layout of the nodes and the control of the network topology to achieve high performance sensing of big data are increasingly becoming a main issue in the applications of the mobile sensing networks. To deal with this problem, we propose a novel on-demand coverage based self-deployment algorithm for big data perception based on mobile sensing networks in this paper. Firstly, by considering characteristics of mobile sensing nodes, we extend the cellular automata model and propose a new mobile cellular automata model for effectively characterizing the spatial\u2013temporal evolutionary process of nodes. Secondly, based on the learning automata theory and the historical information of node movement, we further explore a new mobile cellular learning automata model, in which nodes can self-adaptively and intelligently decide the best direction of movement with low energy consumption. Finally, we propose a new optimization algorithm which can quickly solve the node self-adaptive deployment problem, thus, we derive the best deployment scheme of nodes in a short time. The extensive simulation results show that the proposed algorithm in this paper outperforms the existing algorithms by as much as 40% in terms of the degree of satisfaction of network coverage, the iterations of the algorithm, the average moving steps of nodes and the energy consumption of nodes. Hence, we believe that our work will make contributions to large-scale adaptive deployment and high performance sensing scenarios of the mobile sensing networks.","Yaguang Lin and Xiaoming Wang and Fei Hao and Liang Wang and Lichen Zhang and Ruonan Zhao","https:\/\/doi.org\/10.1016\/j.future.2018.01.007","2018","An on-demand coverage based self-deployment algorithm for big data perception in mobile sensing networks"],["Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.","Javier Carnicero and David Rojas","https:\/\/doi.org\/10.1016\/B978-0-12-809556-0.00008-3","2019","Chapter 8 - Healthcare Decision-Making Support Based on the Application of Big Data to Electronic Medical Records: A Knowledge Management Cycle"],["A group of disparate translational bioinformatics experts convened at the 6th Annual Precision Medicine Partnership Meeting, October 29\u201330, 2014 to discuss big data challenges and key strategic decisions needed to advance precision medicine, emerging solutions, and the anticipated path to success. This article reports the panel discussion.","Carol Isaacson Barash and Keith O. Elliston and W. {Andrew Faucett} and Jonathan Hirsch and Gauri Naik and Alice Rathjen and Grant Wood","https:\/\/doi.org\/10.1016\/j.atg.2015.02.002","2015","Harnessing big data for precision medicine: A panel of experts elucidates the data challenges and proposes key strategic decisions points"],["In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and\/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.","Manish Shukla and Lana Mattar","https:\/\/doi.org\/10.1016\/j.cie.2018.04.055","2019","Next generation smart sustainable auditing systems using Big Data Analytics: Understanding the interaction of critical barriers"],["Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.","Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett","https:\/\/doi.org\/10.1016\/j.jii.2018.02.002","2018","Improving early OSV design robustness by applying \u2018Multivariate Big Data Analytics\u2019 on a ship's life cycle"],["Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5\u202f\u03bcm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6\u202f\u00b1\u202f18.4\u202f\u03bcg\/m3. Specifically, the concentration in non-heating season was 34.9\u202f\u00b1\u202f15.8\u202f\u03bcg\/m3, which was 24% lower than that in heating season (46.1\u202f\u00b1\u202f21.2\u202f\u03bcg\/m3). A significant correlation between indoor and ambient PM2.5 (p\u202f<\u202f0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor\/outdoor (I\/O) ratio was estimated to be 0.73\u202f\u00b1\u202f0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8\u202f\u00b1\u202f27.4\u202f\u03bcg\/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.","JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong","https:\/\/doi.org\/10.1016\/j.envpol.2018.05.030","2018","Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing"],["Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation \u2013 or even amplification \u2013 of bias in the input data of ADM systems.","Antonio Vetr\u00f2 and Marco Torchiano and Mariachiara Mecati","https:\/\/doi.org\/10.1016\/j.giq.2021.101619","2021","A data quality approach to the identification of discrimination risk in automated decision making systems"],["The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-405891-0.00011-8","2013","Chapter 11 - Data-Driven Architecture for Big Data"],["This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.","John R. Talburt and Yinle Zhou","https:\/\/doi.org\/10.1016\/B978-0-12-800537-8.00001-6","2015","Chapter 1 - The Value Proposition for MDM and Big Data"],["The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition\/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.","Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski","https:\/\/doi.org\/10.1016\/j.envsoft.2019.03.008","2019","A pilot infrastructure for searching rainfall metadata and generating rainfall product using the big data of NEXRAD"],["User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses\u2019 strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers\u2019 minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.","Neil T. Bendle and Xin (Shane) Wang","https:\/\/doi.org\/10.1016\/j.bushor.2015.10.001","2016","Uncovering the message from the mess of big data"],["ABSTRACT\nThe information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.","Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li","https:\/\/doi.org\/10.1080\/01441647.2019.1649315","2019","How big data enriches maritime research \u2013 a critical review of Automatic Identification System (AIS) data applications"],["Purpose\nTo prepare for big data analyses on radiation therapy data, we developed Stature, a tool-supported approach for standardization of structure names in existing radiation therapy plans. We applied the widely endorsed nomenclature standard TG-263 as the mapping target and quantified the structure name inconsistency in 2 real-world data sets.\nMethods and Materials\nThe clinically relevant structures in the radiation therapy plans were identified by reference to randomized controlled trials. The Stature approach was used by clinicians to identify the synonyms for each relevant structure, which was then mapped to the corresponding TG-263 name. We applied Stature to standardize the structure names for 654 patients with prostate cancer (PCa) and 224 patients with head and neck squamous cell carcinoma (HNSCC) who received curative radiation therapy at our institution between 2007 and 2017. The accuracy of the Stature process was manually validated in a random sample from each cohort. For the HNSCC cohort we measured the resource requirements for Stature, and for the PCa cohort we demonstrated its impact on an example clinical analytics scenario.\nResults\nAll but 1 synonym group (\u201cHydrogel\u201d) was mapped to the corresponding TG-263 name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures were exact matches (ie, following local naming convention), 284 were matched via a synonym, and 3 required manual matching. This original radiation therapy structure names therefore had a naming inconsistency rate of 4.81%. For the HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym, 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The clinician hours required were one-fifth of those that would be required for manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).\nConclusions\nThe Stature approach was highly accurate and had significant resource efficiencies compared with manual curation.","Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill and Brooke Evans and Blanca Gallego","https:\/\/doi.org\/10.1016\/j.adro.2018.09.013","2019","Big Data Readiness in Radiation Oncology: An Efficient Approach for Relabeling Radiation Therapy Structures With Their TG-263 Standard Name in Real-World Data Sets"],["In the digital age, the interaction between privacy, data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the \u201cparadox\u201d of on one side protecting fundamental human rights against external terrorist and\/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods.","Daniel Drewer and Vesela Miladinova","https:\/\/doi.org\/10.1016\/j.clsr.2017.03.006","2017","The BIG DATA Challenge: Impact and opportunity of large quantities of information under the Europol Regulation"],[null,null,"https:\/\/doi.org\/10.1016\/B978-0-12-821737-5.00016-X","2022","In praise of Meeting the Challenges of Data Quality Management"],["In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.","Lars Richter and Gerhard F. Ecker","https:\/\/doi.org\/10.1016\/j.ddtec.2015.06.001","2015","Medicinal chemistry in the era of big data"],["The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher\u05f3s work, and more importantly, facilitate transparency and repeatability of a researcher\u05f3s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of \u201csupporting online material\u201d (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.","Marin M. Kress","https:\/\/doi.org\/10.1016\/B978-0-12-409548-9.10557-3","2019","Big Data for Ecological Models"],["Manufacturing data offers big potential for improving management of manufacturing operations. The paper addresses an approach to data analytics in engineer-to-order (ETO) manufacturing systems where the product quality and due-date reliability play a key role in management decisionmaking. The objective of the research is to investigate manufacturing data which are collected by a manufacturing execution system (MES) during operations in an ETO enterprise and to develop tools for supporting scheduling of operations. The developed tools can be used for simulation of production and forecasting of potential resource overloads.","Dominik Kozjek and Rok Vrabi\u010d and Borut Rihtar\u0161i\u010d and Peter Butala","https:\/\/doi.org\/10.1016\/j.procir.2018.03.098","2018","Big data analytics for operations management in engineer-to-order manufacturing"],[null,"Andrea Damiani and Graziano Onder and Vincenzo Valentini","https:\/\/doi.org\/10.1016\/j.ejim.2018.05.019","2018","Large databases (Big Data) and evidence-based medicine"],["Web-based device fingerprinting is the process of collecting security information through the browser to perform stateless device identification. Fingerprints may then be used to identify and track computing devices in the web. There are various reasons why device-related information may be needed. Among the others, this technique could help to efficiently analyze security information for sustainability. In this paper we introduce a fingerprinting analytics tool that discovers the most appropriate device fingerprints and their corresponding optimal implementations. The fingerprints selected in the result of the performed analysis are used to enrich and improve an open-source fingerprinting analytics tool Fingerprintjs2, daily consumed by hundreds of websites. As a result, the paper provides a noticeable progress in analytics of dozens of values of device fingerprints, and enhances analysis of fingerprints security information.","Anna Kobusi\u0144ska and Kamil Pawluczuk and Jerzy Brzezi\u0144ski","https:\/\/doi.org\/10.1016\/j.future.2017.12.061","2018","Big Data fingerprinting information analytics for sustainability"],["Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.","Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun","https:\/\/doi.org\/10.1016\/j.canlet.2016.05.033","2016","Big Data and machine learning in radiation oncology: State of the art and future prospects"],["In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5\u202f\u03bcm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80\u202f\u03bcg\/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.","Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood","https:\/\/doi.org\/10.1016\/j.envpol.2019.06.057","2019","Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data"],["Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients\u2019 risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.","Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy","https:\/\/doi.org\/10.1016\/j.annemergmed.2015.06.024","2016","Exploring the Potential of Predictive Analytics and Big Data in Emergency Care"],["Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.","Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon","https:\/\/doi.org\/10.1016\/j.suscom.2017.10.009","2018","Energy-harvesting based on internet of things and big data analytics for smart health monitoring"],["Background and objective\nIn recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.\nMethods\nWe identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.\nResults\nWe identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.\nConclusion\nWe identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.","Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli","https:\/\/doi.org\/10.1016\/j.cmpb.2019.05.017","2019","Towards a content agnostic computable knowledge repository for data quality assessment"],[null,"Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham","https:\/\/doi.org\/10.1053\/j.ajkd.2017.04.008","2017","The China Kidney Disease Network (CK-NET): \u201cBig Data\u2014Big Dreams\u201d"],["With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., \u201cPhysicalTherapy-as-a-Service\u201d (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a \u2018Synchronous Big Data\u2019 application due to its: (i) high data-in-motion velocity (i.e., peak data rate is \u2248400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.","Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic","https:\/\/doi.org\/10.1016\/j.pmcj.2015.09.004","2016","Synchronous Big Data analytics for personalized and remote physical therapy"],["The first four chapters provided you an introduction to Big Data, the complexities associated with Big Data, and the processing techniques and technologies for Big Data. This chapter will focus on use cases of Big Data and how real-world companies are implementing Big Data.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-405891-0.00005-2","2013","Chapter 5 - Big Data Driving Business Value"],["Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking\/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.","Dirk U. Pfeiffer and Kim B. Stevens","https:\/\/doi.org\/10.1016\/j.prevetmed.2015.05.012","2015","Spatial and temporal epidemiological analysis in the Big Data era"],["Smart agriculture is one of the most diverse research. In addition, the quantity of data to be stored and the choice of the most efficient algorithms to process are significant elements in this field. The storage of collecting data from Internet of Things (IoT), existing on distributed, local databases and open data need a particular infrastructure to federate all these data to make complex treatments. The storage of this wide range of data that comes at high frequency and variable throughput is particularly difficult. In this paper, we propose the use of distributed databases and high-performance computing architecture in order to exploit multiple re-configurable computing and application specific processing such as CPUs, GPUs, TPUs and FPGAs efficiently. This exploitation allows an accurate training for an application to machine learning, deep learning and unsupervised modeling algorithms. The last ones are used for training supervised algorithms on images when it labels a set of images and unsupervised algorithms on IoT data which are unlabeled with variable qualities. The processing of data is based on Hadoop 3.1 MapReduce to achieve parallel processing and use containerization technologies to distribute treatments on Multi GPU, MIC and FPGA. This architecture allows efficient treatments of data coming from several sources with a cloud high-performance heterogeneous architecture. The proposed 4 layers infrastructure can also implement FPGA and MIC which are now natively supported by recent version of Hadoop. Moreover, with the advent of new technologies like Intel\u00ae MovidiusTM; it is now possible to deploy CNN at the Fog level in the IoT network and to make inference with the cloud and therefore limit significantly the network traffic that result in reducing the move of large amounts of data to the cloud.","Olivier Debauche and Sidi Ahmed Mahmoudi and Sa\u00efd Mahmoudi and Pierre Manneback","https:\/\/doi.org\/10.1016\/j.procs.2018.10.156","2018","Cloud Platform using Big Data and HPC Technologies for Distributed and Parallels Treatments"],["In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.","Tsan-Ming Choi and Suyuan Luo","https:\/\/doi.org\/10.1016\/j.tre.2019.09.019","2019","Data quality challenges for sustainable fashion supply chain operations in emerging markets: Roles of blockchain, government sponsors and environment taxes"],["This chapter will discuss the building and delivery of the application. We will look at all aspects of what needs to be done to complete the process from requirements to outcomes, program management, testing, methodology, and all risks and pitfalls. We will discuss KANBAN, budgets and finances, governance, timeline, increase of efficiency, maintenance, support, and application implementation.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-815746-6.00010-7","2020","10 - Building the big data application"],["Rapid advances in the collection, storage, and analysis of large volumes of data\u2014Big Data\u2014offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the \u201cmost complex disease of the most complex organ.\u201d While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and\/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches\u2014text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.","Denes V. Agoston","https:\/\/doi.org\/10.1016\/B978-0-12-809556-0.00004-6","2019","Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma"],["The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.","Babak Sohrabi and Ahmad Khalilijafarabad","https:\/\/doi.org\/10.1016\/j.techfore.2018.08.003","2018","Systematic method for finding emergence research areas as data quality"],["Streaming Data Analysis (SDA) of Big Data Streams (BDS) for Condition Based Maintenance (CBM) in the context of Rail Transportation Systems (RTS) is a state-of-the-art field of re- search. SDA of BDS is the problem of analyzing, modeling and extracting information from huge amounts of data that continuously come from several sources in real time through com- putational aware solutions. Among others, CBM for Rail Transportation is one of the most challenging SDA problems, consisting of the implementation of a predictive maintenance system for evaluating the future status of the monitored assets in order to reduce risks related to failures and to avoid service disruptions. The challenge is to collect and analyze all the data streams that come from the numerous on-board sensors monitoring the assets. This paper deals with the problem of CBM applied to the condition monitoring and predictive maintenance of train axle bearings based on sensors data collection, with the purpose of maximizing their Remaining Useful Life (RUL). In particular we propose a novel algorithm for CBM based on SDA that takes advantage of the Online Support Vector Regression (OL-SVR) for predicting the RUL. The novelty of our proposal is the heuristic approach for optimizing the trade-off between the accuracy of the OL-SVR models and the computational time and resources needed in order to build them. Results from tests on a real-world dataset show the actual benefits brought by the proposed methodology.","Emanuele Fumeo and Luca Oneto and Davide Anguita","https:\/\/doi.org\/10.1016\/j.procs.2015.07.321","2015","Condition Based Maintenance in Railway Transportation Systems Based on Big Data Streaming Analysis"],["Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive\u2014i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.","Simon Elias Bibri and John Krogstie","https:\/\/doi.org\/10.1016\/j.scs.2017.04.012","2017","ICT of the new wave of computing for sustainable urban forms: Their big data and context-aware augmented typologies and design concepts"],[null,"George A. Heckman and John P. Hirdes and Robert S. McKelvie","https:\/\/doi.org\/10.1016\/j.cjca.2019.09.018","2020","The Role of Physicians in the Era of Big Data"],["Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.","Anna Bernasconi","https:\/\/doi.org\/10.1016\/j.cmpbup.2021.100009","2021","Data quality-aware genomic data integration"],["Purpose\nTo characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.\nDesign\nReview of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.\nMethods\nStatistical analysis of observational data.\nResults\nThe overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P\u00a0= .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1\u20137\u00a0days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20\/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20\/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20\/40.\nConclusions\nBig Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20\/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.","Anne Louise Coleman","https:\/\/doi.org\/10.1016\/j.ajo.2015.09.028","2015","How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture"],["Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms.","Ivana Semanjski and Sidharta Gautama and Rein Ahas and Frank Witlox","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2017.07.004","2017","Spatial context mining approach for transport mode recognition from mobile sensed big data"],["This final chapter reviews best practices for incrementally adopting big data into the enterprise. The chapter revisits assessing the need and value of big data, organizational buy-in, building the big data team, scoping and piloting a proof of concept, technology evaluation and selection, application development, testing, and implementation, platform and project scoping, the big data integration plan, management and maintenance, assessment of success criteria, and overall summary and considerations.","David Loshin","https:\/\/doi.org\/10.1016\/B978-0-12-417319-4.00011-9","2013","Chapter 11 - Developing the Big Data Roadmap"],["Although large volumes of information are entered into our electronic health care records, radiation oncology information systems and treatment planning systems on a daily basis, the goal of extracting and using this big data has been slow to emerge. Development of strategies to meet this goal is aided by examining issues with a data farming instead of a data mining conceptualization. Using this model, a vision of key data elements, clinical process changes, technology issues and solutions, and role for professional societies is presented. With a better view of technology, process and standardization factors, definition and prioritization of efforts can be more effectively directed.","Charles S. Mayo and Marc L. Kessler and Avraham Eisbruch and Grant Weyburne and Mary Feng and James A. Hayman and Shruti Jolly and Issam {El Naqa} and Jean M. Moran and Martha M. Matuszak and Carlos J. Anderson and Lynn P. Holevinski and Daniel L. McShan and Sue M. Merkel and Sherry L. Machnak and Theodore S. Lawrence and Randall K. {Ten Haken}","https:\/\/doi.org\/10.1016\/j.adro.2016.10.001","2016","The big data effort in radiation oncology: Data mining or data farming?"],["This study aims to understand the assessment of basic education in the perspective of the State Reviewer as a mechanism that generates information regarding the positivity and weaknesses of a school or an educational system to provide improvements. For this reason, a Data Warehouse was created and later some analysis of the indicators were performed through clustering.","Thiago Graca Ramos and Jean Cristian Ferreira Machado and Bruna Principe Vieira Cordeiro","https:\/\/doi.org\/10.1016\/j.procs.2015.07.061","2015","Primary Education Evaluation in Brazil Using Big Data and Cluster Analysis"],["Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.","Ajay Kumar and Ravi Shankar and Lakshman S. Thakur","https:\/\/doi.org\/10.1016\/j.jocs.2017.06.006","2018","A big data driven sustainable manufacturing framework for condition-based maintenance prediction"],["The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.","Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang","https:\/\/doi.org\/10.1016\/j.dss.2020.113381","2020","Minimizing the data quality problem of information systems: A process-based method"],["The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict\/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.","Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz","https:\/\/doi.org\/10.1016\/j.bdr.2019.04.001","2019","Anomaly Detection and Repair for Accurate Predictions in Geo-distributed Big Data"],["This chapter considers the specific issues that Big Data presents for law enforcement agencies (LEAs). In particular, it looks at the dilemmas created for LEAs seeking to use the advantages Big Data gives them while remaining compliant with the developing legal framework governing privacy and the protection of personal data, and how those very advantages can present challenges in law enforcement.","Fraser Sampson","https:\/\/doi.org\/10.1016\/B978-0-12-801967-2.00015-X","2015","Chapter 15 - The Legal Challenges of Big Data Application in Law Enforcement"],["In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.","Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das","https:\/\/doi.org\/10.1016\/B978-0-12-818146-1.00009-X","2019","Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data Analytics"],["Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.","Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.01.005","2019","A hybrid IT framework for identifying high-quality physicians using big data analytics"],[null,"Sadok Ben Yahia and Anne Laurent and Gabriella Pasi","https:\/\/doi.org\/10.1016\/j.fss.2018.05.022","2018","Preface: Special Issue on Big Data"],["Abstract:\nIoT connects devices, humans, places, and even abstract items like events. Driven by smart sensors, powerful embedded microelectronics, high-speed connectivity and the standards of the internet, IoT is on the brink of disrupting today's value chains. Big Data, characterized by high volume, high velocity and a high variety of formats, is a result of and also a driving force for IoT. The datafication of business presents completely new opportunities and risks. To hedge the technical risks posed by the interaction between \u201ceverything\u201d, IoT requires comprehensive modelling tools. Furthermore, new IT platforms and architectures are necessary to process and store the unprecedented flow of structured and unstructured, repetitive and non-repetitive data in real-time. In the end, only powerful analytics tools are able to extract \u201csense\u201d from the exponentially growing amount of data and, as a consequence, data science becomes a strategic asset. The era of IoT relies heavily on standards for technologies which guarantee the interoperability of everything. This paper outlines some fundamental standardization activities. Big Data approaches for real-time processing are outlined and tools for analytics are addressed. As consequence, IoT is a (fast) evolutionary process whose success in penetrating all dimensions of life heavily depends on close cooperation between standardization organizations, open source communities and IT experts.","Norbert Jesse","https:\/\/doi.org\/10.1016\/j.ifacol.2016.11.079","2016","Internet of Things and Big Data \u2013 The Disruption of the Value Chain and the Rise of New Software Ecosystems"],["Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.","Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch","https:\/\/doi.org\/10.1016\/j.jeconom.2019.04.019","2019","Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices"],["Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.","Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard","https:\/\/doi.org\/10.1016\/bs.pmch.2017.12.003","2018","Chapter Five - Big Data in Drug Discovery"],["The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.\nResumen\nLa aparici\u00f3n de los sistemas de informaci\u00f3n cl\u00ednica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos cl\u00ednicos en formato electr\u00f3nico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas cl\u00ednicas, para su uso en la gesti\u00f3n de recursos o para sugerir l\u00edneas de investigaci\u00f3n que luego pueden ser explotadas mediante ensayos cl\u00ednicos aleatorizados. Sin embargo, los m\u00e9dicos cl\u00ednicos carecen de la formaci\u00f3n necesaria para la explotaci\u00f3n de grandes bases de datos, lo que supone un obst\u00e1culo para aprovechar esta oportunidad. Adem\u00e1s, existen cuestiones de \u00edndole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estad\u00edsticos, epidemi\u00f3logos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigaci\u00f3n cl\u00ednica o an\u00e1lisis de resultados (benchmarking). Se describe la reuni\u00f3n multidisciplinar (Critical Care Datathon) realizada en Madrid los d\u00edas 1, 2 y 3 de diciembre de 2017. Esta reuni\u00f3n, celebrada bajo los auspicios de la Sociedad Espa\u00f1ola de Medicina Intensiva, Cr\u00edtica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovaci\u00f3n y el Servicio de Medicina Intensiva del Hospital Cl\u00ednico San Carlos, as\u00ed como el grupo de investigaci\u00f3n \u00abLife Supporting Technologies\u00bb de la Universidad Polit\u00e9cnica de Madrid. Tras unas ponencias de formaci\u00f3n sobre big data, seguridad y calidad de los datos, y su aplicaci\u00f3n al entorno de la medicina intensiva, un grupo de cl\u00ednicos, analistas de datos, estad\u00edsticos, expertos en seguridad inform\u00e1tica de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas cl\u00ednicas establecidas previamente a la reuni\u00f3n. El trabajo colaborativo permiti\u00f3 establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias l\u00edneas de investigaci\u00f3n cl\u00ednica a desarrollar en el futuro. Adem\u00e1s, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en Espa\u00f1a, y se estableci\u00f3 un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparaci\u00f3n de resultados entre unidades o la elaboraci\u00f3n de nuevas l\u00edneas de investigaci\u00f3n cl\u00ednica.","Antonio {N\u00fa\u00f1ez Reiz} and Fernando {Mart\u00ednez Sagasti} and Manuel {\u00c1lvarez Gonz\u00e1lez} and Antonio {Blesa Malpica} and Juan Carlos {Mart\u00edn Ben\u00edtez} and Mercedes {Nieto Cabrera} and \u00c1ngela {del Pino Ram\u00edrez} and Jos\u00e9 Miguel {Gil Perdomo} and Jes\u00fas {Prada Alonso} and Leo Anthony Celi and Miguel \u00c1ngel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {Gonz\u00e1lez Ferrer} and \u00c1ngel Rey and Henar {Gonz\u00e1lez Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura L\u00f3pez and Beatriz Merino and Mar\u00eda Fernanda Cabrera and Mar\u00eda Teresa Arredondo and Mar\u00eda Bod\u00ed and Josep G\u00f3mez and Alejandro Rodr\u00edguez and Miguel {S\u00e1nchez Garc\u00eda}","https:\/\/doi.org\/10.1016\/j.medin.2018.06.002","2019","Big data and machine learning in critical care: Opportunities for collaborative research"],["We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.","Dominique Duncan and Paul Vespa and Asla Pitk\u00e4nen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga","https:\/\/doi.org\/10.1016\/j.nbd.2018.05.026","2019","Big data sharing and analysis to advance research in post-traumatic epilepsy"],["The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.","Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia","https:\/\/doi.org\/10.1016\/j.procs.2019.09.223","2019","Data quality in ETL process: A preliminary study"],["Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies.","Rajiv Ranjan and Dhavalkumar Thakker and Armin Haller and Rajkumar Buyya","https:\/\/doi.org\/10.1016\/j.future.2017.06.032","2017","A note on exploration of IoT generated big data using semantics"],["A concussion is an invisible and poorly understood mild traumatic brain injury (mTBI) that can alter the way the brain functions. Patients who have screened positive for mTBI are at an increased risk of depression, post-traumatic stress disorder (PTSD), headaches, sleep disorders, and other neurological and psychological problems. Early detection of psychological conditions such as PTSD following a concussion might improve the overall outcome of a patient and could potentially reduce the cost associated with intense interventions often required when conditions go untreated for a long time. Statistical and predictive models that leverage large-scale clinical repositories and use pre-existing conditions to determine the probability of a patient developing psychological conditions following a concussion have not been widely studied. This paper presents an SVM-based model that has been trained with a longitudinal dataset of over 5.3 million clinical encounters of 89,840 service members that have sustained a concussion. The model has been tested and validated with over 16,045 patients that developed PTSD and it has shown an accuracy of over 85% (AUC of 86.52%) at predicting the condition within the first year following the injury.","Filip Dabek and Jesus J. Caban","https:\/\/doi.org\/10.1016\/j.procs.2015.07.303","2015","Leveraging Big Data to Model the Likelihood of Developing Psychological Conditions After a Concussion"],["Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.","Sunho Kim and Ricardo P\u00e9rez-Castillo and Ismael Caballero and Downgwoo Lee","https:\/\/doi.org\/10.1016\/j.jii.2021.100256","2022","Organizational process maturity model for IoT data quality management"],["The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering\/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.","J.G. Enr\u00edquez and F.J. Dom\u00ednguez-Mayo and M.J. Escalona and M. Ross and G. Staples","https:\/\/doi.org\/10.1016\/j.eswa.2017.03.010","2017","Entity reconciliation in big data sources: A systematic mapping study"],["For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.","Alexander Markowetz and Konrad B\u0142aszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer","https:\/\/doi.org\/10.1016\/j.mehy.2013.11.030","2014","Psycho-Informatics: Big Data shaping modern psychometrics"],["Sustainability is a dynamic, complex and composite data relationship among geographically distributed human and environment ecosystems. The ecosystems may have strong interactions among their elements and processes, but with dynamic implicit boundaries. Multi-scalable and multidimensional ecosystems have significance based on a commonality of basic structural units and domains. We intend to develop a holistic information system for managing different ecosystems within a sustainability framework\/context, using an empirical qualitative and quantitative interpretation and analysis of the measured observations. Design Science Research (DSR) approach is aimed at developing an information system using the volumes of unstructured Big Data observations. Collaborating multiple domains, interpreting and evaluating the commonality, uncovering the connectivity among multiple systems are key aspects of the study. The Design Science Information System (DSIS), evolved from DSR approach is used in solving the ecosystem issues associated with multiple domains, in which the sustainability challenges manifest. In this context, we propose a human-environment-economic ecosystem (HEES) framework consisting of human, environment and economic elements and processes. In broad terms, human, environment and economic domains are conceptualized as different players\/agents that operate within a range of sustainability scenarios. This approach recognizes the existing constraints of the systems as well as the emerging knowledge of the boundaries of ecosystems and their connectivity. The connectivity and interaction among the systems are analyzed by data mining, visualization and interpretation artefacts within a sustainability policy framework.","Shastri L Nimmagadda and Torsten Reiners and and {Gary Burke}","https:\/\/doi.org\/10.1016\/j.procs.2017.08.233","2017","Big Data Guided Design Science Information System (DSIS) Development for Sustainability Management and Accounting"],["This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user\u2019s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.","Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio","https:\/\/doi.org\/10.1016\/j.eswa.2015.06.050","2015","DQ2S \u2013 A framework for data quality-aware information management"],[null,"P. Mackie and F. Sim and C. Johnman","https:\/\/doi.org\/10.1016\/j.puhe.2015.02.013","2015","Big data! Big deal?"],["This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.","Laura Sebastian-Coleman","https:\/\/doi.org\/10.1016\/B978-0-12-821737-5.00009-2","2022","Chapter 9 - Core Data Quality Management Capabilities"],["Under the background of big data era today, once been widely used method \u2013 multiple linear regressions can not satisfy people's need to handle big data any more because of its bad characteristics such as multicollinearity, instability, subjectivity in model chosen etc. Contrary to MLR, LASSO method has many good natures. it is stable and can handle multicollinearity and successfully select the best model and do estimation in the same time. LASSO method is an effective improvement of multiple linear regressions. It is a natural change and innovation to introduce LASSO method into the accounting field and use it to deal with the debt costs problems. It helps us join the statistic field and accounting field together step by step. What's more, in order to proof the applicability of LASSO method in dealing with debt costs problems, we take 2301 companies\u2019 data from Shanghai and Shenzhen A-share market in 2012 as samples, and chose 18 indexes to verify that the results of LASSO method is scientific, reasonable and accurate. In the end, we compare LASSO method with traditional multiple linear regressions and ridge regression, finding out that LASSO method can not only offer the most accurate prediction but also simplify the model.","Li Lin and Wang Shuang and Liu Yifang and Wang Shouyang","https:\/\/doi.org\/10.1016\/j.procs.2014.05.299","2014","A New Idea of Study on the Influence Factors of Companies\u2019 Debt Costs in the Big Data Era"],["In the modern society, there are a rich number of low-carbon enterprises that the explicitly\/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs\u2019 cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly\/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions\/correlations of million-scale enterprises.","Yi Liu and Jie Xu and Weijie Yi","https:\/\/doi.org\/10.1016\/j.future.2021.01.002","2021","Massive-scale carbon pollution control and biological fusion under big data context"],["When the idea for this book was originally conceived, Big Data and Hadoop were not the most popular themes on the tech circuit, although cloud computing was somewhat more prominent. Some of the reviewer feedback suggested that these topics should be addressed in the context of the conceptual layout of analytics solutions. In this chapter their use in an overall analytics solution will be explained using the previous chapters as a foundation. Big Data, Hadoop, and cloud computing are presented as standalone material, each tying back into the overall analytics solution implementations presented in preceding chapters.","Nauman Sheikh","https:\/\/doi.org\/10.1016\/B978-0-12-401696-5.00011-6","2013","Chapter 11 - Big Data, Hadoop, and Cloud Computing"],["Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.","Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh","https:\/\/doi.org\/10.1016\/j.apenergy.2020.116057","2021","Investments in data quality: Evaluating impacts of faulty data on asset management in power systems"],[null,null,"https:\/\/doi.org\/10.1016\/j.denabs.2015.10.005","2016","Clinical research and big data"],["With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting\/managing\/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.","Xiaohong Huang and Kun Xie and Supeng Leng and Tingting Yuan and Maode Ma","https:\/\/doi.org\/10.1016\/j.future.2018.02.046","2018","Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data"],[null,"Suzanne McDermott and Margaret A. Turk","https:\/\/doi.org\/10.1016\/j.dhjo.2015.04.003","2015","What are the implications of the big data paradigm shift for disability and health?"],["This paper presents a bi-level mathematical programming model for the data-pricing problem that considers both data quality and data versioning strategies. Data products and data-related services differ from information products or services in terms of quality assessment methods. For this problem, we consider two aspects of data quality: (1) its multidimensionality and (2) the interaction between the dimensions. We designed a multi-version data strategy and propose a data-pricing bi-level programming model based on the data quality to maximize the profit by the owner of the data platform and the utility to consumers. A genetic algorithm was used to solve the model. The numerical solutions for the data-pricing model indicate that the multi-version strategy achieves a better market segmentation and is more profitable and feasible when the multiple dimensions of data quality are considered. These results also provide managerial guidance on data provision and data pricing for platform owners.","Haifei Yu and Mengxiao Zhang","https:\/\/doi.org\/10.1016\/j.cie.2017.08.008","2017","Data pricing strategy based on data quality"],["Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.","David Gil and Il-Yeol Song and Jos\u00e9 F. Aldana and Juan Trujillo","https:\/\/doi.org\/10.1016\/j.csi.2017.03.006","2017","Big Data. New approaches of modelling and management"],[null,"Xian-He Sun and Marc Frincu and Charalampos Chelmis","https:\/\/doi.org\/10.1016\/j.jpdc.2017.05.020","2017","Special Issue on Scalable Computing Systems for Big Data Applications"],["Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.","Claudia Vitolo and Yehia Elkhatib and Dominik Reusser and Christopher J.A. Macleod and Wouter Buytaert","https:\/\/doi.org\/10.1016\/j.envsoft.2014.10.007","2015","Web technologies for environmental Big Data"],["Nowadays, companies are facing challenges due to increasingly dynamic market environments, a growing internal and external complexity, as well as globally intensifying competition. To keep pace, companies need to establish extensive knowledge about their business and its surroundings based on insights generated through the analysis of data. The digital shadow is a novel information system concept that integrates data of heterogeneous sources to provide product-related information to stakeholders across the company. The concept aims at improving the results of decision making, enabling advanced data analyses, and increasing information handling efficiency. As insufficient information quality has immediate effects on the utility of the information and induces significant costs, managing the quality of the digital shadow data basis is crucial. However, there are currently no comprehensive methodologies for the assessment and improvement of the data quality of digital shadows. Therefore, this paper introduces a methodology that supports the derivation of data quality projects aimed at optimizing the digital shadow data basis. The proposed methodology comprises four steps: First, digital shadow use cases along the product lifecycle are described. Next, the use cases are prioritized with regard to the expected benefits of applying the digital shadow. Third, quality deficiencies in the digital shadow data basis are assessed with respect to use case specific requirements. Finally, the prioritized use cases in relation with the identified quality deficits allow deriving needs for action, which are addressed by data quality projects. Together, the data quality projects constitute a data quality program. The methodology is applied in an industry case to prove the practical effectivity and efficiency.","G\u00fcnther Schuh and Eric Rebentisch and Michael Riesener and Thorben Ipers and Christian T\u00f6nnes and Merle-Hendrikje Jank","https:\/\/doi.org\/10.1016\/j.procir.2020.01.027","2019","Data quality program management for digital shadows of products"],["A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.","Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian","https:\/\/doi.org\/10.1016\/j.autcon.2021.104070","2022","BIM-integrated portfolio-based strategic asset data quality management"],["The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility.","Markus Hammer and Ken Somers and Hugo Karre and Christian Ramsauer","https:\/\/doi.org\/10.1016\/j.procir.2017.03.094","2017","Profit Per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure"],["Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization \u2013 increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.","Mandy Chessell and Dan Wolfson and Tim Vincent","https:\/\/doi.org\/10.1016\/B978-0-12-805467-3.00003-X","2017","Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture"],["The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.","David Gil and Il-Yeol Song","https:\/\/doi.org\/10.1016\/j.future.2015.07.019","2016","Modeling and Management of Big Data: Challenges and opportunities"],["This chapter deals with how to implement information life-cycle management principles to Big Data and create a sustainable process that will ensure that business continuity is not interrupted and data is available on demand.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-405891-0.00012-X","2013","Chapter 12 - Information Management and Life Cycle for Big Data"],["Why this book? Why now? The goal of this book is to provide readers with a concise perspective into the biggest buzz in the industry\u2014Big Data\u2014and, more importantly, its impact on data processing, management, decision support, and data warehousing. At the time of this writing, there is a lot of interest to adopt a Big Data solution, but the profound confusion is what is the future of data warehousing and many investments that have been made over the years into building the decision support platform. This book addresses those areas of concern and provides readers an introduction to the next-generation of data management and data warehousing. This chapter provides you a concise and example driven introduction to what is Big Data, and how any organization needs to understand the value of Big Data.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-405891-0.00001-5","2013","Chapter 1 - Introduction to Big Data"],["Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.","Haluk Demirkan and Dursun Delen","https:\/\/doi.org\/10.1016\/j.dss.2012.05.048","2013","Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud"],["This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.","Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou","https:\/\/doi.org\/10.1016\/j.joi.2014.05.001","2014","Knowledge diffusion path analysis of data quality literature: A main path analysis"],["This chapter expands on the previous one by looking at some key issues that often plague new technology adoption and show that the key issues are not new ones, and that there is likely to be organizational knowledge that can help in fleshing out a reasonable strategic plan. We look at the typical hype cycle, and how its flaws can be mitigated by instituting good practices for defining expectations and continuing to measure performance. We help define the acceptability criteria for evaluating the result of a big data pilot that can be used to make a go\/no-go decision. We then pose some thoughts about preparing the organization for massive scalability, data reuse, and the need for oversight and governance. The objective is to provide a pathway for mainstreaming big data into the technology infrastructure that is integrated with the existing investment.","David Loshin","https:\/\/doi.org\/10.1016\/B978-0-12-417319-4.00004-1","2013","Chapter 4 - Developing a Strategy for Integrating Big Data Analytics into the Enterprise"],["This chapter describes the five challenges in data quality management (data, process, technology, people, and culture\/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today\u2019s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.","Laura Sebastian-Coleman","https:\/\/doi.org\/10.1016\/B978-0-12-821737-5.00002-X","2022","Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality"],["Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.","Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha R\u00f6ning","https:\/\/doi.org\/10.1016\/j.procs.2019.08.035","2019","How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected from Wearables"],["Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research\u2014including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.","Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens","https:\/\/doi.org\/10.1016\/j.survophthal.2016.01.003","2016","Big data and ophthalmic research"],["The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.","Truong Nguyen and Li ZHOU and Virginia Spiegler and Petros Ieromonachou and Yong Lin","https:\/\/doi.org\/10.1016\/j.cor.2017.07.004","2018","Big data analytics in supply chain management: A state-of-the-art literature review"],["Big Data pervades nearly all areas of life sciences, yet the analysis of large integrated data sets remains a major challenge. Moreover, the field of life sciences is highly fragmented and, consequently, so is its data, knowledge, and standards. This, in turn, makes integrated data analysis and knowledge gathering across sub-fields a demanding task. At the same time, the integration of various research angles and data types is crucial for modelling the complexity of organisms and biological processes in a holistic manner. This is especially valid in the context of drug development and chemical safety assessment where computational methods can provide solutions for the urgent need of fast, effective, and sustainable approaches. At the same time, such computational methods require the development of methodologies suitable for an integrated and data centred Big Data view. Here we discuss Knowledge Graphs (KG) as a solution to a data centred analysis approach for drug and chemical development and safety assessment. KGs are knowledge bases, data analysis engines, and knowledge discovery systems all in one, allowing them to be used from simple data retrieval, over meta-analysis to complex predictive and knowledge discovery systems. Therefore, KGs have immense potential to advance the data centred approach, the re-usability, and informativity of data. Furthermore, they can improve the power of analysis, and the complexity of modelled processes, all while providing knowledge in a natively human understandable network data model.","Alisa Pavel and Laura A. Saarim\u00e4ki and Lena M\u00f6bus and Antonio Federico and Angela Serra and Dario Greco","https:\/\/doi.org\/10.1016\/j.csbj.2022.08.061","2022","The potential of a data centred approach & knowledge graph data representation in chemical safety and drug design"],["Big data has been used in the health sector to improve the quality of life, predict epidemics, cure diseases, and avoid preventable deaths, beyond increasing profits or reducing the burden of excess labor. Data sources in healthcare have become quite diversified and accessible to individuals, such as wearable and implantable devices, smartphones, and real-time sensors. When combined with existing health data, daily (even instantaneous) data from these devices can be used to predict future health conditions of individuals and to identify necessary intervention points. This chapter discusses a number of recent studies that introduces methods for using big data to create intelligent systems for patient diagnosis, triage, predicting lab results, and even detecting tumors. These studies open ways for researchers in the healthcare sector to improve the quality of services provided to the patients as well as reducing costs for the healthcare institutions.","Ozge Doguc and Zehra Nur Canbolat and Gokhan Silahtaroglu","https:\/\/doi.org\/10.1016\/B978-0-323-91907-4.00006-6","2022","Chapter 9 - Recent applications of data mining in medical diagnosis and prediction"],["The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.","Devarshi Shah and Jin Wang and Q. Peter He","https:\/\/doi.org\/10.1016\/j.ifacol.2019.06.122","2019","An Internet-of-things Enabled Smart Manufacturing Testbed"],["Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.","Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana","https:\/\/doi.org\/10.1016\/j.envsoft.2022.105327","2022","A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions"],["Big Data is not synonymous with lots and lots of data. Useful Big Data resources adhere to a set of data management principles that are fundamentally different from the traditional practices followed for small data projects. The areas of difference include: data collection; data annotation (including metadata and identifiers); location and distribution of stored data; classification of data; data access rules; data curation; data immutability; data permanence; verification and validity methods for the contained data; analytic methods; costs; and incumbent legal, social, and ethical issues. Skilled professionals who are adept in the design and management of small data resources may be unprepared for the unique challenges posed by Big Data. This chapter is an introduction to topics that will be fully explained in later chapters.","Jules J. Berman","https:\/\/doi.org\/10.1016\/B978-0-12-815609-4.00001-7","2018","1 - Introduction"],["Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.","Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen","https:\/\/doi.org\/10.1016\/j.jcss.2016.02.005","2017","A dynamic prime number based efficient security mechanism for big sensing data streams"],["Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.","Karthika D.","https:\/\/doi.org\/10.1016\/j.matpr.2021.06.046","2021","A study on artificial intelligence for monitoring smart environments"],["Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.","Guy {De Tr\u00e9} and Robin {De Mol} and Antoon Bronselaer","https:\/\/doi.org\/10.1016\/j.ins.2017.09.008","2018","Handling veracity in multi-criteria decision-making: A multi-dimensional approach"],["This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis of emerging Big Data and data intensive technologies and provides the general definition of the Big Data Architecture Framework (BDAF) that includes the following components: Big Data definition, Data Management including data lifecycle and data structures, generically cloud based BDI, Data Analytics technologies and platforms, and Big Data security, compliance, and privacy. The chapter refers to NIST Big Data Reference Architecture (BDRA) and summarizes general requirements to Big Data systems described in NIST documents. The proposed BDI and its cloud-based components are defined in accordance with the NIST BDRA and BDAF. This chapter provides detailed analysis of the two bioinformatics use cases as typical example of the Big Data applications that have being developed by the authors in the framework of the CYCLONE project. The effective use of cloud for bioinformatics applications requires maximum automation of the applications deployment and management that may include resources from multiple clouds and providers. The proposed CYCLONE platform for multicloud multiprovider applications deployment and management is based on the SlipStream cloud automation platform and includes all necessary components to build and operate complex scientific applications. The chapter discusses existing platforms for cloud powered applications development and deployment automation, in particularly referring to the SlipStream cloud automation platform, which allows multicloud applications deployment and management. The chapter also includes a short overview of the existing Big Data platforms and services provided by the major cloud services providers which can be used for fast deployment of customer Big Data applications using the benefits of cloud technologies and global cloud infrastructure.","Yuri Demchenko and Fatih Turkmen and Cees {de Laat} and Ching-Hsien Hsu and Christophe Blanchet and Charles Loomis","https:\/\/doi.org\/10.1016\/B978-0-12-809393-1.00002-7","2017","Chapter 2 - Cloud Computing Infrastructure for Data Intensive Applications"],["Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.","Erisa Karafili and Konstantina Spanaki and Emil C. Lupu","https:\/\/doi.org\/10.1016\/j.compind.2017.09.002","2018","An argumentation reasoning approach for data processing"],["Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts\u2019 perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.","Ahmed O. Oyedele and Anuoluwapo O. Ajayi and Lukumon O. Oyedele","https:\/\/doi.org\/10.1016\/j.mlwa.2021.100158","2021","Machine learning predictions for lost time injuries in power transmission and distribution projects"],["Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.","Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro","https:\/\/doi.org\/10.1016\/j.jbusres.2020.05.053","2020","Assessing the drivers of machine learning business value"],["After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.","Chengxu Li and Wenmin Fei and Yang Han and Xiaoli Ning and Ziyi Wang and Keke Li and Ke Xue and Jingkai Xu and Ruixing Yu and Rusong Meng and Feng Xu and Weimin Ma and Yong Cui","https:\/\/doi.org\/10.1016\/j.imed.2021.04.003","2021","Construction of an artificial intelligence system in dermatology: effectiveness and consideration of Chinese Skin Image Database (CSID)"],["Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.","Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth","https:\/\/doi.org\/10.1016\/j.dcan.2017.10.002","2018","Machine learning for internet of things data analysis: a survey"],["The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.","Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik","https:\/\/doi.org\/10.1016\/j.technovation.2022.102477","2022","What's driving the diffusion of next-generation digital technologies?"],["This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2\u03c3 range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.","Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng","https:\/\/doi.org\/10.1016\/j.est.2021.103342","2021","Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation"],["The importance of healthcare technologies has been made clear in the current pandemic. Healthcare informatics plays an important role in facilitating healthcare and providing healthcare services in real time. Healthcare informatics has developed from Healthcare 1.0 to Healthcare 4.0 in the last few decades. The data generated from the various sources are stored as electronic health record. These data are collected in different forms and formats. The inconsistent data could be handled using various techniques of big data. The information obtained from big data analytics can be used for the prediction of diseases or conditions using artificial intelligence, machine learning, and deep learning techniques. 5G plays an important role in healthcare informatics by enabling real-time remote monitoring and improving augmented reality, virtual reality, and spatial computing. With 5G technologies, a large number of devices can be connected using high-performance computing over large distances to provide healthcare services. Blockchain is applied in healthcare for health record management, insurance claims, drug tracking, authentication, and ensuring the integrity of medical data. Deep learning techniques can be applied to ever-changing data for the detection and prevention of disease. For the classification challenge, deep convolutional neural networks using pictures of diseased regions are often utilized. In many research techniques, AlexNet and GoogLeNet have been utilized to identify plant diseases. This chapter discusses the state of the art for detecting human sickness as well as the associated 5G healthcare framework for improving it.","Rahil Parmar and Dhruval Patel and Naitik Panchal and Uttam Chauhan and Jitendra Bhatia","https:\/\/doi.org\/10.1016\/B978-0-323-90615-9.00016-5","2022","18 - 5G-enabled deep learning-based framework for healthcare mining: State of the art and challenges"],["Objectives\nThis study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.\nMethods\nThe study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.\nResults\nSecondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.\nConclusions\nAlthough healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed\u2014data access\/sharing and data quality.","Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu","https:\/\/doi.org\/10.1016\/j.vhri.2021.05.002","2022","Real-World Data for Healthcare Research in China: Call for Actions"],["Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.","Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi","https:\/\/doi.org\/10.1016\/j.jbusres.2021.03.054","2021","Examining the determinants of successful adoption of data analytics in human resource management \u2013 A framework for implications"],["Maintaining equipment is critical for increasing production capacity and decreasing production time. With the advent of digitalization, industries are able to access massive amounts of data that can be used to ensure their long-term viability and competitive advantage by implementing predictive maintenance. Therefore, this study aims to demonstrate a predictive maintenance application for a robot cell using real-world manufacturing big data coming from a company in the automotive industry. A hyperparameter tuned Long Short-Term Memory (LSTM) model is developed, and the results show that this model is capable of predicting the day of failure with good accuracy. The difficulties inherent in conducting real-world industrial initiatives are analyzed, and recommendations for improvement are presented.","Doyel Joseph and Tilani Gallege and Ebru Turanoglu Bekar and Catarina Dudas and Anders Skoogh","https:\/\/doi.org\/10.1016\/j.ifacol.2022.09.193","2022","A Predictive Maintenance Application for A Robot Cell using LSTM Model"],["This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.","Nisreen Ameen and Sameer Hosany and Ali Tarhini","https:\/\/doi.org\/10.1016\/j.chb.2021.106761","2021","Consumer interaction with cutting-edge technologies: Implications for future research"],["Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10\u202fyears, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network \u2013 i.e. the presence of eminent contributors \u2013 were the variables that contributed more to the forecasting of international airport arrivals.","Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella","https:\/\/doi.org\/10.1016\/j.dss.2019.113075","2019","Using social network and semantic analysis to analyze online travel forums and forecast tourism demand"],["Introduction\nUntil the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.\nMethods\nThis non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients\u2019 characteristics, and assessment of treatment outcomes.\nResults\nWe analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N\u202f=\u202f4138, 18.2 %) were diagnosed as IIIA (N\u202f=\u202f2,547, 11.2 %) or IIIB (N\u202f=\u202f1,591. 7.0 %). Treated stage III patients (N\u202f=\u202f2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N\u202f=\u202f1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N\u202f=\u202f648, 25.6 %); palliative CT (N\u202f=\u202f270, 10.7 %), or thoracic RT (N\u202f=\u202f170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0\u201356.5), 52.5 (43.1\u201361.9), 30.3 (26.6\u201334.0), 14.7 (13.0\u201316.4), 8.8 (6.2\u201311.3), and 2.0 (1.0\u20133.0) months, respectively.\nConclusions\nThis unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.","Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park","https:\/\/doi.org\/10.1016\/j.lungcan.2020.05.033","2020","Ten-year patient journey of stage III non-small cell lung cancer patients: A single-center, observational, retrospective study in Korea (Realtime autOmatically updated data warehOuse in healTh care; UNIVERSE-ROOT study)"],["In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science\u2014a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.","Bing Wang","https:\/\/doi.org\/10.1016\/j.psep.2020.10.008","2021","Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework"],["The authenticity and quality of industrial statistical data directly affects all types of systematic research based on it. Considering the limitations of extant data quality evaluation literature on research objects and evaluation methods, we constructed a new data quality comprehensive inspection and evaluation model based on Benford's Law (BL) and the technique for order of preference by similarity to ideal solution (TOPSIS), selected coal-related industries as the research object, and conducted an empirical test along the research path of \u201cIndustry\u2192Province\u2192Indicator\u201d. The results showed that, at industry level, the quality of statistical data for China's coal-related industries from 2001 to 2016 was generally poor. Among the eight sample industries selected, the data quality for five industries (including coal, electricity, and steel) was assessed as poor or slightly poor. Furthermore, at the provincial level, there is significant spatial heterogeneity in the quality of statistical data for various industries affected by factors such as economic structure, marketization level, and industrial diversity. Compared with other types of statistical indicators, industry financial indicators are more prone to data quality problems at the indicator level, and the suspicious indicators of different industries show certain common characteristics and some industry differences. To improve the quality of industrial statistical data and reduce the possible adverse impacts of data quality problems, based on the research findings, we propose targeted countermeasures and suggestions on how to prevent data fraud and effectively identify and rationally use suspicious data.","Delu Wang and Fan Chen and Jingqi Mao and Nannan Liu and Fangyu Rong","https:\/\/doi.org\/10.1016\/j.eneco.2022.106310","2022","Are the official national data credible? Empirical evidence from statistics quality evaluation of China's coal and its downstream industries"],["With the advancement of technology, the world is changing and automating at a rapid pace. Digitization plays a major role in the automation of technology. In this context, Industry 4.0 shows how industrial production is developing along with the latest technology. In Industry 4.0, much manual work has been replaced by automated machines that can be controlled with developing technologies such as artificial intelligence (AI), big data, cloud computing, and so on. Various technologies have been introduced in the healthcare sector due to Industry 4.0. These include AI, three-dimensional printing, machine learning, cognitive systems, autonomous robots, autonomous vehicles, augmented reality, big data, Internet of Things (IoT), blockchain technology, and more. This chapter discusses the transformation of the healthcare industry in the context of Industry 4.0. It presents a detailed study on big data, IoT, and blockchain technology with different applications that can enhance what is known as Healthcare 4.0. The chapter includes three case studies that illustrate the use of innovation in Healthcare 4.0 to detect and diagnose disease using portable medical devices connected to the IoT.","Sumit Koul","https:\/\/doi.org\/10.1016\/B978-0-323-88468-6.00002-4","2022","Chapter 1 - Influence and implementation of Industry 4.0 in health care"],["This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.","M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec","https:\/\/doi.org\/10.1016\/j.ifacol.2020.12.299","2020","Artificial Intelligence Platform Proposal for Paint Structure Quality Prediction within the Industry 4.0 Concept"],["As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal\u2019s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.","Suresh Neethirajan and Bas Kemp","https:\/\/doi.org\/10.1016\/j.sbsr.2021.100408","2021","Digital Livestock Farming"],["Data is one component in a system. It has value. The economics of increasingly data-centric systems is explored. There is a growing reliance on data to create systems that are larger scale, have wider scope and are more complex. As reliance on the data component increases, a self-reinforcing problem of implementing checks and balances necessary to enforce appropriate levels of risk reduction arises. This chapter introduces data architecture elements of container and content. It places this architecture within an appropriate data context. It introduces the concepts of data quality and data integrity. Data integrity is placed within the Safety Management and Safety Assurance processes. Big data and machine learning are considered in this context.","Alastair Faulkner and Mark Nicholson","https:\/\/doi.org\/10.1016\/B978-0-12-820790-1.00017-6","2020","4 - Data Fundamentals"],["The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.","Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2020.102246","2021","Identifying influencers on social media"],["The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology\u2013organization\u2013environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.","Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis","https:\/\/doi.org\/10.1016\/j.chb.2018.04.014","2018","Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability"],["Observational data research studying access, utilization, cost, and outcomes of image-guided interventions using publicly available \u201cbig data\u201d sets is growing in the interventional radiology (IR) literature. Publicly available data sets offer insight into real-world care and represent an important pillar of IR research moving forward. They offer insights into how IR procedures are being used nationally and whether they are working as intended. On the other hand, large data sources are aggregated using complex sampling frames, and their strengths and weaknesses only become apparent after extensive use. Unintentional misuse of large data sets can result in misleading or sometimes erroneous conclusions. This review introduces the most commonly used databases relevant to IR research, highlights their strengths and limitations, and provides recommendations for use. In addition, it summarizes methodologic best practices pertinent to all data sets for planning and executing scientifically rigorous and clinically relevant observational research.","Premal S. Trivedi and Vincent M. Timpone and Rustain L. Morgan and Alexandria M. Jensen and Margaret Reid and P. Michael Ho and Osman Ahmed","https:\/\/doi.org\/10.1016\/j.jvir.2022.08.003","2022","A Practical Guide to Use of Publicly Available Data Sets for Observational Research in Interventional Radiology"],["With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype\u2013phenotype\u2013envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.","Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo","https:\/\/doi.org\/10.1016\/j.jclepro.2020.123651","2021","The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform"],["The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study\u2019s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.","Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen","https:\/\/doi.org\/10.1016\/j.jclepro.2021.125834","2021","Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities"],["This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics\u2014descriptive, diagnostic, predictive, and prescriptive\u2014are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, big data analytics, to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open source tools and resources for developing data analytics systems are listed. Future directions in data analytics are indicated. The chapter concludes by providing a summary. To reinforce and enhance the reader\u2019s data analytics knowledge and tools, questions and exercise problems are provided at the end of the chapter.","Venkat N. Gudivada","https:\/\/doi.org\/10.1016\/B978-0-12-809715-1.00002-X","2017","Chapter 2 - Data Analytics: Fundamentals"],["Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.","Fa Li and Zhipeng Gui and Huayi Wu and Jianya Gong and Yuan Wang and Siyu Tian and Jiawen Zhang","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2018.01.010","2018","Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China"],["Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.","Daowen Liu and Liqi Lei and Tong Ruan and Ping He","https:\/\/doi.org\/10.24920\/003579","2019","Constructing Large Scale Cohort for Clinical Study on Heart Failure with Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies in Data Reuse"],["Self-reported life satisfaction of China's population has not improved as much as expected during the economic boom, which was accompanied by a significant decline in environmental performance. Is environmental pollution the culprit for the lagging subjective well-being? To explore this issue, this paper adopts the sentiment analysis method to construct a real-time daily subjective well-being metric at the city level based on the big data of online search traces. Using daily data from 13 Chinese cities centred on Beijing between August 2014 and December 2019, we look at the corelation between subjective well-being and air pollution and the heterogeneity in this relationship based on two separate identification strategies. We find that air pollutants are negatively correlated with subjective well-being, and well-being tends to decline more from pollution during hot seasons. In addition, residents in wealthier regions tend to be more sensitive to air pollution. This result may be explained by the differences in the subjective perception of air pollution and personal preferences at different levels of income. These findings provide information about the concerns of the public to the central government, thereby helping it take appropriate actions to respond to the dynamics of subjective well-being.","Lu Cheng and Zhifu Mi and Yi-Ming Wei and Shidong Wang and Klaus Hubacek","https:\/\/doi.org\/10.1016\/j.jclepro.2022.134380","2022","Dirty skies lower subjective well-being"],["Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.","Tao Zhang and Weixi Ji and Yongtao Qiu","https:\/\/doi.org\/10.1016\/j.seta.2021.101336","2021","A framework of energy-consumption driven discrete manufacturing system"],["Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical \u00a0declination\u00a0in the Internet\u00a0of Things (IoT) costs.\u00a0Flood\u00a0research and\u00a0applications\u00a0will progress with this emerging technology, as AI is creating new\u00a0flood\u00a0data sources, enhancing our capability to analyze the data, and improving our accuracy of\u00a0flood\u00a0predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of \u201creview of the reviews\u201d with example highlight the emerging AI\u00a0applications\u00a0in the field of\u00a0flood\u00a0hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.","Ruo-Qian Wang","https:\/\/doi.org\/10.1016\/B978-0-12-819412-6.00013-4","2021","Chapter 13 - Artificial Intelligence for Flood Observation"],["The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.","Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen","https:\/\/doi.org\/10.1016\/j.ijhm.2021.102938","2021","Construction of a service quality scale for the online food delivery industry"],["The development of data-driven artificial intelligence technology has given birth to a variety of big data applications. Data has become an essential factor to improve these applications. Federated learning, a privacy-preserving machine learning method, is proposed to leverage data from different data owners. It is typically used in conjunction with cryptographic methods, in which data owners train the global model by sharing encrypted model updates. However, data encryption makes it difficult to identify the quality of these model updates. Malicious data owners may launch attacks such as data poisoning and free-riding. To defend against such attacks, it is necessary to find an approach to audit encrypted model updates. In this paper, we propose a blockchain-based audit approach for encrypted gradients. It uses a behavior chain to record the encrypted gradients from data owners, and an audit chain to evaluate the gradients\u2019 quality. Specifically, we propose a privacy-preserving homomorphic noise mechanism in which the noise of each gradient sums to zero after aggregation, ensuring the availability of aggregated gradient. In addition, we design a joint audit algorithm that can locate malicious data owners without decrypting individual gradients. Through security analysis and experimental evaluation, we demonstrate that our approach can defend against malicious gradient attacks in federated learning.","Zhe Sun and Junping Wan and Lihua Yin and Zhiqiang Cao and Tianjie Luo and Bin Wang","https:\/\/doi.org\/10.1016\/j.dcan.2022.05.006","2022","A blockchain-based audit approach for encrypted data in federated learning"],["Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.","Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan","https:\/\/doi.org\/10.1016\/j.compeleceng.2022.107691","2022","A holistic survey on the use of emerging technologies to provision secure healthcare solutions"],["Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber\u2013Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber\u2013physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.","Gifty R. and Bharathi R.","https:\/\/doi.org\/10.1016\/j.comcom.2019.11.018","2020","Weibull Cumulative Distribution based real-time response and performance capacity modeling of Cyber\u2013Physical Systems through software defined networking"],["The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.","Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson","https:\/\/doi.org\/10.1016\/j.dss.2016.07.003","2016","A snail shell process model for knowledge discovery via data analytics"],["Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.","Mar\u00e7al Farr\u00e9 and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farr\u00e9","https:\/\/doi.org\/10.1016\/B978-0-12-821122-9.00002-6","2022","Chapter 7 - Data-driven policy evaluation"],["Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV\/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected \u201cbig data\u201d from electronic health records and social media.","James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar","https:\/\/doi.org\/10.1016\/j.evalprogplan.2021.101994","2021","Advances in monitoring and evaluation in low- and middle-income countries"],["With thousands of publications per year, the volume of data published on perovskite solar cells since the spark of the \u201cperovskite fever\u201d\u00a0in 2013 is enormous and far exceeds the amount that any individual\u00a0researcher could digest. To tackle this issue, Jacobsson et\u00a0al.1 have created The Perovskite Database, which is part of a larger trend to harness the power of big data and artificial intelligence to accelerate the commercialization of perovskite solar cells.","Kameron R. Hansen and Luisa Whittaker-Brooks","https:\/\/doi.org\/10.1016\/j.matt.2022.06.001","2022","Finding the FAIRness in perovskite photovoltaics research"],["Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from fa\u00e7ade to fa\u00e7ade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.","Gabriel Valen\u00e7a and Filipe Moura and Ana {Morais de S\u00e1}","https:\/\/doi.org\/10.1016\/j.urbmob.2021.100008","2021","Main challenges and opportunities to dynamic road space allocation: From static to dynamic urban designs"],["Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.","Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang","https:\/\/doi.org\/10.1016\/j.techfore.2018.06.009","2019","Technology in the 21st century: New challenges and opportunities"],["The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.","Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan","https:\/\/doi.org\/10.1016\/j.accinf.2017.03.003","2017","Impact of business analytics and enterprise systems on managerial accounting"],["Big data is a concept that aimed at collecting, storing, processing and transforming large amount of data into value using new combination of strategies and technologies. Big data is characterized by data that have a large volume, massive velocity, numerous variety, useful value, and veracity. Big Data Analytics offers tremendous insights to different organizations especially in healthcare. Currently, Big healthcare data has the highest potential for improving patient outcomes, gaining valuable insights, predicting outbreaks of epidemics, avoiding preventable diseases and effectively minimizing the cost of healthcare delivery. However, the dynamic nature of health data presents various conceptual, technical, legal and ethical challenges associated with the data processing and analysis activities. The big data security and privacy concepts are some of the most pertinent issues and have become increasingly significant associated with big healthcare data in the modern world. In this paper, we give an overview of big data characteristics and challenges in healthcare and present big healthcare data lifecycle integrated with security threats and attacks to provide encompass policies and mechanisms that aim at solving the various security challenges in each step of big data lifecycle. The focus is also placed on the description of the recently proposed techniques related to authentication, encryption, anonymization, access control, and privacy. We finally propose an approach to secure threat model for big healthcare data lifecycle as a main contribution of this paper.","Hayat Khaloufi and Karim Abouelmehdi and Abderrahim Beni-hssane and Mostafa Saadi","https:\/\/doi.org\/10.1016\/j.procs.2018.10.199","2018","Security model for Big Healthcare Data Lifecycle"],["Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma\u2019s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.","I. Kregel and D. Stemann and J. Koch and A. Coners","https:\/\/doi.org\/10.1016\/j.cie.2020.107083","2021","Process Mining for Six Sigma: Utilising Digital Traces"],["Web and smartphone surveys are increasingly being used to collect travel information. This workshop explored respondent interaction with these tools, covering a range of research concerns. While smartphone surveys facilitate real-time passive collection of continuous data, thereby reducing respondent burden, their use raises many issues common with those present in web surveys. These include survey design, sample representativeness, privacy, respondent burden, data quality and validation. Workshop participants considered possible areas for future research on these issues and others such as provision of feedback to respondents, linking with big data and focusing on attitudinal and behavioural motivations.","Jo\u00e3o de Abreu e Silva and Mark Davis","https:\/\/doi.org\/10.1016\/j.trpro.2015.12.025","2015","Workshop Synthesis: Respondent\/Survey Interaction in a World of Web and Smartphone Apps"],["Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.","Elisabetta Raguseo and Federico Pigni and Claudio Vitari","https:\/\/doi.org\/10.1016\/j.im.2021.103451","2021","Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness"],["This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate \u2018big data\u2019, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.","Turgut Ozkan","https:\/\/doi.org\/10.1016\/j.soscij.2018.10.010","2019","Criminology in the age of data explosion: New directions"],["The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.","Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim","https:\/\/doi.org\/10.1016\/j.jpdc.2018.03.013","2019","QDaS: Quality driven data summarisation for effective storage management in Internet of Things"],["Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.","Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi","https:\/\/doi.org\/10.1016\/j.indmarman.2020.09.015","2020","Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence"],["Sensors are becoming ubiquitous in everyday life, generating data at an unprecedented rate and scale. However, models that assess impacts of human activities on environmental and human health, have typically been developed in contexts where data scarcity is the norm. Models are essential tools to understand processes, identify relationships, associations and causality, formalize stakeholder mental models, and to quantify the effects of prevention and interventions. They can help to explain data, as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results. We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing \u2018Big Data\u2019 and, more importantly, make the vital step from \u2018Big Data\u2019 to \u2018Big Information\u2019. In this paper, we illustrate current developments and identify key research needs using human and environmental health challenges as an example.","Stefan Reis and Edmund Seto and Amanda Northcross and Nigel W.T. Quinn and Matteo Convertino and Rod L. Jones and Holger R. Maier and Uwe Schlink and Susanne Steinle and Massimo Vieno and Michael C. Wimberly","https:\/\/doi.org\/10.1016\/j.envsoft.2015.06.003","2015","Integrating modelling and smart sensors for environmental and human health"],["In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.","Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui","https:\/\/doi.org\/10.1016\/j.petrol.2021.109904","2022","Downhole data correction for data-driven rate of penetration prediction modeling"],["Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.","Pramod Kumar and Jaiprakash Bhamu and Kuldip Singh Sangwan","https:\/\/doi.org\/10.1016\/j.procir.2021.01.010","2021","Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: an ISM Approach"],["Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users\u2019 process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.","Camilla Schaefer and Ana Makatsaria","https:\/\/doi.org\/10.1016\/j.ijin.2021.09.004","2021","Framework of Data Analytics and Integrating Knowledge Management"],["Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.","Wei Xiong and Li Xiong","https:\/\/doi.org\/10.1016\/j.ins.2020.10.053","2021","Anti-collusion data auction mechanism based on smart contract"],["This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human\u2013AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human\u2013artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.","Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine","https:\/\/doi.org\/10.1016\/j.techfore.2021.121201","2022","Data intelligence and analytics: A bibliometric analysis of human\u2013Artificial intelligence in public sector decision-making effectiveness"],["As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal \u201cblack box\u201d algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.","Emily Tat and Mark Rabbat","https:\/\/doi.org\/10.1016\/B978-0-12-820273-9.00017-8","2021","Chapter 17 - Ethical and legal challenges"],["Human reliability analysis plays an important role in the safety assessment and management of rail operations. This paper discusses how the increasing availability of operational data can be used to develop an understanding of train driver reliability. The paper derives human reliability data for two driving tasks, stopping at red signals and controlling speed on approach to buffer stops. In the first of these cases, a tool has been developed that can estimate the number of times a signal is approached at red by trains on the Great Britain (GB) rail network. The tool has been developed using big data techniques and ideas, recording and analysing millions of pieces of data from live operational feeds to update and summarise statistics from thousands of signal locations in GB on a daily basis. The resulting driver reliability data are compared to similar analyses of other train driving tasks. This shows human reliability approaching the currently accepted limits of human performance. It also shows higher error rates amongst freight train drivers than passenger train drivers for these tasks. The paper highlights the importance of understanding the task specific performance limits if further improvements in human reliability are sought. It also provides a practical example of how big data could play an increasingly important role in system error management, whether from the perspective of understanding normal performance and the limits of performance for specific tasks or as the basis for dynamic safety indicators which, if not leading, could at least become closer to real time.","Chris Harrison and Julian Stow and Xiaocheng Ge and Jonathan Gregory and Huw Gibson and Alice Monk","https:\/\/doi.org\/10.1016\/j.apergo.2022.103795","2022","At the limit? Using operational data to estimate train driver human reliability"],["Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation, optimisation and accurate forecasting. However, the theoretical framework and practical implementations of digital twin (DT) are yet to fully achieve this vision at scale. Although an increasing number of successful implementations exist in research and industrial works, sufficient implementation details are not publicly available, making it difficult to fully assess their components and effectiveness, to draw comparisons, identify successful solutions, share lessons, and thus to jointly advance and benefit from the DT methodology. This work first presents a review of relevant DT research and industrial works, focusing on the key DT features, current approaches in different domains, and successful DT implementations, to infer the key DT components and properties, and to identify current limitations and reasons behind the delay in the widespread implementation and adoption of digital twin. This work identifies that the major reasons for this delay are: the fact the DT is still a fast evolving concept; the lack of a universal DT reference framework, e.g. DT standards are scarce and still evolving; problem- and domain-dependence; security concerns over shared data; lack of DT performance metrics; and reliance of digital twin on other fast-evolving technologies. Advancements in machine learning, Internet of Things (IoT) and big data have led to significant improvements in DT features such as real-time monitoring and accurate forecasting. Despite this progress and individual company-based efforts, certain research and implementation gaps exist in the field, which have so far prevented the widespread adoption of the DT concept and technology; these gaps are also discussed in this work. Based on reviews of past work and the identified gaps, this work then defines a conceptualisation of DT which includes its components and properties; these also validate the uniqueness of DT as a concept, when compared to similar concepts such as simulation, autonomous systems and optimisation. Real-life case studies are used to showcase the application of the conceptualisation. This work discusses the state-of-the-art in DT, addresses relevant and timely DT questions, and identifies novel research questions, thus contributing to a better understanding of the DT paradigm and advancing the theory and practice of DT and its allied technologies.","Angira Sharma and Edward Kosasih and Jie Zhang and Alexandra Brintrup and Anisoara Calinescu","https:\/\/doi.org\/10.1016\/j.jii.2022.100383","2022","Digital Twins: State of the art theory and practice, challenges, and open research questions"],["While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.","Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.01.020","2019","Analytics-based decision-making for service systems: A qualitative study and agenda for future research"],["Multi-tenant database is a new database solution which is significant for Software as a service (SaaS) and Big Data applications in the context of cloud computing paradigm. This multi-tenant database has significant design challenges to develop a solution that ensures a high level of data quality, accessibility, and manageability for the tenants using this database. In this paper, we propose a multi-tenant data management service called Elastic Extension Tables Schema Handler Service (EETSHS), which is based on a multi-tenant database schema called Elastic Extension Tables (EET). This data management service satisfies tenants\u2019 different business requirements, by creating, managing, organizing, and administratinglarge volumes of structured, semi-structured, and unstructured data. Furthermore, it combines traditional relational data with virtual relational data in a single database schema and allows tenants to manage this data by calling functions from this service. We present algorithms for frequently used functions of this service, and perform several experiments to measure the feasibility and effectiveness of managing multi-tenant data using these functions. We report experimental results of query execution timesfor managing tenants\u2019 virtual and traditional relational data showing that EET schema is a good candidate for the management of multi-tenant data for SaaS and Big Data applications.","Haitham Yaish and Madhu Goyal and George Feuerlicht","https:\/\/doi.org\/10.1016\/j.procs.2014.05.202","2014","Multi-tenant Elastic Extension Tables Data Management"],["The emergence of new technologies such as Internet\/Web\/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.","Li-Minn Ang and Kah Phooi Seng","https:\/\/doi.org\/10.1016\/j.bdr.2015.12.003","2016","Big Sensor Data Applications in Urban Environments"],["Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.","Siqi Liu and Tianyu Wang and Shaowei Wang","https:\/\/doi.org\/10.1016\/j.dcan.2021.09.014","2021","Toward intelligent wireless communications: Deep learning - based physical layer technologies"],["Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.","Ziqian Xia and Yurong Liu","https:\/\/doi.org\/10.1016\/j.crbeha.2021.100055","2021","Aiding pro-environmental behavior measurement by Internet of Things"],["With the rapid development of sequencing technologies towards higher throughput and lower cost, sequence data are generated at an unprecedentedly explosive rate. To provide an efficient and easy-to-use platform for managing huge sequence data, here we present Genome Sequence Archive (GSA; http:\/\/bigd.big.ac.cn\/gsa or http:\/\/gsa.big.ac.cn), a data repository for archiving raw sequence data. In compliance with data standards and structures of the International Nucleotide Sequence Database Collaboration (INSDC), GSA adopts four data objects (BioProject, BioSample, Experiment, and Run) for data organization, accepts raw sequence reads produced by a variety of sequencing platforms, stores both sequence reads and metadata submitted from all over the world, and makes all these data publicly available to worldwide scientific communities. In the era of big data, GSA is not only an important complement to existing INSDC members by alleviating the increasing burdens of handling sequence data deluge, but also takes the significant responsibility for global big data archive and provides free unrestricted access to all publicly available data in support of research activities throughout the world.","Yanqing Wang and Fuhai Song and Junwei Zhu and Sisi Zhang and Yadong Yang and Tingting Chen and Bixia Tang and Lili Dong and Nan Ding and Qian Zhang and Zhouxian Bai and Xunong Dong and Huanxin Chen and Mingyuan Sun and Shuang Zhai and Yubin Sun and Lei Yu and Li Lan and Jingfa Xiao and Xiangdong Fang and Hongxing Lei and Zhang Zhang and Wenming Zhao","https:\/\/doi.org\/10.1016\/j.gpb.2017.01.001","2017","GSA: Genome Sequence Archive*"],["The popularity of the big data domain has boosted corporate interest in collecting and storing tremendous amounts of consumers\u2019 textual information. However, decision makers are often overwhelmed by the abundance of information, and the usage of text mining (TM) tools is still at its infancy. This study validates an extended technology acceptance model integrating information quality (IQ) and top management support. Results confirm that IQ influences behavioral intentions and TM tools usage, through perceptions of external control, perceived ease of use, and perceived usefulness; top management support also has a key role in determining the usage of TM tools.","Nathalie T.M. Demoulin and Kristof Coussement","https:\/\/doi.org\/10.1016\/j.im.2018.10.006","2020","Acceptance of text-mining systems: The signaling role of information quality"],["Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.","Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre","https:\/\/doi.org\/10.1016\/j.jpdc.2020.05.003","2020","Estimating record linkage costs in distributed environments"],["Achieving predictive, precise, participatory, preventive, and personalized health (abbreviated as p-Health) requires comprehensive evaluations of an individual's conditions captured by various measurement technologies. Since the 1950s, analysis of care providers' and physicians' notes and measurement data by computers to improve healthcare delivery has been termed clinical informatics. Since the 2010s, wide adoptions of Electronic Health Records (EHRs) have greatly improved clinical informatics development with fast growing pervasive wearable technologies that continuously capture the human physiological profile in-clinic (EHRs) and out-of-clinic (PHRs or Personal Health Records) to bolster mobile health (mHealth). In addition, after the Human Genome Project in the 1990s, medical genomics has emerged to capture the high-throughput molecular profile of a person. As a result, integrated data analytics is becoming one of the fast-growing areas under Biomedical Big Data to improve human healthcare outcomes. In this chapter, we first introduce the scope of data integration and review applications, data sources, and tools for clinical informatics and medical genomics. We then describe the data integration analytics at the raw data level, feature level, and decision level with case studies, and the opportunity for research and translation using advanced artificial intelligence (AI), such as deep learning. Lastly, we summarize the opportunities in biomedical big data integration that can reshape healthcare toward p-health.","Li Tong and Hang Wu and May D. Wang and Geoffrey Wang","https:\/\/doi.org\/10.1016\/bs.pmbts.2022.05.002","2022","Chapter One - Introduction of medical genomics and clinical informatics integration for p-Health care"],["Apache Spark is one of the most popular big data frameworks that abstract the underlying distributed computation details. However, even though Spark provides various abstractions, developers may still encounter challenges related to the peculiarity of distributed computation and environment. To understand the challenges that developers encounter, and provide insight for future studies, in this paper, we conduct an empirical study on the questions that developers encounter. We manually analyze 1,000 randomly selected questions that we collected from Stack Overflow. We find that: 1) questions related to data processing (e.g., transforming data format) are the most common among the 11 types of questions that we uncovered. 2) Even though data processing questions are the most common ones, they require the least amount of time to receive an answer. Questions related to configuration and performance require the most time to receive an answer. 3) Most of the issues are caused by developers\u2019 insufficient knowledge in API usages, data conversation across frameworks, and environment-related configurations. We also discuss the implication of our findings for researchers and practitioners. In summary, our work provides insights for future research directions and highlight the need for more software engineering research in this area.","Zehao Wang and Tse-Hsun (Peter) Chen and Haoxiang Zhang and Shaowei Wang","https:\/\/doi.org\/10.1016\/j.jss.2022.111488","2022","An empirical study on the challenges that developers encounter when developing Apache Spark applications"],["To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the \"clogging point\" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for \"meeting the people's increasing needs for a better life\".","Yang Wang and Hong Zhang and Libing Liu","https:\/\/doi.org\/10.1016\/j.iref.2022.01.004","2022","Does city construction improve life quality?-evidence from POI data of China"],["The key topic of digital research data raises a multitude of issues: big data, data sharing, data quality, data management, data curation, data citation, data literacy. This chapter addresses questions related to the definition of these concepts, to the frameworks constructed for a better understanding and treatment of the different phenomena, as well as ethical considerations. The potential of libraries and information professionals in fulfilling data-related activities is outlined, together with the associated requirements of them.","Tibor Koltay","https:\/\/doi.org\/10.1016\/B978-0-08-100251-3.00005-6","2016","Chapter 5 - Digital Research Data: Where are we Now?"],["In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.","Monika Solanki and Bojan Bo\u017ei\u0107 and Christian Dirschl and Rob Brennan","https:\/\/doi.org\/10.1016\/j.jss.2018.12.017","2019","Towards a knowledge driven framework for bridging the gap between software and data engineering"],["In the age of \u201cInternet+\u201d, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has \u201cfree rider\u201d tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.","Zehua Xiang and Minli Xu","https:\/\/doi.org\/10.1016\/j.jclepro.2019.01.310","2019","Dynamic cooperation strategies of the closed-loop supply chain involving the internet service platform"],["Each year, 1.35 million people are killed on the world\u2019s roads and another 20\u201350 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data \u2013 provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.","Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund","https:\/\/doi.org\/10.1016\/j.aap.2021.106543","2022","Understanding the potential of emerging digital technologies for improving road safety"],["The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.","Anja {du Plessis}","https:\/\/doi.org\/10.1016\/j.envc.2021.100100","2021","Necessity of making water smart for proactive informed decisive actions: A case study of the upper vaal catchment, South Africa"],["Our \u201cdigified\u201d lives have provided researchers with an unprecedented opportunity to study society at a much higher frequency and granularity. Such data can have a large sample size but can be sparse, biased, and exclusively contributed by the users of the technologies. We look at the increasing importance of missing data and under-representation and propose a new perspective that considers missing data as useful data to understand the underlying reasons for missingness and that provides a realistic view of the sample size of large but under-represented data.","Anahid Basiri and Chris Brunsdon","https:\/\/doi.org\/10.1016\/j.patter.2022.100587","2022","Missing data as data"],["Intelligent sensing, mechanism understanding, and the deterioration forecasting based on spatio\u2013temporal big data not only promote the safety of the infrastructure but also indicate the basic theory and key technology for the infrastructure construction to turn to intelligentization. The advancement of underground space utilization has led to the development of three characteristics (deep, big, and clustered) that help shape a tridimensional urban layout. However, compared to buildings and bridges overground, the diseases and degradation that occur underground are more insidious and difficult to identify. Numerous challenges during the construction and service periods remain. To address this gap, this paper summarizes the existing methods and evaluates their strong points and weak points based on real-world space safety management. The key scientific issues, as well as solutions, are discussed in a unified intelligent monitoring system.","Bowen Du and Junchen Ye and Hehua Zhu and Leilei Sun and Yanliang Du","https:\/\/doi.org\/10.1016\/j.eng.2022.07.016","2022","Intelligent Monitoring System Based on Spatio\u2013Temporal Data for Underground Space Infrastructure"],["The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.","Marta Zorrilla and Juan Yebenes","https:\/\/doi.org\/10.1016\/j.csi.2021.103595","2022","A reference framework for the implementation of data governance systems for industry 4.0"],["In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people\u2019s food habits, physical activity are extracted from social media using the API\u2019s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.","J. Ramsingh and V. Bhuvaneswari","https:\/\/doi.org\/10.1016\/j.asoc.2021.107423","2021","An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm"],["This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.","Jodie Moll and Ogan Yigitbasioglu","https:\/\/doi.org\/10.1016\/j.bar.2019.04.002","2019","The role of internet-related technologies in shaping the work of accountants: New directions for accounting research"],["Hyper-resolution datasets for urban flooding are rare. This problem prevents detailed flooding risk analysis, urban flooding control, and the validation of hyper-resolution numerical models. We employed social media and crowdsourcing data to address this issue. Natural Language Processing and Computer Vision techniques are applied to the data collected from Twitter and MyCoast (a crowdsourcing app). We found these big data based flood monitoring approaches can complement the existing means of flood data collection. The extracted information is validated against precipitation data and road closure reports to examine the data quality. The two data collection approaches are compared and the two data mining methods are discussed. A series of suggestions is given to improve the data collection strategy.","Ruo-Qian Wang and Huina Mao and Yuan Wang and Chris Rae and Wesley Shaw","https:\/\/doi.org\/10.1016\/j.cageo.2017.11.008","2018","Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data"],["The so-called Fourth Paradigm has witnessed a boom during the past two decades, with large volumes of observational data becoming available to scientists and engineers. Big data is characterized by the rule of the five Vs: Volume, Variety, Value, Velocity and Veracity. The concept of big data naturally matches well with the features of geoengineering and geoscience. Large-scale, comprehensive, multidirectional and multifield geotechnical data analysis is becoming a trend. On the other hand, Machine learning (ML), Deep Learning (DL) and Optimization Algorithm (OA) provide the ability to learn from data and deliver in-depth insight into geotechnical problems. Researchers use different ML, DL and OA models to solve various problems associated with geoengineering and geoscience. Consequently, there is a need to extend its research with big data research through integrating the use of ML, DL and OA techniques. This work focuses on a systematic review on the state-of-the-art application of ML, DL and OA algorithms in geoengineering and geoscience. Various ML, DL, and OA approaches are firstly concisely introduced, concerning mainly the supervised learning, unsupervised learning, deep learning and optimization algorithms. Then their representative applications in the geoengineering and geoscience are summarized via VOSviewer demonstration. The authors also provided their own thoughts learnt from these applications as well as work ongoing and future recommendations. This review paper aims to make a comprehensive summary and provide fundamental guidelines for researchers and engineers in the discipline of geoengineering and geoscience or similar research areas on how to integrate and apply ML, DL and OA methods.","Wengang Zhang and Xin Gu and Libin Tang and Yueping Yin and Dongsheng Liu and Yanmei Zhang","https:\/\/doi.org\/10.1016\/j.gr.2022.03.015","2022","Application of machine learning, deep learning and optimization algorithms in geoengineering and geoscience: Comprehensive review and future challenge"],["Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.","I\u00f1igo Martinez and Elisabeth Viles and Igor {G. Olaizola}","https:\/\/doi.org\/10.1016\/j.bdr.2020.100183","2021","Data Science Methodologies: Current Challenges and Future Approaches"],["Data mining and big data computing are the emerging domains in the current era of predictions for societal applications. Millions of people are interested in sharing their views through tweets. Healthcare predictions are one of the attractive researches in big data social mining. Healthcare predictions are derived by implementing topic models by the ailments data. An ailment refers to either illness or sign of a particular health problem. Millions of tweets are collected based on conditions and assessed with ailment topic aspect models. The existing topic model, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing, Probabilistic LSI (PLSI), limits the healthcare results assessment concerning any one of the ailments aspects. Recent ailments topic aspect model (ATAM) overcome the problems of these topic models and delivers the healthcare assessment results concerning the fundamental aspects of ailments data except side-effects analysis of treatments. The scalability performance of ATAM is degraded in showing healthcare results over the massive amounts of health data. A high-performance computing model of ATAM has been developed in the distributed environment to address scalability. Its intelligent model is designed in the cloud and multi-node Hadoop environment to deliver high-performance social computing results for healthcare. Experiments are conducted on many comparative studies is demonstrated between the existing and proposed high-performance models using the massive amount of health-related tweets concerning the ailments aspects.","K Narasimhulu and K.T. {Meena Abarna}","https:\/\/doi.org\/10.1016\/j.micpro.2022.104690","2022","HIGH PERFORMANCE SOCIAL DATA COMPUTING WITH DEVELOPMENT OF INTELLIGENT TOPIC MODELS FOR HEALTHCARE"],["Significant technological advances made in recent years have shepherded a dramatic increase in utilization of digital technologies for biomedicine\u2013 everything from the widespread use of electronic health records to improved medical imaging capabilities and the rising ubiquity of genomic sequencing contribute to a \u201cdigitization\u201d of biomedical research and clinical care. With this shift toward computerized tools comes a dramatic increase in the amount of available data, and current tools for data analysis capable of extracting meaningful knowledge from this wealth of information have yet to catch up. This article seeks to provide an overview of emerging mathematical methods with the potential to improve the abilities of clinicians and researchers to analyze biomedical data, but may be hindered from doing so by a lack of conceptual accessibility and awareness in the life sciences research community. In particular, we focus on topological data analysis (TDA), a set of methods grounded in the mathematical field of algebraic topology that seeks to describe and harness features related to the \u201cshape\u201d of data. We aim to make such techniques more approachable to non-mathematicians by providing a conceptual discussion of their theoretical foundations followed by a survey of their published applications to scientific research. Finally, we discuss the limitations of these methods and suggest potential avenues for future work integrating mathematical tools into clinical care and biomedical informatics.","Yara Skaf and Reinhard Laubenbacher","https:\/\/doi.org\/10.1016\/j.jbi.2022.104082","2022","Topological data analysis in biomedicine: A review"],["Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.","Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu","https:\/\/doi.org\/10.1016\/B978-0-12-816639-0.00007-7","2019","Chapter 7 - Smart city is a safe city: information and communication technology\u2013enhanced urban space monitoring and surveillance systems: the promise and limitations"],["Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.","Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine R\u00f8e Redalen and Alberto Traverso","https:\/\/doi.org\/10.1016\/j.radonc.2020.09.054","2020","From multisource data to clinical decision aids in radiation oncology: The need for a clinical data science community"],["The effects of data governance (as a means to maximize big data value creation in fire risk management) performance on fire risk was analyzed based on multi-source statistical data of 105 cities in China from 2016 to 2018. Specifically, data governance was first quantified with ten detailed indicators, which were then selected for explaining urban fire risk through correlation analysis. Next, the sample cities were clustered in terms of major socio-economic characteristics, and then the effects of data governance were examined by constructing multivariate regression models for each city cluster with ordinary least squares (OLS). The results showed that the constructed regression models produced good interpretation of fire risk in different types of cities, with coefficient of determination (R2) in each model exceeding 0.65. Among the indicators, the development of infrastructures (e.g. data collection devices and data analysis platforms), the level of data use, and the updating of fire risk related data were proved to produce significant effects on the reduction of fire frequency and fire consequence. Moreover, the organizational maturity of data governance was proved to be helpful in reducing fire frequency. For the cities with large population, the cross-department sharing of high-value data was found to be another important determinant of urban fire frequency. In comparison with existing statistical models which interpreted fire risk with general social factors (with the highest R2\u00a0=\u00a00.60), these new regression models presented a better statistical performance (with the average R2\u00a0=\u00a00.72). These findings are expected to provide decision support for the local governments of China and other jurisdictions to facilitate big data projects in improving fire risk management.","Zhao-Ge Liu and Xiang-Yang Li and Grunde Jomaas","https:\/\/doi.org\/10.1016\/j.ijdrr.2022.103138","2022","Effects of governmental data governance on urban fire risk: A city-wide analysis in China"],["The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.","Rutian Liu and Eric Simon and Bernd Amann and St\u00e9phane Gan\u00e7arski","https:\/\/doi.org\/10.1016\/j.is.2020.101495","2020","Discovering and merging related analytic datasets"],["Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.","Lei Cao and Jun Chen","https:\/\/doi.org\/10.1016\/j.measurement.2020.107850","2020","Online investigation of vibration serviceability limitations using smartphones"],["Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.","Sascha Eichst\u00e4dt and Anke Keidel and Julia Tesch","https:\/\/doi.org\/10.1016\/j.measen.2021.100232","2021","Metrology for the digital age"],["The rapid increase in the number of Earth Observation (EO) systems generates a massive amount of heterogeneous data. It has raised big issues in collecting, preprocessing, storing, and the visualization these data. However, traditional techniques are facing serious challenges when dealing with big EO data dimensions (i.e., Volume, Veracity, Variety, and Velocity), especially in natural hazards management. Therefore, big data techniques and tools attract more attention. In this paper we propose a multidimensional model framework for Big EO data warehousing. This framework includes 3 parts: (1) Data collection and preprocessing, being responsible for collecting data and improving their quality; (2) Data loading and storage, performing the ingestion task which consists of transferring the data from external resources to the Big data platform for storage; and (3) Visualization and interpretation, aiming to provide spatio-temporal analysis. This framework could be useful for decision-makers in monitoring the effects of drought disasters and, consequently, planning the mitigation and remediation measures. Experiments are carried out on drought monitoring in China along the period 2000\u20132020. The input data include remote sensing data, biophysical data, and climatological data. The results reveal that the proposed framework has a higher retrieval speed and a greater elasticity with different kinds (i.e. spatial, temporal, or spatiotemporal) of requests compared to traditional frameworks, indicating its superiority.","Hanen Balti and Ali Ben Abbes and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle","https:\/\/doi.org\/10.1016\/j.future.2022.05.010","2022","Multidimensional architecture using a massive and heterogeneous data: Application to drought monitoring"],["Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.","Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou","https:\/\/doi.org\/10.1016\/j.ijpe.2014.12.031","2015","How \u2018big data\u2019 can make big impact: Findings from a systematic review and a longitudinal case study"],["The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.","Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu","https:\/\/doi.org\/10.1016\/j.ins.2016.07.007","2016","Towards felicitous decision making: An overview on challenges and trends of Big Data"],[null,"Susanna K.P. Lau and Patrick C.Y. Woo","https:\/\/doi.org\/10.1016\/j.diagmicrobio.2018.12.006","2019","Pitfalls in big data analysis: next-generation technologies, last-generation data"],[null,"Patrick Mikalef and Ilias O. Pappas and John Krogstie and Paul A. Pavlou","https:\/\/doi.org\/10.1016\/j.im.2019.103237","2020","Big data and business analytics: A research agenda for realizing business value"],["The amount of data created by people, machines and corporations around the world is growing every year. Thanks to innovations such as the Internet of Things, this trend will continue, giving rise to the creation of Big Data. Data visualization leverages principles of visual psychology to help stakeholders identify patterns, trends and correlations that might go undetected in text-based or spreadsheet data. The return on investment (ROI) of big data visualization is well-documented in numerous studies and use cases. However, to achieve ROI from analytics investments, key insights must be uncovered, understood and communicated. Synthesizing huge quantities of data into key insights grows more challenging as data volumes and varieties increase. To address visualization challenges posed by big and high-dimensional data, this paper explores algorithms and techniques that compress the amount of data and\/or reduce the number of attributes to be analyzed and visualized. Specifically, this paper examines applying dimensionality reduction and data compression algorithms to reduce attributes, tuples and data points returned to the visualization. By reducing data returned to the visualization, trends, patterns and correlations are easier to view and visualization tool performance is optimized.","Amy Genender-Feltheimer","https:\/\/doi.org\/10.1016\/j.procs.2018.10.308","2018","Visualizing High Dimensional and Big Data"],["Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.","Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao","https:\/\/doi.org\/10.1016\/j.jii.2017.08.001","2018","Data and knowledge mining with big data towards smart production"],["Data management tools and analytics have provided managers with the opportunity to contemplate inventory performance as an ongoing activity by no longer examining only data agglomerated from ERP systems, but also, considering internet information derived from customers\u2019 online buying behaviour. The realisation of this complex relationship has increased interest in business intelligence through data and text mining of structured, semi-structured and unstructured data, commonly referred to as \u201cbig data\u201d to uncover underlying patterns which might explain customer behaviour and improve the response to demand volatility. This paper explores how sales structured data can be used in conjunction with non-structured customer data to improve inventory management either in terms of forecasting or treating some inventory as \u201ctop-selling\u201d based on specific customer tendency to acquire more information through the internet. A medical condition is considered - namely pain - by examining 129 weeks of sales data regarding analgesics and information seeking data by customers through Google, online newspapers and YouTube. In order to facilitate our study we consider a VARX model with non-structured data as exogenous to obtain the best estimation and we perform tests against several univariate models in terms of best fit performance and forecasting.","Christos I. Papanagnou and Omeiza Matthews-Amune","https:\/\/doi.org\/10.1016\/j.cor.2017.08.009","2018","Coping with demand volatility in retail pharmacies with the aid of big data exploration"],["The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.","Desamparados Blazquez and Josep Domenech","https:\/\/doi.org\/10.1016\/j.techfore.2017.07.027","2018","Big Data sources and methods for social and economic analyses"],["The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.","Sagit Valeev and Natalya Kondratyeva","https:\/\/doi.org\/10.1016\/B978-0-12-822066-5.00001-7","2021","Chapter 6 - Big data analytics and process safety"],["It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore\u2019s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.","C.L. {Philip Chen} and Chun-Yang Zhang","https:\/\/doi.org\/10.1016\/j.ins.2014.01.015","2014","Data-intensive applications, challenges, techniques and technologies: A survey on Big Data"],["Recent advances on prospective monitoring and retrospective analysis of health information at national or regional level are generating high expectations for the application of Big Data technologies that aim to analyze at real time high-volumes and\/or complex of data from healthcare delivery (e.g., electronic health records, laboratory and radiology information, electronic prescriptions, etc.) and citizens' lifestyles (e.g., personal health records, personal monitoring devices, social networks, etc.). Along these same lines, advances in the field of genomics are revolutionizing biomedical research, both in terms of data volume and prospects, as well as in terms of the social impact it entails. The potential of Big Data applications that consider all of the above levels of health information lies in the possibility of combining and integrating de-identified health information to allow secondary uses of data. This is\u00a0the use and re-use of various sources of health information for purposes in addition to the direct clinical care of specific patients or the direct investigation of specific biomedical research hypotheses. Current applications include: epidemiological and pharmacovigilance studies, facilitating recruitment to randomized controlled trials, carrying out audits and benchmarking studies, financial and service planning, and ultimately supporting the generation of novel biomedical research outcomes.","Isaac Cano and Akos Tenyi and Emili Vela and Felip Miralles and Josep Roca","https:\/\/doi.org\/10.1016\/j.coisb.2017.04.012","2017","Perspectives on Big Data applications of health information"],["Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.","Roger Bivand and Konstantin Krivoruchko","https:\/\/doi.org\/10.1016\/j.spl.2018.02.012","2018","Big data sampling and spatial analysis: \u201cwhich of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?\u201d"],["In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.","Luciana {Dalla Valle} and Ron Kenett","https:\/\/doi.org\/10.1016\/j.eswa.2017.12.044","2018","Social media big data integration: A new approach based on calibration"],["Two concepts currently at the leading edge of todays information technology revolution are Analytics and Big Data. The public transportation industry has been at the forefront in utilizing and implementing Analytics and Big Data, from ridership forecasting to transit operations Rail transit systems have been especially involved with these IT concepts, and tend to be especially amenable to the advantages of Analytics and Big Data because they are generally closed systems that involve sophisticated processing of large volumes of data. The more that public transportation professionals and decision makers understand the role of Analytics and Big Data in their industry in perspective, the more effectively they will be able to utilize its promise. This paper gives an overview of Big Data technologies in context of transportation with specific to Railways. This paper also gives an insight on how the existing data modules from the transport authority combines Big Data and how can be incorporated in providing maintenance decision making.","Adithya Thaduri and Diego Galar and Uday Kumar","https:\/\/doi.org\/10.1016\/j.procs.2015.07.323","2015","Railway Assets: A Potential Domain for Big Data Analytics"],["Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.","Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla","https:\/\/doi.org\/10.1016\/j.cie.2018.04.013","2019","Barriers to big data analytics in manufacturing supply chains: A case study from Bangladesh"],["Among the so-called \u201c4Vs\u201d (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of \u201cVolume\u201d in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization\/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization\/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the \u201cprocess\u201d level for coarse-grained distribution and the \u201cfunctionality\u201d and \u201celementary functions\u201d levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.","Mahfoud Bala and Omar Boussaid and Zaia Alimazighi","https:\/\/doi.org\/10.1016\/j.datak.2017.08.003","2017","A Fine\u2010Grained Distribution Approach for ETL Processes in Big Data Environments"],["The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.","Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santiba\u00f1ez Gonzalez}","https:\/\/doi.org\/10.1016\/j.techfore.2018.06.030","2019","Circular economy and big data analytics: A stakeholder perspective"],["Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information \u2013 big data \u2013 can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency\u2013university partnerships and research-to-practice.","Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan","https:\/\/doi.org\/10.1016\/j.childyouth.2015.09.014","2015","Using big data for evidence based governance in child welfare"],["The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but \u201cBig Data\u201d is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset\u2014but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.","Robert K. Perrons and Jesse W. Jensen","https:\/\/doi.org\/10.1016\/j.enpol.2015.02.020","2015","Data as an asset: What the oil and gas sector can learn from other industries about \u201cBig Data\u201d"],["From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.","Dong-Hee Shin and Min Jae Choi","https:\/\/doi.org\/10.1016\/j.tele.2014.09.006","2015","Ecological views of big data: Perspectives and issues"],["The growth of cities in the 21st century has put more pressure on resources and conditions of urban life. There are several reasons why the health-care industry is the focus of this investigation. For instance, in the UK various studies point to the lack of failure of basic quality control procedures and misalignment between customer needs and provider services and duplication of logistics practices. The development of smart cities and big data present unprecedented challenges and opportunities for operations managers; they need to develop new tools and techniques for network planning and control. Our paper aims to make a contribution to big data and city operations theory by exploring how big data can lead to improvements in transport capacity sharing. We explore using Markov models the integration of big data with future city (health-care) transport sharing. A mathematical model was designed to illustrate how sharing transport load (and capacity) in a smart city can improve efficiencies in meeting demand for city services. The results from our analysis of 13 different sharing\/demand scenarios are presented. A key finding is that the probability for system failure and performance variance tends to be highest in a scenario of high demand\/zero sharing.","Rashid Mehmood and Gary Graham","https:\/\/doi.org\/10.1016\/j.procs.2015.08.566","2015","Big Data Logistics: A health-care Transport Capacity Sharing Model"],[null,"Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang","https:\/\/doi.org\/10.1016\/j.scib.2019.09.011","2019","Opportunities and challenges of using big data for global health"],["Abstract:\nIn conditions of rapid external changes the requirement to quality of control of purposeful development of complex system (states, regions, corporations etc.) dramatically increases. Automation support of the key stages of decision making process is one of the ways to cope with the challenges. This paper focuses on the approach based on the Big Data Refining during cognitive modeling that proves the correctness of modeling and decision-making. The approach uses the requests to the Big Data for cognitive model components verification. The requests are created by intelligent agents with feedback from decision makers. Some practical results confirm the adequacy of the proposed approach.","Alexander N. Raikov and Z. Avdeeva and A. Ermakov","https:\/\/doi.org\/10.1016\/j.ifacol.2016.12.205","2016","Big Data Refining on the Base of Cognitive Modeling"],[null,"Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li","https:\/\/doi.org\/10.1016\/j.jval.2019.04.303","2019","PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS"],["While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (\u201cBDA\u201d).","Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu","https:\/\/doi.org\/10.1016\/j.techfore.2017.09.012","2018","A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics"],["Big data is an increasingly hot concept in the past five years in the area of computer science, e-commence, and bioinformatics, because more and more data has been collected by the internet, remote sensing network, wearable devices and the Internet of Things. The big data technology provides techniques and analytical tools to handle large datasets, so that creative ideas and new values can be extracted from them. However, the hydroinformatics research community are not so familiar with big data. This paper provides readers who are embracing the data-rich era with a timely review on big data and its relevant technology, and then points out the relevance with hydroinformatics in three aspects.","Yiheng Chen and Dawei Han","https:\/\/doi.org\/10.1016\/j.proeng.2016.07.443","2016","On Big Data and Hydroinformatics"],["Shipping is a heavily regulated industry and responsible for around 3% of global carbon emissions. Global trade is highly dependent on shipping which covers around 90% of commercial demand. Now the industry is expected to navigate through many twists and turns of different situations like upcoming regulations, climate change, energy shortages and technological revolutions. Technological development is apparent across all marine sectors due to the rapid development of sensor technology, IT, automation and robotics. The industry must continue to develop at a rapid pace over the next decade in order to be able to adapt to upcoming regulations and market pressure. Ship intelligence will be the driving force shaping the future of the industry. Ships generate a large volume of data from different sources and in different formats. So big data has become the talk of the industry nowadays. Big data analysis discovers correlations between different measurable or unmeasurable parameters to determine hidden patterns and trends. This analysis will have a significant impact on vessel performance monitoring and provide performance prediction, real-time transparency, and decision-making support to the ship operator. Big data will also bring new opportunities and challenges for the maritime industry. It will increase the capability of performance monitoring, remove human error and increase interdependencies of components. However, the industry will have to face many challenges such as data processing, reliability, and data security. Many regulations rely on ship data including the new EU MRV (Monitoring, Reporting and Verification) regulation to quantify the CO2 emissions for ships above 5000 gross tonnage. As a result, ship operators will have to monitor and report the verified amount of CO2 emitted by their vessels on voyages to, from and between EU ports and will also be required to provide information on energy efficiency parameters. The MRV is a data-oriented regulation requiring ship operators to capture and monitor the ship emissions and other related data and although it is a regional regulation at the moment there is scope for the International Maritime Organisation (IMO) to implement it globally in the near future.","Ibna Zaman and Kayvan Pazouki and Rose Norman and Shervin Younessi and Shirley Coleman","https:\/\/doi.org\/10.1016\/j.proeng.2017.08.182","2017","Challenges and Opportunities of Big Data Analytics for Upcoming Regulations and Future Transformation of the Shipping Industry"],["Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.","Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.05.013","2016","Big data reduction framework for value creation in sustainable enterprises"],[null,"Tao Hong and Pierre Pinson","https:\/\/doi.org\/10.1016\/j.ijforecast.2019.05.004","2019","Energy forecasting in the big data world"],["With the significant increase in the volume, variety, velocity and veracity of data generated, collected and transmitted through computing and networking systems, it is of little surprise that big data analysis and processing is the subject of focus from enterprise, academia and government. Outsourcing is one popular solution considered in big data processing, although security and privacy are two key concerns often attributed to the underutilization of outsourcing and other promising big data analysis and processing technologies. In this paper, we survey the state-of-the-art literature on cryptographic solutions designed to ensure the security and\/or privacy in big data outsourcing. For example, we provide concrete examples to explain how these cryptographic solutions can be deployed. We summarize the existing state-of-play before discussing research opportunities.","Zhe Liu and Kim-Kwang Raymond Choo and Minghao Zhao","https:\/\/doi.org\/10.1016\/j.cose.2016.12.006","2017","Practical-oriented protocols for privacy-preserving outsourced big data analysis: Challenges and future research directions"],["Background\nIn recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.\nMethods\nTo explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers\u2019 production trends in the field and the trend of each paper\u2019s co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.\nResults\nBy conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People\u2019s Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).\nConclusion\nThis study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.","Dongxiao Gu and Jingjing Li and Xingguo Li and Changyong Liang","https:\/\/doi.org\/10.1016\/j.ijmedinf.2016.11.006","2017","Visualizing the knowledge structure and evolution of big data research in healthcare informatics"],["Online user-generated content is playing a progressively important role as information source for social scientists seeking for digging out value. Advances procedures and technologies to enable the capture, storage, management, and analysis of the data make possible to exploit increasing amounts of data generated directly by users. In that regard, Big Data is gaining meaning into social science from quantitative datasets side, which differs from traditional social science where collecting data has always been hard, time consuming, and resource intensive. Hence, the emergent field of computational social science is broadening researchers' perspectives. However, it also requires a multidisciplinary approach involving several and different knowledge areas. This paper outlines an architectural framework and methodology to collect Big Data from an electronic Word-of-Mouth (eWOM) website containing user-generated content. Although the paper is written from the social science perspective, it must be also considered together with other complementary disciplines such as data accessing and computing.","M. Olmedilla and M.R. Mart\u00ednez-Torres and S.L. Toral","https:\/\/doi.org\/10.1016\/j.csi.2016.02.003","2016","Harvesting Big Data in social science: A methodological approach for collecting online user-generated content"],["In today\u2019s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN\u2019s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN\u2019s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework\u2019s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.","Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and J\u00f6rg Hartlief and Jens Rautenstengel and Christine Loeser and J\u00f6rg B\u00f6hme","https:\/\/doi.org\/10.1016\/j.procs.2022.01.218","2022","An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks"],["Background\nClinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.\nObjective\nTo determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.\nMaterials and methods\nMortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n\u00a0=\u00a045253), nurses (159027), and both (n\u00a0=\u00a0204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.\nResults\nModels built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians\u2019 and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.\nConclusion\nClinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.","Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel J\u00f6nsson and Ryan T. Demmer and Joanne Enticott","https:\/\/doi.org\/10.1016\/j.compbiomed.2021.104305","2021","Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data"],["Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals\u2019 psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.","Ruth E. Appel and Sandra C. Matz","https:\/\/doi.org\/10.1016\/B978-0-12-819200-9.00015-6","2021","Chapter 6 - Psychological targeting in the age of Big Data"],["Background\nBuilding cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.\nMethods\nWe used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.\nResults\nAmong 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC\u202f=\u202f0.799, Sensitivity\u202f=\u202f0.753, Specificity\u202f=\u202f0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.\nConclusions\nWe produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.","Aaron N. Richter and Taghi M. Khoshgoftaar","https:\/\/doi.org\/10.1016\/j.compbiomed.2019.04.039","2019","Efficient learning from big data for cancer risk modeling: A case study with melanoma"],["Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product\u2013services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five \u201cVs\u201d in Big Data\u2013Value, in addition to the other four \u201cVs\u201d\u2014Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value\u2014 \u201cinformation\u201d, beside the two existing ones: product and service. The results have strategic implications for managers.","David Opresnik and Marco Taisch","https:\/\/doi.org\/10.1016\/j.ijpe.2014.12.036","2015","The value of Big Data in servitization"],["Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics\u2014the systematic use of data combined with quantitative and qualitative analysis to make decisions\u2014can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.","A.F. Simpao and L.M. Ahumada and M.A. Rehman","https:\/\/doi.org\/10.1093\/bja\/aeu552","2015","Big data and visual analytics in anaesthesia and health care\u2020"],["This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.","Cheikh {Kacfah Emani} and Nadine Cullot and Christophe Nicolle","https:\/\/doi.org\/10.1016\/j.cosrev.2015.05.002","2015","Understandable Big Data: A survey"],["The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon\u2014dubbed as Big Data\u2014has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.","Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha","https:\/\/doi.org\/10.1016\/j.aei.2016.07.001","2016","Big Data in the construction industry: A review of present status, opportunities, and future trends"],["As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.","Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier","https:\/\/doi.org\/10.1016\/j.techfore.2017.06.029","2018","Big data analytics sentiment: US-China reaction to data collection by business and government"],[null,"Jane Englebright and Barbara Caspers","https:\/\/doi.org\/10.1016\/j.mnl.2016.01.001","2016","The Role of the Chief Nurse Executive in the Big Data Revolution"],["Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the \u201cumbrella\u201d of the social networks, social media and big data paradigms.","Gema Bello-Orgaz and Jason J. Jung and David Camacho","https:\/\/doi.org\/10.1016\/j.inffus.2015.08.005","2016","Social big data: Recent achievements and new challenges"],["Summary\nArtificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.","Natalia Norori and Qiyang Hu and Florence Marcelle Aellen and Francesca Dalia Faraci and Athina Tzovara","https:\/\/doi.org\/10.1016\/j.patter.2021.100347","2021","Addressing bias in big data and AI for health care: A call for open science"],["Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.","Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan","https:\/\/doi.org\/10.1016\/j.ijnss.2019.03.001","2019","The application of big data and the development of nursing science: A discussion paper"],["Big data and deep learning will profoundly change various areas of professions and research in the future. This will also happen in medicine and medical imaging in particular. As medical physicists, we should pursue beyond the concept of technical quality to extend our methodology and competence towards measuring and optimising the diagnostic value in terms of how it is connected to care outcome. Functional implementation of such methodology requires data processing utilities starting from data collection and management and culminating in the data analysis methods. Data quality control and validation are prerequisites for the deep learning application in order to provide reliable further analysis, classification, interpretation, probabilistic and predictive modelling from the vast heterogeneous big data. Challenges in practical data analytics relate to both horizontal and longitudinal analysis aspects. Quantitative aspects of data validation, quality control, physically meaningful measures, parameter connections and system modelling for the future artificial intelligence (AI) methods are positioned firmly in the field of Medical Physics profession. It is our interest to ensure that our professional education, continuous training and competence will follow this significant global development.","Mika Kortesniemi and Virginia Tsapaki and Annalisa Trianni and Paolo Russo and Ad Maas and Hans-Erik K\u00e4llman and Marco Brambilla and John Damilakis","https:\/\/doi.org\/10.1016\/j.ejmp.2018.11.005","2018","The European Federation of Organisations for Medical Physics (EFOMP) White Paper: Big data and deep learning in medical imaging and in relation to medical physics profession"],["Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm\u05f3s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.","Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang","https:\/\/doi.org\/10.1016\/j.ijpe.2014.12.034","2015","Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph"],["Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.","Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon","https:\/\/doi.org\/10.1016\/j.compeleceng.2017.12.009","2019","Deep learning in big data Analytics: A comparative study"],["This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: \u201cWhat do we mean by high-quality data?\u201d \u201cHow do we detect low-quality data?\u201d and \u201cWhat action will we take when data does not meet quality standards?\u201d This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.","Laura Sebastian-Coleman","https:\/\/doi.org\/10.1016\/B978-0-12-821737-5.00010-9","2022","Chapter 10 - Dimensions of Data Quality"],["Health care is thought to be one of the business fields with the largest big data potential. Based on the prevailing definition, big data has a large amount of data which can be processed easily and can be modified or updated easily. These data can be quickly stored, processed, and transformed into valuable information using older technologies. At present, many new trends regarding new data resources and innovative data analysis are followed in medicine and health care. In practice, electronic health-care records, free open-source data, and the \u201cquantified self\u201d provide new approaches for analyzing data. Some of these advancements have been made in information extraction from the text data based on analytics, which is useful in data unlocking for analytics purposes from clinical documentation. Choosing big data approaches in the medicine and health-care fields has been lagging. This has led to the rise specific problems regarding data complexity and organizational, legal, and ethical challenges. With the growth of the uptake of big data in general, and medicine and health care in specific, innovative ideas and solutions are expected. Telemedicine is a new opportunity for the Internet of Things (IoT). This enables the specialist to consult a patient despite them being in different places. Medical image segmentation is needed for the analysis, storage, and protection of medical images in telemedicine. Telemedicine is defined by the World Health Organization (WHO) as \u201cthe practice of medical care using interactive audiovisual and data communications. This includes the delivery of medical care services, diagnosis, consultation, treatment, as well as health education and the transfer of medical data.\u201d IoT-based applications mainly include remote patient monitoring and clinical monitoring. In addition, preventive measures-based applications are also part of smart health care. These applications require image processing-based technologies which could be integrated into medical health-care systems. Various types of input taken from cameras and processing of CT and MRI images could be integrated into IoT-based medical applications.","A. Vidhyalakshmi and C. Priya","https:\/\/doi.org\/10.1016\/B978-0-12-821326-1.00001-2","2020","Chapter 1 - Medical big data mining and processing in e-health care"],["In January 2017 the Consultative Committee of Convention 108 adopted its Guidelines on the Protection of Individuals with Regard to the Processing of Personal Data in a World of Big Data. These are the first guidelines on data protection provided by an international body which specifically address the issues surrounding big data applications. This article examines the main provisions of these Guidelines and highlights the approach adopted by the Consultative Committee, which contextualises the traditional principles of data protection in the big data scenario and also takes into account the challenges of the big data paradigm. The analysis of the different provisions adopted focuses primarily on the core of the Guidelines namely the risk assessment procedure. Moreover, the article discusses the novel solutions provided by the Guidelines with regard to the data subject's informed consent, the by-design approach, anonymization, and the role of the human factor in big data-supported decisions. This critical analysis of the Guidelines introduces a broader reflection on the divergent approaches of the Council of Europe and the European Union to regulating data processing. Where the principle-based model of the Council of Europe differs from the approach adopted by the EU legislator in the detailed Regulation (EU) 2016\/679. In the light of this, the provisions of the Guidelines and their attempt to address the major challenges of the new big data paradigm set the stage for concluding remarks about the most suitable regulatory model to deal with the different issues posed by the development of technology.","Alessandro Mantelero","https:\/\/doi.org\/10.1016\/j.clsr.2017.05.011","2017","Regulating big data. The guidelines of the Council of Europe in the context of the European data protection framework"],["This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.","Kannan Govindan and T.C.E. Cheng and Nishikant Mishra and Nagesh Shukla","https:\/\/doi.org\/10.1016\/j.tre.2018.03.011","2018","Big data analytics and application for logistics and supply chain management"],["Software project is collaborative enterprise of making a desired software for the client. Each software is unique and is delivered by following the process. The process includes understanding the requirement, planning, designing the software and implementation. Risk occurs in the software project which need attention by the managers and workers to make the project efficient. Big data analytics is commonly used in all fields. Big data deals with huge data which are unstructured. Using analytics tools, it can be chunked down and analyzed to provide valuable solutions. In this paper, a review of risk in software project and big data analytics are briefed out.","J.H. Rekha and R. Parvathi","https:\/\/doi.org\/10.1016\/j.procs.2015.04.045","2015","Survey on Software Project Risks and Big Data Analytics"],["The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability\u2013business strategy alignment on the BDAC\u2013FPER relationship.","Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe","https:\/\/doi.org\/10.1016\/j.ijpe.2016.08.018","2016","How to improve firm performance using big data analytics capability and business strategy alignment?"],["Large amounts of data are increasingly accumulated in the energy sector with the continuous application of sensors, wireless transmission, network communication, and cloud computing technologies. To fulfill the potential of energy big data and obtain insights to achieve smart energy management, we present a comprehensive study of big data driven smart energy management. We first discuss the sources and characteristics of energy big data. Also, a process model of big data driven smart energy management is proposed. Then taking smart grid as the research background, we provide a systematic review of big data analytics for smart energy management. It is discussed from four major aspects, namely power generation side management, microgrid and renewable energy management, asset management and collaborative operation, as well as demand side management (DSM). Afterwards, the industrial development of big data-driven smart energy management is analyzed and discussed. Finally, we point out the challenges of big data-driven smart energy management in IT infrastructure, data collection and governance, data integration and sharing, processing and analysis, security and privacy, and professionals.","Kaile Zhou and Chao Fu and Shanlin Yang","https:\/\/doi.org\/10.1016\/j.rser.2015.11.050","2016","Big data driven smart energy management: From big data to big insights"],["The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.\nResumen\nLa aparici\u00f3n de los sistemas de informaci\u00f3n cl\u00ednica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos cl\u00ednicos en formato electr\u00f3nico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas cl\u00ednicas, para su uso en la gesti\u00f3n de recursos o para sugerir l\u00edneas de investigaci\u00f3n que luego pueden ser explotadas mediante ensayos cl\u00ednicos aleatorizados. Sin embargo, los m\u00e9dicos cl\u00ednicos carecen de la formaci\u00f3n necesaria para la explotaci\u00f3n de grandes bases de datos, lo que supone un obst\u00e1culo para aprovechar esta oportunidad. Adem\u00e1s, existen cuestiones de \u00edndole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estad\u00edsticos, epidemi\u00f3logos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigaci\u00f3n cl\u00ednica o an\u00e1lisis de resultados (benchmarking). Se describe la reuni\u00f3n multidisciplinar (Critical Care Datathon) realizada en Madrid los d\u00edas 1, 2 y 3 de diciembre de 2017. Esta reuni\u00f3n, celebrada bajo los auspicios de la Sociedad Espa\u00f1ola de Medicina Intensiva, Cr\u00edtica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovaci\u00f3n y el Servicio de Medicina Intensiva del Hospital Cl\u00ednico San Carlos, as\u00ed como el grupo de investigaci\u00f3n \u00abLife Supporting Technologies\u00bb de la Universidad Polit\u00e9cnica de Madrid. Tras unas ponencias de formaci\u00f3n sobre big data, seguridad y calidad de los datos, y su aplicaci\u00f3n al entorno de la medicina intensiva, un grupo de cl\u00ednicos, analistas de datos, estad\u00edsticos, expertos en seguridad inform\u00e1tica de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas cl\u00ednicas establecidas previamente a la reuni\u00f3n. El trabajo colaborativo permiti\u00f3 establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias l\u00edneas de investigaci\u00f3n cl\u00ednica a desarrollar en el futuro. Adem\u00e1s, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en Espa\u00f1a, y se estableci\u00f3 un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparaci\u00f3n de resultados entre unidades o la elaboraci\u00f3n de nuevas l\u00edneas de investigaci\u00f3n cl\u00ednica.","A. {N\u00fa\u00f1ez Reiz}","https:\/\/doi.org\/10.1016\/j.medine.2018.06.006","2019","Big data and machine learning in critical care: Opportunities for collaborative research"],["In this paper, a big data analytic framework is introduced for processing high-frequency data stream. This framework architecture is developed by combining an advanced evolving learning algorithm namely Parsimonious Network Fuzzy Inference System (PANFIS) with MapReduce parallel computation, where PANFIS has the capability of processing data stream in large volume. Big datasets are learnt chunk by chunk by processors in MapReduce environment and the results are fused by rule merging method, that reduces the complexity of the rules. The performance measurement has been conducted, and the results are showing that the MapReduce framework along with PANFIS evolving system helps to reduce the processing time around 22 percent in average in comparison with the PANFIS algorithm without reducing performance in accuracy.","Choiru Za\u2019in and Mahardhika Pratama and Edwin Lughofer and Meftahul Ferdaus and Qing Cai and Mukesh Prasad","https:\/\/doi.org\/10.1016\/j.procs.2018.10.514","2018","Big Data Analytics based on PANFIS MapReduce"],["To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.","\u00c1lvaro Valencia-Parra and Luisa Parody and \u00c1ngel Jes\u00fas Varela-Vaca and Ismael Caballero and Mar\u00eda Teresa G\u00f3mez-L\u00f3pez","https:\/\/doi.org\/10.1016\/j.dss.2020.113450","2021","DMN4DQ: When data quality meets DMN"],["This paper considers the implications of so-called \u2018big data\u2019 for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly \u2018special\u2019 about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.","Dave Milne and David Watling","https:\/\/doi.org\/10.1016\/j.jtrangeo.2017.11.004","2019","Big data and understanding change in the context of planning transport systems"],[null,"Betty Rambur and Therese Fitzpatrick","https:\/\/doi.org\/10.1016\/j.profnurs.2017.10.005","2018","A plea to nurse educators: Incorporate big data use as a foundational skill for undergraduate and graduate nurses"],["Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.","Fadoua Khennou and Youness Idrissi Khamlichi and Nour El Houda Chaoui","https:\/\/doi.org\/10.1016\/j.procs.2018.01.098","2018","Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR"],["Tender price evaluation of construction project is one of the most important works for the clients to control project cost in the bidding stage. However,the previously underutilization of project cost data made the tender price evaluation of new projects lack of effective evaluation criterion, which brings challenge to cost control. With the improvement of companies\u2019 information technology application and the advent of big data era, the project cost-related data can be completely and systematically recorded in real time, as well as fully utilized to support decision-making for construction project cost management. In this paper, a system for tender price evaluation of construction project based on big data is presented, aiming to use related technique of big data to analysis project cost data to give a reasonable cost range, which contributes to obtaining the evaluation criterion to support the tender price controls. The paper introduced the data sources, data extraction, data storage and data analysis of the system respectively. A case study is conducted in a metro station project to evaluate the system. The results show that the system based on big data is significant for tender price evaluation in construction project.","Yongcheng Zhang and Hanbin Luo and Yi He","https:\/\/doi.org\/10.1016\/j.proeng.2015.10.114","2015","A System for Tender Price Evaluation of Construction Project Based on Big Data"],["The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault\/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.","Wei Ji and Lihui Wang","https:\/\/doi.org\/10.1016\/j.jmsy.2017.03.008","2017","Big data analytics based fault prediction for shop floor scheduling"],["In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet\u00a0all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.","Thomas Lef\u00e8vre","https:\/\/doi.org\/10.1016\/j.jflm.2017.08.001","2018","Big data in forensic science and medicine"],["There are many expectations and concerns about Big Data in the sector of Earth Observation. It is necessary to understand whether Big Data is a radical shift or an incremental change for the existing digital infrastructures. This manuscript explores the impact of Big Data dimensionalities (commonly known as \u2018V\u2019 axes: volume, variety, velocity, veracity, visualization) on the Global Earth Observation System of Systems (GEOSS) and particularly its common digital infrastructure (i.e. the GEOSS Common Infrastructure). GEOSS is a global and flexible network of content providers allowing decision makers to access an extraordinary range of data and information. GEOSS is a pioneering framework for global and multidisciplinary data sharing in the EO realm. The manuscript introduces and discusses the general GEOSS strategies to address Big Data challenges, focusing on the cloud-based discovery and access solutions. A final section reports the results of the scalability and flexibility performance tests.","Stefano Nativi and Paolo Mazzetti and Mattia Santoro and Fabrizio Papeschi and Max Craglia and Osamu Ochiai","https:\/\/doi.org\/10.1016\/j.envsoft.2015.01.017","2015","Big Data challenges in building the Global Earth Observation System of Systems"],[null,"C.J. Puranik and Sreenivasa Rao and S. Chennamaneni","https:\/\/doi.org\/10.1016\/j.jtos.2019.07.010","2019","The perils and pitfalls of big data analysis in medicine"],["Biomedical data sets are becoming increasingly larger and a plethora of high-dimensionality data sets (\u201cBig Data\u201d) are now freely accessible for neurodegenerative diseases, such as Alzheimer's disease. It is thus important that new informatic analysis platforms are developed that allow the organization and interrogation of Big Data resources into a rational and actionable mechanism for advanced therapeutic development. This will entail the generation of systems and tools that allow the cross-platform correlation between data sets of distinct types, for example, transcriptomic, proteomic, and metabolomic. Here, we provide a comprehensive overview of the latest strategies, including latent semantic analytics, topological data investigation, and deep learning techniques that will drive the future development of diagnostic and therapeutic applications for Alzheimer's disease. We contend that diverse informatic \u201cBig Data\u201d platforms should be synergistically designed with more advanced chemical\/drug and cellular\/tissue-based phenotypic analytical predictive models to assist in either de novo drug design or effective drug repurposing.","Stuart Maudsley and Viswanath Devanarayan and Bronwen Martin and Hugo Geerts","https:\/\/doi.org\/10.1016\/j.jalz.2018.01.014","2018","Intelligent and effective informatic deconvolution of \u201cBig Data\u201d and its future impact on the quantitative nature of neurodegenerative disease therapy"],["Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.","Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang","https:\/\/doi.org\/10.1016\/j.cjche.2016.05.039","2016","A review of control loop monitoring and diagnosis: Prospects of controller maintenance in big data era"],["Big Data, the new buzz word in the industry, is data that exceeds the processing and analytic capacity of conventional database systems within the time necessary to make them useful. With multiple data stores in abundant formats, billions of rows of data with hundreds of millions of data combinations and the urgent need of making best possible decisions, the challenge is big and the solution bigger, Big Data. Comes with it, new advances in computing technology together with its high performance analytics for simpler and faster processing of only relevant data to enable timely and accurate insights using data mining and predictive analytics, text mining, forecasting and optimization on complex data to continuously drive innovation and make the best possible decisions. While Big Data provides solutions to complex business problems like analyzing larger volumes of data than was previously possible to drive more precise answers, analyzing data in motion to capture opportunities that were previously lost, it poses bigger challenges in testing these scenarios. Testing such highly volatile data, which is more often than not unstructured generated from myriad sources such as web logs, radio frequency Id (RFID), sensors embedded in devices, GPS systems etc. and mostly clustered data for its accuracy, high availability, security requires specialization. One of the most challenging things for a tester is to keep pace with changing dynamics of the industry. While on most aspects of testing, the tester need not know the technical details behind the scene however this is where testing Big Data Technology is so different. A tester not only needs to be strong on testing fundamentals but also has to be equally aware of minute details in the architecture of the database designs to analyze several performance bottlenecks and other issues. Like in the example quoted above on In-Memory databases, a tester would need to know how the operating systems allocate and de-allocate memory and understand how much memory is being used at any given time. So, concluding, as the data- analytics Industry evolves further we would see the IT Testing Services getting closely aligned with the Database Engineering and the industry would need more skilled testing professional in this domain to grab the new opportunities.","Naveen Garg and Sanjay Singla and Surender Jangra","https:\/\/doi.org\/10.1016\/j.procs.2016.05.285","2016","Challenges and Techniques for Testing of Big Data"],["Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as \u201cbig data\u201d, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.","Guillaume Taglang and David B. Jackson","https:\/\/doi.org\/10.1016\/j.ygyno.2016.02.022","2016","Use of \u201cbig data\u201d in drug discovery and clinical trials"],["The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.","Fernando Gualo and Mois\u00e9s Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini","https:\/\/doi.org\/10.1016\/j.jss.2021.110938","2021","Data quality certification using ISO\/IEC 25012: Industrial experiences"],["Big data solutions represent a significant challenge for some organizations. There are a huge variety of software products, deployment patterns and solution options that need to be considered to ensure a successful outcome for an organization trying to implement a big data solution. With that in mind, the chapter \u201cBig Data: a practitioner's perspective\u201d will focus on four key areas associated with big data that require consideration from a practical and implementation perspective: (i) Big Data is a new Paradigm \u2013 Differences with Traditional Data Warehouse, Pitfalls and Considerations; (ii) Product considerations for Big Data \u2013 Use of Open Source products for Big Data, Pitfalls and Considerations; (iii) Use of Cloud for hosting Big Data \u2013 Why use Cloud, Pitfalls and Considerations; and (iv) Big Data Implementation \u2013 Architecture definition, processing framework and migration patterns from Data Warehouse to Big Data.","Darshan Lopes and Kevin Palmer and Fiona O'Sullivan","https:\/\/doi.org\/10.1016\/B978-0-12-805467-3.00010-7","2017","Chapter 10 - Big Data: A Practitioners Perspective"],["Large digital sky surveys are becoming the dominant source of data in astronomy. There are more than 100 terabytes of data in major archives, and that amount is growing rapidly. A typical sky survey archive has approximately 10 terabytes of image data and a billion detected sources (stars, galaxies, quasars, etc.), with hundreds of measured attributes per source. These surveys span the full range of wavelengths, radio through gamma ray, yet they are just a taste of the much larger datasets to come. Yearly advances in electronics bring new instruments that double the amount of data collected each year and lead to the exponential growth of information in astronomy. Thus, datasets that are orders of magnitude larger, more complex, and more homogeneous than in the past are on the horizon. In comparison, the size of the human genome is about 1 gigabyte and that of the Library of Congress is about 20 terabytes. Truly, astronomy has come to the big data era.","Jiale Lei and Linghe Kong","https:\/\/doi.org\/10.1016\/B978-0-12-819084-5.00010-9","2020","2 - Fundamentals of big data in radio astronomy"],["The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.","John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko","https:\/\/doi.org\/10.1016\/j.bcp.2018.03.014","2018","Integrity, standards, and QC-related issues with big data in pre-clinical drug discovery"],["We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.","Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood","https:\/\/doi.org\/10.1016\/j.procs.2019.09.284","2019","On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information Systems"],["Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.","Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard","https:\/\/doi.org\/10.1016\/j.accpm.2018.09.008","2019","Big data and targeted machine learning in action to assist medical decision in the ICU"],["With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.","Michael Alles and Glen L. Gray","https:\/\/doi.org\/10.1016\/j.accinf.2016.07.004","2016","Incorporating big data in audits: Identifying inhibitors and a research agenda to address those inhibitors"],["Big data is the collection and analysis of large set of data which holds many intelligence and raw information based on user data, Sensor data, Medical and Enterprise data. The Hadoop platform is used to Store, Manage, and Distribute Big data across several server nodes. This paper shows the Big data issues and focused more on security issue arises in Hadoop Architecture base layer called Hadoop Distributed File System (HDFS). The HDFS security is enhanced by using three approaches like Kerberos, Algorithm and Name node.","B. Saraladevi and N. Pazhaniraja and P. Victer Paul and M.S. Saleem Basha and P. Dhavachelvan","https:\/\/doi.org\/10.1016\/j.procs.2015.04.091","2015","Big Data and Hadoop-a Study in Security Perspective"],["Background\nBig data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge.\nPurpose\nThe purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals.\nMethods\nA literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice.\nDiscussion\nNurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods.\nConclusion\nThere are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science.","Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney","https:\/\/doi.org\/10.1016\/j.outlook.2016.11.021","2017","Big data science: A literature review of nursing research exemplars"],["The rapid growth of single-cell RNA-seq studies (scRNA-seq) demands efficient data storage, processing, and analysis. Big-data technology provides a framework that facilitates the comprehensive discovery of biological signals from inter-institutional scRNA-seq datasets. The strategies to solve the stochastic and heterogeneous single-cell transcriptome signal are discussed in this article. After extensively reviewing the available big-data applications of next-generation sequencing (NGS)-based studies, we propose a workflow that accounts for the unique characteristics of scRNA-seq data and primary objectives of single-cell studies.","Pingjian Yu and Wei Lin","https:\/\/doi.org\/10.1016\/j.gpb.2016.01.005","2016","Single-cell Transcriptome Study as Big Data"],["Safety data and information are the most valuable assets for organizations\u2019 safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.","Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang","https:\/\/doi.org\/10.1016\/j.ssci.2018.05.012","2018","Big-data-driven safety decision-making: A conceptual framework and its influencing factors"],["This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.","Marcos D. Assun\u00e7\u00e3o and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya","https:\/\/doi.org\/10.1016\/j.jpdc.2014.08.003","2015","Big Data computing and clouds: Trends and future directions"],["Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.","Yuan Guo and Yuan Guo and K. Wu","https:\/\/doi.org\/10.1016\/j.datak.2018.08.002","2018","Research on case retrieval of Bayesian network under big data"],["Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.","Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt","https:\/\/doi.org\/10.1016\/j.agsy.2017.01.023","2017","Big Data in Smart Farming \u2013 A review"],["Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.","Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao","https:\/\/doi.org\/10.1016\/j.arcontrol.2018.09.003","2018","Review and big data perspectives on robust data mining approaches for industrial process modeling with outliers and missing data"],["In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.","Shabnam Shadroo and Amir Masoud Rahmani","https:\/\/doi.org\/10.1016\/j.comnet.2018.04.001","2018","Systematic survey of big data and data mining in internet of things"],["The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.","Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.05.002","2016","The role of big data in smart city"],["The use of algorithms in modern data processing techniques, as well as data-intensive technological trends, suggests the adoption of a broader view of the data protection impact assessment. This will force data controllers to go beyond the traditional focus on data quality and security, and consider the impact of data processing on fundamental rights and collective social and ethical values. Building on studies of the collective dimension of data protection, this article sets out to embed this new perspective in an assessment model centred on human rights (Human Rights, Ethical and Social Impact Assessment-HRESIA). This self-assessment model intends to overcome the limitations of the existing assessment models, which are either too closely focused on data processing or have an extent and granularity that make them too complicated to evaluate the consequences of a given use of data. In terms of architecture, the HRESIA has two main elements: a self-assessment questionnaire and an ad hoc expert committee. As a blueprint, this contribution focuses mainly on the nature of the proposed model, its architecture and its challenges; a more detailed description of the model and the content of the questionnaire will be discussed in a future publication drawing on the ongoing research.","Alessandro Mantelero","https:\/\/doi.org\/10.1016\/j.clsr.2018.05.017","2018","AI and Big Data: A blueprint for a human rights, social and ethical impact assessment"],["Background\nA universally applicable approach that provides standard HALE measurements for different regions has yet to be developed because of the difficulties of health information collection. In this study, we developed a natural language processing (NLP) based HALE estimation approach by using individual-level electronic medical records (EMRs), which made it possible to calculate HALE timely in different temporal or spatial granularities.\nMethods\nWe performed diagnostic concept extraction and normalisation on 13\u202299 million EMRs with NLP to estimate the prevalence of 254 diseases in WHO Global Burden of Disease Study (GBD). Then, we calculated HALE in Chongqing, 2017, by using the life table technique and Sullivan's method, and analysed the contribution of diseases to the expected years \u201clost\u201d due to disability (DLE).\nFindings\nOur method identified a life expectancy at birth (LE0) of 77\u20229 years and health-adjusted life expectancy at birth (HALE0) of 71\u20227 years for the general Chongqing population of 2017. In particular, the male LE0 and HALE0 were 76\u20223 years and 68\u20229 years, respectively, while the female LE0 and HALE0 were 80\u20220 years and 74\u20224 years, respectively. Cerebrovascular diseases, cancers, and injuries were the top three deterioration factors, which reduced HALE by 2\u202267, 2\u202215, and 1\u202219 years, respectively.\nInterpretation\nThe results demonstrated the feasibility and effectiveness of EMRs-based HALE estimation. Moreover, the method allowed for a potentially transferable framework that facilitated a more convenient comparison of cross-sectional and longitudinal studies on HALE between regions. In summary, this study provided insightful solutions to the global ageing and health problems that the world is facing.\nFunding\nNational Key R and D Program of China (2018YFC2000400).","Xiaowen Ruan and Yue Li and Xiaohui Jin and Pan Deng and Jiaying Xu and Na Li and Xian Li and Yuqi Liu and Yiyi Hu and Jingwen Xie and Yingnan Wu and Dongyan Long and Wen He and Dongsheng Yuan and Yifei Guo and Heng Li and He Huang and Shan Yang and Mei Han and Bojin Zhuang and Jiang Qian and Zhenjie Cao and Xuying Zhang and Jing Xiao and Liang Xu","https:\/\/doi.org\/10.1016\/j.lanwpc.2021.100110","2021","Health-adjusted life expectancy (HALE) in Chongqing, China, 2017: An artificial intelligence and big data method estimating the burden of disease at city level"],[null,"Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline","https:\/\/doi.org\/10.1016\/j.jsmc.2016.01.009","2016","The Role of Big Data in the Management of Sleep-Disordered Breathing"],["The etiology and trajectory of addictions is complex, caused and moderated by individual differences in cognition that are themselves a function of genetics and of environment. In this chapter, we discuss how \u201cBig Data\u201d can shed light on the cognitive correlates of addiction. Big Data is primarily data-driven, using algorithms that search for patterns in data, with accurate prediction on previously unseen data as the metric of success. In this chapter, we introduce and provide practical advice on Big Data approaches for addiction. In the first part of this chapter, we describe how online methods of data collection facilitate the collection of large datasets. In the second section, we outline some recent advances in neuroimaging, with a focus on prediction of substance use using machine learning methods. In the final section, we present advances in genetics\u2014meta- and megaanalyses\u2014which may provide breakthroughs in our understanding of the genetics of addiction.","Robert Whelan and Zhipeng Cao and Laura O'Halloran and Brian Pennie","https:\/\/doi.org\/10.1016\/B978-0-12-815298-0.00027-7","2020","Chapter 27 - Genetics, imaging, and cognition: big data approaches to addiction research"],["The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public\u2013private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.","Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba","https:\/\/doi.org\/10.1016\/j.jclepro.2016.03.059","2017","The role of Big Data in explaining disaster resilience in supply chains for sustainability"],["Nowadays, data warehouse tools and technologies cannot handle the load and analytic process of data into meaningful information for top management. Big data technology should be implemented to extend the existing data warehouse solutions. Universities already collect vast amounts of data so the academic data of university has been growing significantly and become a big academic data. These datasets are rich and growing. University\u2019s top-level management needs tools to produce information from the records. The generated information is expected to support the decision-making process of top-level management. This paper explores how big data technology could be implemented with data warehouse to support decision making process. In this framework, we propose Hadoop as big data analytic tools to be implemented for data ingestion\/staging. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.","Leo Willyanto Santoso and  Yulia","https:\/\/doi.org\/10.1016\/j.procs.2017.12.134","2017","Data Warehouse with Big Data Technology for Higher Education"],["With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called \u2018big data\u2019. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.","Q. Peter He and Jin Wang","https:\/\/doi.org\/10.1016\/j.jprocont.2017.06.012","2018","Statistical process monitoring as a big data analytics tool for smart manufacturing"],[null,"Walter Verbrugghe and Kirsten Colpaert","https:\/\/doi.org\/10.1016\/j.jcrc.2019.09.005","2019","The electronic medical record: Big data, little information?"],["Summary\nFungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.","Lotus A. Lofgren and Jason E. Stajich","https:\/\/doi.org\/10.1016\/j.cub.2021.06.083","2021","Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes"],["High-speed trains are very fast (e.g. 350km\/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.","Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}","https:\/\/doi.org\/10.1016\/j.ifacol.2018.09.318","2018","A Platform for Fault Diagnosis of High-Speed Train based on Big Data\u204e\u204eProject supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802)."],["Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients\/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.","Manuel Gon\u00e7alves-Pinho and Alberto Freitas","https:\/\/doi.org\/10.1016\/B978-0-12-822884-5.00009-X","2021","Chapter 8 - The use of Big Data in Psychiatry\u2014The role of administrative databases"],["This paper aims to figure out the potential impact of Big Data (BD) on Critical Success Factors (CSFs) of Customer Relationship Management (CRM). In fact, while some authors have posited a relationship between BD and CRM, literature lacks works that go into the heart of the matter. Through an extensive up-to-date in-depth literature review about CRM, twenty (20) CSFs were singled out from 104 selected papers, and organized within an ad-hoc classification framework. The consistency of the classification was checked by means of a content analysis. Evidences were discussed and linked to the BD literature, and five propositions about how BD could affect CRM CSFs were formalized. Our results suggest that BD-enabled CRM initiatives could require several changes in the pertinent CSFs. In order to get rid of the hype effect surrounding BD, we suggest to adopt an explorative approach towards them by defining a mandatory business direction through sound business cases and pilot tests. From a general standpoint, BD could be framed as an enabling factor of well-known projects, like CRM initiatives, in order to reap the benefits from the new technologies by addressing the efforts through already acknowledged management paths.","Pierluigi Zerbino and Davide Aloini and Riccardo Dulmin and Valeria Mininno","https:\/\/doi.org\/10.1016\/j.ipm.2017.10.005","2018","Big Data-enabled Customer Relationship Management: A holistic approach"],["As experts at searching, retrieving, analyzing, and managing information, librarians are uniquely suited to work with big data. This chapter provides an overview of the popular big data technology. We examine what big data is, challenges and opportunities, and how it is currently being used in many industries and libraries. The chapter concludes with additional resources, some technologies for managing big data, big data terminology, and questions for further discussion.","Ida Arlene Joiner","https:\/\/doi.org\/10.1016\/B978-0-08-102253-5.00005-8","2018","Chapter 5 - Information Seeking With Big Data: Not Just the Facts"],["We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by \u201cdeep learning\u201d, along with a burst of \u201cbig data\u201d that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.","Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo","https:\/\/doi.org\/10.1016\/j.csbj.2021.10.009","2021","AI applications in functional genomics"],["The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System \u2013 the \u201cSICV Brasil\u201d launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.","Luri {Shirosaki Mar\u00e7al de Souza} and Andr\u00e9a Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}","https:\/\/doi.org\/10.1016\/j.spc.2020.09.021","2021","Evaluating and ranking secondary data sources to be used in the Brazilian LCA database \u2013 \u201cSICV Brasil\u201d"],["One of the applications of big data applications and infrastructure is in the pharmaceutical industry. The complexity of the queries that are executed in these applications and the results they generate, make us feel the statement of torture the data and it will confess to anything. The relationships between the data in the different subject areas, the clinical trials and results, the communities in social media, the research labs and their outcomes, the clinical labs and patient results, and the financial outcomes of the pharmaceutical enterprise. Wow, think of all kinds of insights, add to this the markets, the competition, and the global industry, and we have phenomenal data to work with.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-815746-6.00005-3","2020","5 - Pharmacy industry applications and usage"],["Companies of the manufacturing industry face increasing process complexity. To remain competitive, increasing the knowledge concerning innovative manufacturing processes is necessary. In other areas, data analytics methods have been successfully applied for this purpose. Currently, their application in large scale manufacturing is hampered by insufficient data availability. Therefore, this study presents a solution approach that enables adaptive data availability by establishing a data-use-case-matrix (DUCM), which allows use case prioritization to support dimensioning of control systems and IT infrastructures. In order to support technology development, further proposed is a scalable implementation of the prioritized use cases starting in early prototyping phases.","Achim Kampker and Heiner Heimes and Ulrich B\u00fchrer and Christoph Lienemann and Stefan Krotil","https:\/\/doi.org\/10.1016\/j.promfg.2018.06.017","2018","Enabling Data Analytics in Large Scale Manufacturing"],["Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles\u2019 turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers\u2019 workloads by providing information regarding where and when to investigate and solve traffic congestion problems.","Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen","https:\/\/doi.org\/10.1016\/j.trc.2019.10.001","2019","Network-wide identification of turn-level intersection congestion using only low-frequency probe vehicle data"],["The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China\u2019s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.","Jing Zhang and Rong Tan and Chunhua Su and Wen Si","https:\/\/doi.org\/10.1016\/j.jisa.2020.102659","2020","Design and application of a personal credit information sharing platform based on consortium blockchain"],["Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and\/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.","Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino","https:\/\/doi.org\/10.1016\/j.trpro.2021.12.052","2022","Transfer\u2019s monitoring in bus transit services by Automatic Vehicle Location data"],["Real-time analytics is a special kind of Big Data analytics in which data elements are required to be processed and analyzed as they arrive in real time. It is important in situations where real-time processing and analysis can deliver important insights and yield business value. This chapter provides an overview of current processing and analytics platforms needed to support such analysis, as well as analytics techniques that can be applied in such environments. The chapter looks beyond traditional event processing system technology to consider a broader big data context that involves \u201cdata at rest\u201d platforms and solutions. The chapter includes a case study showing the use of EventSwarm complex event processing engine for a class of analytics problems in finance. The chapter concludes with several research challenges, such as the need for new approaches and algorithms required to support real-time data filtering, data exploration, statistical data analysis, and machine learning.","Z. Milosevic and W. Chen and A. Berry and F.A. Rabhi","https:\/\/doi.org\/10.1016\/B978-0-12-805394-2.00002-7","2016","Chapter 2 - Real-Time Analytics"],["Abstract\nIn the past ten years, the application of artificial intelligence (AI) in biomedicine has increased rapidly, which roots in the rapid growth of biomedicine data, the improvement of computing performance, and the development of deep learning methods. At present, there are great difficulties in front of AI for solving complex and comprehensive medical problems. Ontology can play an important role in how to make machines have stronger intelligence and has wider applications in the medical field. By using ontologies, (meta) data can be standardized so that data quality is improved and more data analysis methods can be introduced, data integration can be supported by the semantics relationships which are specified in ontologies, and effective logic expression in nature language can be better understood by machine. This can be a pathway to stronger AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and Terminology was held in Beijing in autumn 2019, with the theme \u201cMaking Machine Understand Data\u201d. The success of this conference further improves the development of ontology in the field of biomedical information in China, and will promote the integration of Chinese ontology research and application with the international standards and the findability, accessibility, interoperability, and reusability(FAIR) Data Principle.","Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu","https:\/\/doi.org\/10.24920\/003701","2019","Ontology: Footstone for Strong Artificial Intelligence"],["Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.","Pengyuan Liu and Filip Biljecki","https:\/\/doi.org\/10.1016\/j.jag.2022.102936","2022","A review of spatially-explicit GeoAI applications in Urban Geography"],["The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow \u201cCloud storage way\u201d or \u201cdistributed learning\u201d approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.","Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni","https:\/\/doi.org\/10.1016\/j.ejim.2018.02.012","2018","A new standardized data collection system for interdisciplinary thyroid cancer management: Thyroid COBRA"],["The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.","Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li","https:\/\/doi.org\/10.1016\/j.isatra.2021.01.056","2021","Data consistency method of heterogeneous power IOT based on hybrid model"],["In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.","Katalin Patonai and Ferenc Jord\u00e1n","https:\/\/doi.org\/10.1016\/j.fooweb.2021.e00203","2021","Integrating trophic data from the literature: The Danube River food web"],["The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.","Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare","https:\/\/doi.org\/10.1016\/j.mfglet.2018.09.002","2018","Industrial Artificial Intelligence for industry 4.0-based manufacturing systems"],["Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number\/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and\/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.","F\u00e1tima Leal and Adriana E. Chis and Simon Caton and Horacio Gonz\u00e1lez\u2013V\u00e9lez and Juan M. Garc\u00eda\u2013G\u00f3mez and Marta Dur\u00e1 and Angel S\u00e1nchez\u2013Garc\u00eda and Carlos S\u00e1ez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier","https:\/\/doi.org\/10.1016\/j.bdr.2020.100172","2021","Smart Pharmaceutical Manufacturing: Ensuring End-to-End Traceability and Data Integrity in Medicine Production"],["Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as \u2039big data\u203a analyses and promises deep insights \u2013 in this case \u2013 to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.","Dominik C. Hezel and Markus Harak and Guy Libourel","https:\/\/doi.org\/10.1016\/j.chemer.2017.05.003","2018","What we know about elemental bulk chondrule and matrix compositions: Presenting the ChondriteDB Database"],["With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.","Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu","https:\/\/doi.org\/10.1016\/B978-0-12-823657-4.00002-6","2022","Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems"],["Abstract:\nDecision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) \u201cMaintenance Descriptive Analytics (monitoring)\u201d; 2) \u201cMaintenance Diagnostic Analytics\u201d; 3) \u201cMaintenance Predictive Analytics\u201d; and 4) \u201cMaintenance Prescriptive analytics\u201d.","Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar","https:\/\/doi.org\/10.1016\/j.ifacol.2016.11.037","2016","Maintenance Analytics \u2013 The New Know in Maintenance"],["This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently \u2013 what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.","Hanne Seter and Petter Arnesen and Odd Andr\u00e9 Hjelkrem","https:\/\/doi.org\/10.1016\/j.tranpol.2019.05.016","2019","The data driven transport research train is leaving the station. Consultants all aboard?"],["Initiated from services and consumer products industries, there is a growing interest in using Internet of Things (IoT) technologies in various industries. In particular, IoT-enabled cybermanufacturing starts to draw increasing attention. Because IoT devices such as IoT sensors are usually much cheaper and smaller than the traditional sensors, there is a potential for instrumenting manufacturing systems with massive number of sensors. The premise is that the big data subsequently collected from IoT sensors can be utilized to advance manufacturing. Therefore, data-driven statistical process monitoring (SPM) is expected to contribute significantly to the advancement of cybermanufacturing. In this work, the state-of-the-art in cybermanufacturing is reviewed; an IoT-enabled manufacturing technology testbed (MTT) was built to explore the potential of IoT sensors for manufacturing, as well as to understand the characteristics of data produced by the IoT sensors; finally, the potentials and challenges associated with big data analytics presented by cybermanufacturing systems is discussed; and we propose statistics pattern analysis (SPA) as a promising SPM tool for cybermanufacturing.","Q. Peter He and Jin Wang and Devarshi Shah and Nader Vahdat","https:\/\/doi.org\/10.1016\/j.ifacol.2017.08.2546","2017","Statistical Process Monitoring for IoT-Enabled Cybermanufacturing: Opportunities and Challenges"],["Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade\u2019s agricultural and human nutrition challenges.","Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles","https:\/\/doi.org\/10.1016\/j.copbio.2020.09.003","2021","The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems"],["Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.","In Lee and Yong Jae Shin","https:\/\/doi.org\/10.1016\/j.bushor.2019.10.005","2020","Machine learning for enterprises: Applications, algorithm selection, and challenges"],["As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.","Ren\u00e9 Bohnsack and Meike Malena Liesner","https:\/\/doi.org\/10.1016\/j.bushor.2019.09.001","2019","What the hack? A growth hacking taxonomy and practical applications for firms"],["In the big data era, small data problems still exist in many industrial sectors. Taking the high-value process industries as an example, a large number of materials and processing methods are often tested at the design stage. However, only a small amount of data can be collected for each material-process combination, which poses a serious challenge to data-driven process modeling. There is a great necessity to integrate the small data measured in different tasks and build the process model by sharing the information. In this work, a deep embedding neural network is proposed to extract the qualitative task information for process modeling. Specifically, an autoencoder is used to learn embeddings which are combined with the quantitative process conditions as the inputs of a feed-forward neural network to produce the final predictions. The feasibility, including interpretability and prediction accuracy, of the developed method is illustrated with an extrusion process.","Haibin Wu and Yu-Han Lo and Le Zhou and Yuan Yao","https:\/\/doi.org\/10.1016\/j.jprocont.2022.04.018","2022","Process modeling by integrating quantitative and qualitative information using a deep embedding network and its application to an extrusion process"],["Standards are - not directly visible to everyone \u2013 omnipresent in nearly every development process. In times of digitalization, where buzzwords such as \"connectivity of machines\", \"artificial intelligence\", \u201cbig data\u201d, \u201ccloud computing\u201d or \u201csmart factories\u201d are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today\u2019s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the \u201c3M Framework of Duisburg\u201d and thus answers questions of modularization, modeling and management, consists of six steps \"extraction\", \"modeling\", \u201cmodification\u201d, \"fusion and storage\", \"provision\" and \"application\", to digitalize existing content, is presented and discussed.","Dominik Ehring and Janosch Luttmer and Robin Pluhnau and Arun Nagarajah","https:\/\/doi.org\/10.1016\/j.procir.2021.05.025","2021","SMART standards - concept for the automated transfer of standard contents into a machine-actionable form"],["Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.","Rene Abraham and Johannes Schneider and Jan {vom Brocke}","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.07.008","2019","Data governance: A conceptual framework, structured review, and research agenda"],["As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in \u201cquick and dirty\u201d methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.","Leah M. Hamilton and Jacob Lahne","https:\/\/doi.org\/10.1016\/j.foodqual.2020.103926","2020","Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development"],["Based on mobile internet user data, we construct an \u201cInternet population\u201d measure and reexamine spatial population distribution in China. The location based service (LBS) data of mobile internet uses is able to capture the accurate location of users' residence and solve the underestimation problem of missing migrants. We have three main findings. First, contrary to previous studies based on traditional population statistics, city size distribution of Internet population fits well into Zipf's law with a R2 of 90.7%. Second, the Internet population indicator is superior to traditional population statistics in explaining inelastic household consumption such as water consumption, electricity consumption, and garbage disposal. It suggests that the \u201cInternet population\u201d is a better proxy of actual city population. Third, the traditional population statistics systematically overestimate population in small cities and underestimate population in large cities. It indicates that the public resource distortions will continue to exist or even worsen off in China if the allocation process relies greatly on traditional population statistics. Although no measures are perfect, our new population measure provides important incremental information for future discussion.","Huixuan Li and Jing Chen and Zihao Chen and Jianguo Xu","https:\/\/doi.org\/10.1016\/j.chieco.2022.101808","2022","Urban population distribution in China: Evidence from internet population"],["The crowd's power, combined with the sensing capabilities of smart mobile de-vices, has resulted in the emergence of a revolutionary data acquisition paradigm known as Mobile Crowdsensing. In exchange for rewards, mobile users collect and share location-specific data values. However, most existing crowdsensing systems built on traditional centralized architectures are highly prone to attacks, intrusions, single point of failure, manipulations, and low reliability. Recently, decentralized blockchain technologies are being applied in mobile crowdsensing systems to ensure workers' privacy, data privacy, and the quality of sensed data at a low service fee. By leveraging blockchain technology, this paper inherits the advantages of the public blockchain without the need for any trusted third-parties. We propose a smart contract-based privacy-preserving data aggregation and quality assessment protocol to infer reliable aggregated results and estimate data quality, wherein, we design a fair quality-driven incentive mechanism to distribute rewards based on the data quality. The protocol ensures a secure, cost-optimal, and reliable aggregation and estimation of the sensed data quality on the public blockchain without disclosing the sensed data's and participants' privacy. We adopt Interplanetary File Systems to offset the blockchain's expensive storage costs. Experiments were conducted using real-world datasets which were implemented on a full-stack on-chain and off-chain decentralized application on the Ethereum blockchain. The experimental results demonstrate our design is highly efficient in achieving privacy-preserving data aggregation and significantly reduces on-chain computation costs.","Ruiyun Yu and Ann Move Oguti and Dennis Reagan Ochora and Shuchen Li","https:\/\/doi.org\/10.1016\/j.jnca.2022.103483","2022","Towards a privacy-preserving smart contract-based data aggregation and quality-driven incentive mechanism for mobile crowdsensing"],["Building operations account for the largest proportion of energy use throughout the building life cycle. The energy saving potential is considerable taking into account the existence of a wide variety of building operation deficiencies. The advancement in information technologies has made modern buildings to be not only energy-intensive, but also information-intensive. Massive amounts of building operational data, which are in essence the reflection of actual building operating conditions, are available for knowledge discovery. It is very promising to extract potentially useful insights from big building operational data, based on which actionable measures for energy efficiency enhancement are devised. Data mining is an advanced technology for analyzing big data. It consists of two main types of data analytics, i.e., supervised and unsupervised analytics. Despite of the power of supervised analytics in predictive modeling, unsupervised analytics are more practical and promising in discovering novel knowledge given limited prior knowledge. This paper provides a comprehensive review on the current utilization of unsupervised data analytics in mining massive building operational data. The commonly used unsupervised analytics are summarized according to their knowledge representations and applications. The challenges and opportunities are elaborated as guidance for future research in this multi-disciplinary field.","Cheng Fan and Fu Xiao and Zhengdao Li and Jiayuan Wang","https:\/\/doi.org\/10.1016\/j.enbuild.2017.11.008","2018","Unsupervised data analytics in mining big building operational data for energy efficiency enhancement: A review"],["With the development of artificial intelligence technology, the widespread application of robot process automation (RPA) in the future financial field has become an inevitable trend. Through the review of the current situation of cost management of A Group\u2019s financial shared service center, the article deeply expounds the problems that the current cross-system data cannot be automatically collected, the cost accounting is not timely, and the cost analysis report mode is too fixed. Based on Robot Process Automation (RPA), cost management process optimization and improvement were made on the cross-system data acquisition, \"Cloud Purchasing Platform\" construction, and comprehensive multi-dimensional cost analysis. It is expected to provide reference for the robot process automation application of the financial shared service center.","Yu Lian Qiu and Guo Fang Xiao","https:\/\/doi.org\/10.1016\/j.procs.2020.02.031","2020","Research on Cost Management Optimization of Financial Sharing Center Based on RPA"],["Context\nThe need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.\nObjective\nApplying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.\nMethod\nThe paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.\nResults\nTo evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.\nConclusion\nIt is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.","Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston","https:\/\/doi.org\/10.1016\/j.infsof.2019.05.009","2019","M-Lean: An end-to-end development framework for predictive models in B2B scenarios"],["The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient\/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.","Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras","https:\/\/doi.org\/10.1016\/j.maturitas.2015.03.015","2015","Connected health and integrated care: Toward new models for chronic disease management"],["During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.","Kim B. Stevens and Dirk U. Pfeiffer","https:\/\/doi.org\/10.1016\/j.sste.2015.04.003","2015","Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems"],["The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of \u2018common knowledge\u2019 fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.","Warwick Powell and Marcus Foth and Shoufeng Cao and Val\u00e9ri Natanelov","https:\/\/doi.org\/10.1016\/j.jii.2021.100261","2022","Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains"],["Energy crisis and climate change have become two seriously concerned issues universally. As a feasible solution, Global Energy Interconnection (GEI) has been highly praised and positively responded by the international community once proposed by China. From strategic conception to implementation, GEI development has entered a new phase of joint action now. Gathering and building a global grid database is a prerequisite for conducting research on GEI. Based on the requirement of global grid data management and application, combining with big data and geographic information technology, this paper studies the global grid data acquisition and analysis process, sorts out and designs the global grid database structure supporting GEI research, and builds a global grid database system.","Xuming Liang","https:\/\/doi.org\/10.14171\/j.2096-5117.gei.2018.01.011","2018","Application and research of global grid database design based on geographic information"],["An important challenge in big data is identification of important variables. For this purpose, methods of discovering variables with non-standard univariate marginal distributions are proposed. The conventional moments based summary statistics can be well-adopted, but their sensitivity to outliers can lead to selection based on a few outliers rather than distributional shape such as bimodality. To address this type of non-robustness, the L-moments are considered. Using these in practice, however, has a limitation since they do not take zero values at the Gaussian distributions to which the shape of a marginal distribution is most naturally compared. As a remedy, Gaussian Centered L-moments are proposed, which share advantages of the L-moments but have zeros at the Gaussian distributions. The strength of Gaussian Centered L-moments over other conventional moments is shown in theoretical and practical aspects such as their performances in screening important genes in cancer genetics data.","Hyowon An and Kai Zhang and Hannu Oja and J.S. Marron","https:\/\/doi.org\/10.1016\/j.csda.2022.107632","2022","Variable screening based on Gaussian Centered L-moments"],["Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with \u201cbig data\u201d from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.","Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick","https:\/\/doi.org\/10.1016\/j.jacc.2018.12.054","2019","Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review"],["Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.","Hassiba Laifa and Raoudha khcherif and Henda Hajjami {Ben Ghezalaa}","https:\/\/doi.org\/10.1016\/j.procs.2021.08.101","2021","Train delay prediction in Tunisian railway through LightGBM model"],["Technological development and innovations has been the focus of research in the field of smart maintenance, whereas there is less research regarding how maintenance organisations adapt the development. This case study focuses to understand what constraints maintenance organisations in the transition into applying more data-driven decisions in maintenance. This paper aims to emphasize the organisational challenges in data-driven maintenance, such as trustworthiness of data-driven decisions, data quality, management and competences. Through a case study at a global company in the automotive industry these challenges are highlighted and discussed through a questionnaire survey participated by 72 people and interviews with 7 people from the maintenance organisation.","P Savolainen and J Magnusson and M. Gopalakrishnan and E. {Turanoglu Bekar} and A. Skoogh","https:\/\/doi.org\/10.1016\/j.ifacol.2020.11.015","2020","Organisational Constraints in Data-driven Maintenance: a case study in the automotive industry"],["Objectives: The objective of the presented work is to present novel methods for big data exploration in the Air Traffic Control (ATC) domain. Data is formed by sets of airplane trajectories, or trails, which in turn records the positions of an aircraft in a given airspace at several time instants, and additional information such as flight height, speed, fuel consumption, and metadata (e.g. flight ID). Analyzing and understanding this time-dependent data poses several non-trivial challenges to information visualization. Materials and methods: To address this Big Data challenge, we present a set of novel methods to analyze aircraft trajectories with interactive image-based information visualization techniques.As a result, we address the scalability challenges in terms of data manipulation and open questions by presenting a set of related visual analysis methods that focus on decision-support in the ATC domain. All methods use image-based techniques, in order to outline the advantages of such techniques in our application context, and illustrated by means of use-cases from the ATC domain. Results: For each considered use-case, we outline the type of questions posed by domain experts, data involved in addressing these questions, and describe the specific image-based techniques we used to address these questions. Further, for each of the proposed techniques, we describe the visual representation and interaction mechanisms that have been used to address the above-mentioned goals. We illustrate these use-cases with real-life datasets from the ATC domain, and show how our techniques can help end-users in the ATC domain discover new insights, and solve problems, involving the presented datasets.","C. Hurter and S. Conversy and D. Gianazza and A.C. Telea","https:\/\/doi.org\/10.1016\/j.trc.2014.03.005","2014","Interactive image-based information visualization for aircraft trajectory analysis"],["The pace of technological evolution in healthcare is advancing. In this article key technology trends are identified that are likely to influence nursing practice and education over the next decade. The complexity of curricular revision can create challenges in the face of rapid practice change. Nurse educators are encouraged to consider the role of electronic health records (EHRs), wearable technologies, big data and data analytics, and increased patient engagement as key areas for curriculum development. Student nurses, and those already in practice, should be offered ongoing educational opportunities to enhance a wide spectrum of professional informatics skills. The nurses of 2025 will most certainly inhabit a very different practice environment than what exists today and technology will be key in this transformation. Nurse educators must prepare now to lead these practitioners into the future.","Tracie Risling","https:\/\/doi.org\/10.1016\/j.nepr.2016.12.007","2017","Educating the nurses of 2025: Technology trends of the next decade"],["This chapter motivates the importance of RDF data management through the Big Data and Web of Data\/Semantic Web phenomena. It also provides some insights of existing RDF stores and presents the dimensions used in this book to compare these systems.",null,"https:\/\/doi.org\/10.1016\/B978-0-12-799957-9.00001-8","2015","Chapter One - Introduction"],["Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.","Dimitrios Kourtesis and Jose Mar\u00eda Alvarez-Rodr\u00edguez and Iraklis Paraskakis","https:\/\/doi.org\/10.1016\/j.future.2013.10.015","2014","Semantic-based QoS management in cloud systems: Current status and future challenges"],["Product realization processes are undergoing radical change considering the increasing digitalization of manufacturing fostered by cyber-physical production systems, the internet of things, big data, cloud computing, and the advancing use of digital twins. These trends are subsumed under the term \u201cindustry 4.0\u201d describing the vision of a digitally connected manufacturing environment. The contribution gives an overview of future challenges and potentials for next generation geometry assurance and geometrical variations management in the context of industry 4.0. Particularly, the focus is set on potentials and risks of increasingly available manufacturing data and the use of digital twins in geometrical variations management.","Benjamin Schleich and Kristina W\u00e4rmefjord and Rikard S\u00f6derberg and Sandro Wartzack","https:\/\/doi.org\/10.1016\/j.procir.2018.04.078","2018","Geometrical Variations Management 4.0: towards next Generation Geometry Assurance"],["Pipelines carrying energy products play vital roles in economic wealth and public safety, but incidents continue occurring. Condition assessment of pipelines is essential to identify anomalies timely. Advanced sensing technologies obtain informative data for condition assessment, while data analysis by human has limited efficiency, accuracy, and reliability. Advances in machine learning offer exciting opportunities for automated condition assessment with minimum human intervention. This paper reviews machine learning approaches to detect, classify, locate, and quantify pipeline anomalies based on intelligent interpretation of routine operation data, nondestructive testing data, and computer vision data. Statistics and uncertainties of performance metrics of machine learning approaches are discussed. An analysis on strengths, weaknesses, opportunities, and threats (SWOT) is performed. Guides for practitioners to perform automated pipeline condition assessment are recommended. This review provide insights into the machine learning approaches for automated pipeline condition assessment. The SWOT analysis will support decision making in the pipeline industry.","Yiming Liu and Yi Bao","https:\/\/doi.org\/10.1016\/j.aei.2022.101687","2022","Review on automated condition assessment of pipelines with machine learning"],["A centralized clinical data repository is essential for inspecting patients\u2019 medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.","Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed","https:\/\/doi.org\/10.1016\/j.smhl.2021.100238","2022","A privacy-preserving National Clinical Data Warehouse: Architecture and analysis"],["With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.","Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol","https:\/\/doi.org\/10.1016\/j.compag.2022.106813","2022","Data analytics platforms for agricultural systems: A systematic literature review"],["The development of theories and techniques for big data analytics offers tremendous possibility for investigating large-scale events and patterns that emerge over space and time. In this research, we utilize a unique open dataset \u201cThe Global Data on Events, Location and Tone\u201d (GDELT) to model the image of China in mass media, specifically, how China has related to the rest of the world and how this connection has evolved upon time. The results of this research contribute to both the methodological and the empirical perspectives: We examined the effectiveness of the dynamic time warping (DTW) distances in measuring the differences between long-term mass media data. We identified four types of connection strength patterns between China and its top 15 related countries. With that, the distance decay effect in mass media is also examined and compared with social media and public transportation data. While using multiple datasets and focusing on mass media, this study generates valuable input regarding the interpretation of the diplomatic and regional correlation for the nation of China. It also provides methodological references for investigating international relations in other countries and regions in the big data era.","Yihong Yuan and Yu Liu and Guixing Wei","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2016.10.012","2017","Exploring inter-country connection in mass media: A case study of China"],["Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data\/machine learning\/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.","Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai","https:\/\/doi.org\/10.1016\/j.enbuild.2020.109941","2020","A bi-directional missing data imputation scheme based on LSTM and transfer learning for building energy data"],["Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and \u03b413C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.","Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou","https:\/\/doi.org\/10.1016\/j.scib.2019.07.004","2019","Internet of Things to network smart devices for ecosystem monitoring"],["In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.","Guangjie Chen and Zhiqiang Ge","https:\/\/doi.org\/10.1016\/j.conengprac.2020.104344","2020","Robust Bayesian networks for low-quality data modeling and process monitoring applications"],["Cloud computing is one of the significant facilitators of the health information revolution in the healthcare business. The global exchange of records in the health sector through electronic media is facilitated by cloud computing. In healthcare, this technology increases safety and creates innovation. Communication with the health matrix throughout the world makes feasible by the application of this technology. Cloud computing has been utilised in health care for many years and has evolved in conjunction with developments in business. This technology establishes standard accessible hardware for diverse healthcare applications via a network connection. Cloud computing and processing ensure safe communication, and the cloud servers secure all essential data. Doctors can counsel their individuals on their health and broadcast their patient's daily health regimes, typically keeping their minds and bodies healthy. Psychologists and psychiatrists can use videoconferencing that makes patients comfortable with their patients. This paper discusses cloud computing and its need for healthcare. Major key advantages, barriers, and challenges of Cloud computing for the healthcare industry are identified. Finally, it discusses the significant applications of cloud computing for healthcare. Today more and more healthcare suppliers are providing Internet of Things (IoT) enabled gadgets to patients, and patient data are instantly communicated to their doctors by linking such devices to the cloud system of hospitals. As a result, cloud computing, in conjunction with fast-expanding technologies such as Big Data analytics, artificial intelligence, and the internet of medical things, improves efficiencies and expands the number of ways to streamline healthcare delivery. It improves resource availability, improves interoperability, and reduces costs.","Mohd Javaid and Abid Haleem and Ravi Pratap Singh and Shanay Rab and Rajiv Suman and Ibrahim Haleem Khan","https:\/\/doi.org\/10.1016\/j.ijcce.2022.06.001","2022","Evolutionary trends in progressive cloud computing based healthcare: Ideas, enablers, and barriers"],["Planning and scheduling for wind farms play a critical role in the costs of maintenance. The use and analysis of field data or so-called Product Use Information (PUI) to improve maintenance activities and to reduce the costs has gained attention in the recent years. The product use data consist of sources such as measure of sensors on the turbines, the alarms information or signals from the condition monitoring, Supervisory Control and Data Acquisition (SCADA) systems, which are currently used in maintenance activities. However, those data have the potential to offer alternative solutions to improve processes and provide better decisions, by transforming them into actionable knowledge. In order to make the right decision it is important to understand, which PUI data source and which data analysis methods, are suitable for what kind of decision making task. The aim of this study is to discover, how analysis of PUI can help in the maintenance processes of off-shore wind power. The techniques from the field of big data analytics for analyzing the PUI are here addressed. The results of this study contain suggestions on the basis of algorithms of data analytics, suitable for each decision type.","Elaheh Gholamzadeh Nabati and Klaus-Dieter Thoben","https:\/\/doi.org\/10.1016\/j.procir.2016.09.026","2017","Data Driven Decision Making in Planning the Maintenance Activities of Off-shore Wind Energy"],["ABSTRACT\nArtificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on \u201cbig data\u201d and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.","Devin S. Zarkowsky and David P. Stonko","https:\/\/doi.org\/10.1053\/j.semvascsurg.2021.10.005","2021","Artificial intelligence's role in vascular surgery decision-making"],["With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.","Vijay Khatri","https:\/\/doi.org\/10.1016\/j.bushor.2016.06.001","2016","Managerial work in the realm of the digital universe: The role of the data triad"],["The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.","P. Amuthabala and R. Santhosh","https:\/\/doi.org\/10.1016\/j.compeleceng.2019.02.003","2019","Robust analysis and optimization of a novel efficient quality assurance model in data warehousing"],["Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.","Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas","https:\/\/doi.org\/10.1016\/j.is.2021.101953","2022","EQUALITY: Quality-aware intensive analytics on the edge"],["This chapter introduces basic terminology of data warehousing, its applications, and the business context. It provides a brief description of its history and where it is heading. Basic data warehouse architectures that have been established in the industry are presented. Issues faced by data warehouse practitioners are explained, including topics such as big data, changing business requirements, performance issues, complexity, auditability, restart checkpoints, and fluctuation of team members.","Daniel Linstedt and Michael Olschimke","https:\/\/doi.org\/10.1016\/B978-0-12-802510-9.00001-5","2016","Chapter 1 - Introduction to Data Warehousing"],["Advances in digitalization present new and emerging Supply Chain (SC) Information Architectures that rely on data and information as vital resources. While the importance of data and information in SCs has long been understood, there is a dearth of research or understanding about the effective governance, control, or management of data ecosystems at the SC level. This chapter examines data architectures through a navigation of the background of database management and data quality research of previous decades. The chapter unfolds the critical architectural elements around data and information sharing in the SC regarding the context, systems, and infrastructure. A review of various frameworks and conceptual models is presented on data and information in SCs, as well as access control policies. The critical importance of data quality and the management of data in the cyber-physical systems are highlighted. Policies for data sharing agreements (DSAs) and access control are discussed and the importance of effective governance in the distributed environments of digitally enabled SCs is emphasized. We extend the concept of data sharing agreements to capture the interplay between the various SC stakeholders around data use. Research gaps and needs relevant to new and emerging SC data and information ecosystems are highlighted.","Konstantina Spanaki and Erisa Karafili and Stella Despoudi","https:\/\/doi.org\/10.1016\/B978-0-323-91614-1.00009-5","2022","Chapter 9 - Digital architectures: frameworks for supply chain data and information governance"],["This chapter gives an introduction to concept of enterprise information management, investigates the influence of Big Data on EIM, and discusses today's key challenges and pressures for EIM.","Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall","https:\/\/doi.org\/10.1016\/B978-0-12-405547-6.00002-X","2014","Chapter 2 - Enterprise Information Management"],["The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.","Andr\u00e9s Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Gr\u00f8n S\u00f8rensen","https:\/\/doi.org\/10.1016\/j.biosystemseng.2019.12.013","2020","Internet of Things in arable farming: Implementation, applications, challenges and potential"],["The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.","Evagelos D. Lioutas and Chrysanthi Charatsari","https:\/\/doi.org\/10.1016\/j.agsy.2020.103023","2021","Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us"],["In the big data era, new technologies and powerful analytics make it possible to collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This is due to the fact that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the larger scale of data processing and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of data processing. On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interests.","Alessandro Mantelero","https:\/\/doi.org\/10.1016\/j.clsr.2016.01.014","2016","Personal data for decisional purposes in the age of analytics: From an individual to a collective dimension of data protection"],["Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.","Roberto O Andrade and Sang Guun Yoo","https:\/\/doi.org\/10.1016\/j.jisa.2019.06.008","2019","Cognitive security: A comprehensive study of cognitive science in cybersecurity"],["The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.","N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni","https:\/\/doi.org\/10.1016\/j.jmsy.2020.04.002","2020","Industrial data management strategy towards an SME-oriented PHM"],["The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.","Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran","https:\/\/doi.org\/10.1016\/j.compag.2018.10.001","2018","Big GIS analytics framework for agriculture supply chains: A literature review identifying the current trends and future perspectives"],["This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.","Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud","https:\/\/doi.org\/10.1016\/j.ijpe.2021.108244","2021","Smart product service system hierarchical model in banking industry under uncertainties"],["In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.","Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos","https:\/\/doi.org\/10.1016\/bs.acc.2020.08.002","2021","Chapter Four - Translational biomarkers in the era of precision medicine"],["Digital technologies are growing in importance for accelerating firms\u2019 circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms\u2019 circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.","Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li","https:\/\/doi.org\/10.1016\/j.techfore.2021.120957","2021","Towards a business analytics capability for the circular economy"],["Manufacturing companies struggle to be efficient and effective when conducting root cause analyses of production disturbances; a fact which hinders them from creating and developing resilient production systems. This article aims to describe the challenges and enablers identified in current research relating to the different phases of root cause analysis. A systematic literature review was conducted, in which a total of 14 challenges and 17 enablers are identified and described. These correlate to the different phases of root cause analysis. Examples of challenges are \u201cneed for expertise\u201d, \u201cemployee bias\u201d, \u201cpoor data quality\u201d and \u201clack of data integration\u201d, among others. Examples of enablers are \u201cvisualisation tools\u201d, \u201ccollaborative platforms\u201d, \u201cthesaurus\u201d and \u201cmachine learning techniques\u201d. Based on these findings, the authors also propose potential areas for further research and then design inputs for new solutions to improve root cause analysis. This article provides a theoretical contribution in that it describes the challenges and enablers of root cause analysis and their correlation to the creation of resilient production systems. The article also provides practical contributions, with an overview of current research to support practitioners in gaining insights into potential solutions to be implemented and further developed, with the aim of improving root cause analysis in production systems.","Adriana Ito and Malin Hagstr\u00f6m and Jon Bokrantz and Anders Skoogh and Mario Nawcki and Kanika Gandhi and Dag Bergsj\u00f6 and Maja B\u00e4rring","https:\/\/doi.org\/10.1016\/j.jmsy.2022.07.015","2022","Improved root cause analysis supporting resilient production systems"],["With increasing population, the demand for agricultural productivity is rising to meet the goal of \u201cZero Hunger\u201d. Consequently, farmers have optimized the agricultural activities in a sustainable way with the modern technologies. This integration has boosted the agriculture production due to high potentiality in assisting the farmers. The impulse towards the technological advancement has revived the traditional agriculture methods and resulted in eco-friendly, sustainable, and efficient farming. This has revolutionized the era of smart farming which primarily alliance with modern technologies like, big data, machine learning, deep learning, swarm intelligence, internet-of-things, block chain, robotics and autonomous system, cloud-fog-edge computing, cyber physical systems, and generative adversarial networks (GAN). To cater the same, a detailed survey on ten hot-spots of smart farming is presented in this paper. The survey covers the technology-wise state-of-the-art methods along with their application domains. Moreover, the publicly available data sets with existing research challenges are investigated. Lastly, the paper concludes with suggestions to the identified problems and possible future research directions.","Vivek Sharma and Ashish Kumar Tripathi and Himanshu Mittal","https:\/\/doi.org\/10.1016\/j.compag.2022.107217","2022","Technological revolutions in smart farming: Current trends, challenges & future directions"],["Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.","Jiaping Wu and Junyu He and George Christakos","https:\/\/doi.org\/10.1016\/B978-0-12-816341-2.00010-1","2022","Chapter 3 - CTDA methodology"],["Radiotherapy is a discipline closely integrated with computer science. Artificial intelligence (AI) has developed rapidly over the past few years. With the explosive growth of medical big data, AI promises to revolutionize the field of radiotherapy through highly automated workflow, enhanced quality assurance, improved regional balances of expert experiences, and individualized treatment guided by multi-omics. In addition to independent researchers, the increasing number of large databases, biobanks, and open challenges significantly facilitated AI studies on radiation oncology. This article reviews the latest research, clinical applications, and challenges of AI in each part of radiotherapy including image processing, contouring, planning, quality assurance, motion management, and outcome prediction. By summarizing cutting-edge findings and challenges, we aim to inspire researchers to explore more future possibilities and accelerate the arrival of AI radiotherapy.","Guangqi Li and Xin Wu and Xuelei Ma","https:\/\/doi.org\/10.1016\/j.semcancer.2022.08.005","2022","Artificial intelligence in radiotherapy"],["In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.","Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja\u2019a}","https:\/\/doi.org\/10.1016\/j.jnlest.2020.100059","2021","Impact of coronavirus pandemic crisis on technologies and cloud computing applications"],["With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.","Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang","https:\/\/doi.org\/10.1016\/j.enbuild.2015.09.060","2015","Temporal knowledge discovery in big BAS data for building energy management"],["The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.","Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kj\u00e6rgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan","https:\/\/doi.org\/10.1016\/j.buildenv.2020.106964","2020","Modelling urban-scale occupant behaviour, mobility, and energy in buildings: A survey"],["Summary\nEarly quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic\/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.","Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang","https:\/\/doi.org\/10.1016\/j.isci.2021.103052","2021","Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models"],["In the last few decades, deep learning techniques for diagnosing and predicting disease conditions from neuroimaging have attracted much attention and interest from the scientific community. Big data and artificial intelligence approaches and innovations are currently being utilized to generate large datasets from images, text, sounds, graphs, and signals. New trends in the utilization of deep learning for disease prediction in neurology, oncology, cardiology, and other areas entail converting patient electronic health records, biological system information, physiological signals, biomarkers, and biomedical images to cognitive functions. The current trends in deep learning techniques focus on utilizing neuroimaging analysis to evaluate alterations in local morphological topographies of different brain sub-regions and then predict novel disorder-linked brain patterns. Hence, this chapter presents a detailed overview of different approaches in deep learning for the prediction of major brain diseases such as mild cognitive impairment, Alzheimer's disease, brain tumors, depressive disorders, traumatic brain injury, schizophrenia, Parkinson's disease, autism spectrum disease, attention-deficit hyperactivity disorder, epilepsy, stroke, multiple sclerosis, and more. The chapter also discusses the current challenges of utilizing deep learning in assessing brain disorders in neuroimaging data.","Olugbemi T. Olaniyan and Charles O. Adetunji and Ayobami Dare and Olorunsola Adeyomoye and Mayowa J. Adeniyi and Alex Enoch","https:\/\/doi.org\/10.1016\/B978-0-323-90277-9.00012-2","2023","Chapter 17 - New trends in deep learning for neuroimaging analysis and disease prediction"],["Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.","Ghasan Fahim Huseien and Kwok Wei Shah","https:\/\/doi.org\/10.1016\/j.egyai.2021.100116","2022","A review on 5G technology for smart energy management and smart buildings in Singapore"],["Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system for data ingestion and reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big data volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and the big data architecture for the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.","Pierfrancesco Bellini and Monica Benigni and Riccardo Billero and Paolo Nesi and Nadia Rauch","https:\/\/doi.org\/10.1016\/j.jvlc.2014.10.023","2014","Km4City ontology building vs data harvesting and cleaning for smart-city services"],["Purpose\nInterest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.\nMethods\nWe assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.\nFindings\nThis article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.\nImplications\nCross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.","Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell","https:\/\/doi.org\/10.1016\/j.clinthera.2022.01.012","2022","Real-World Evidence for Regulatory Decision-Making: Guidance From Around the World"],["Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers\u2019 participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.","Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and Fran\u00e7ois M\u00e9lard and Bernard Tychon and Joost Wellens and Pierre Defourny","https:\/\/doi.org\/10.1016\/j.compag.2017.08.026","2017","Crowdsourcing for agricultural applications: A review of uses and opportunities for a farmsourcing approach"],["State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.","Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan","https:\/\/doi.org\/10.1016\/j.est.2021.102867","2021","State-of-health estimation based on real data of electric vehicles concerning user behavior"],["Planning support systems (PSS) enabled by smart city technologies (big data and information and communication technologies (ICTs)) are becoming more widespread in their availability, but have not yet been fully recognized as being useful in planning practice. Thus, a better understanding of the determinants of PSS usefulness in practice helps to improve the functional support of PSS for smart cities. This study is based on a recent international questionnaire (268 respondents) designed to evaluate the perceptions of scholars and practitioners in the smart city planning field. Based on the empirical evidence, this paper recommends that it is imperative for PSS developers and users to be more responsive to the fit for task-technology and user-technology (i.e., utility and usability, respectively) since they positively contribute to PSS usefulness in practice and to be more sensitive to the potential negative effects of contextual factors on PSS usefulness in smart cities. The empirical analyses further suggest that rather than merely striving for integrating smart city technologies into advancing PSS, the way that innovative PSS are integrated into the planning framework (i.e., how well PSS can satisfy the needs of planning tasks and users by considering context-specificities) is of great significance in promoting PSS's actual usefulness.","Huaxiong Jiang and Stan Geertman and Patrick Witte","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2020.101505","2020","Ignorance is bliss? An empirical analysis of the determinants of PSS usefulness in practice"],["Background\nArtificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.\nMethods and results\nThis review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%\u201357% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.\nConclusion\nThese examples describe the use of artificial intelligence methods in the field of hypertension.","Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis","https:\/\/doi.org\/10.1016\/j.ijchy.2020.100027","2020","Uses and opportunities for machine learning in hypertension research"],["Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.","Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark","https:\/\/doi.org\/10.1016\/j.scitotenv.2021.150742","2022","Continental patterns in marine debris revealed by a decade of citizen science"],["Traditional air quality data have a spatial resolution of 1\u00a0km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1\u00a0km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.","Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau","https:\/\/doi.org\/10.1016\/j.jes.2022.02.041","2023","Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data"],["Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists\u2019 medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.","Lijing Yao and Hengyuan Zhang and Mengqin Zhang and Xing Chen and Jun Zhang and Jiyi Huang and Lu Zhang","https:\/\/doi.org\/10.1016\/j.ceh.2021.11.003","2021","Application of artificial intelligence in renal disease"],["Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).","Judy S. LaKind and Cian O\u2019Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman","https:\/\/doi.org\/10.1016\/j.envres.2019.01.039","2019","ExpoQual: Evaluating measured and modeled human exposure data"],["Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories \u2013 morphological, biochemical, physiological, and performance traits \u2013 and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.","Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen","https:\/\/doi.org\/10.1016\/j.tplants.2021.07.015","2022","Advances in optical phenotyping of cereal crops"],["This chapter discusses various types of sampling such as random sampling and sampling based on business criteria (age of customer, time as client, etc.). It also discusses extracting train and test datasets for specific business objectives and considers the issue of Big Data, given that it is currently a hot topic.","David Nettleton","https:\/\/doi.org\/10.1016\/B978-0-12-416602-8.00007-8","2014","Chapter 7 - Data Sampling and Partitioning"],["Remote sensing image products (e.g. brightness of nighttime lights and land cover\/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100\u202f\u00d7\u202f100\u202fm grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.","Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia","https:\/\/doi.org\/10.1016\/j.scitotenv.2018.12.276","2019","Improved population mapping for China using remotely sensed and points-of-interest data within a random forests model"],["Humanitarian mapping refers to the production of spatial data and cartographic products to improve situational awareness and decision-making around humanitarian issues from acute events such as natural disasters and public health emergencies to longer term events such as refugee crises and political unrest. Mapping is a key part of the broader area of humanitarian information management, which has traditionally been undertaken by governments and international humanitarian organizations. As a core aspect of the field of digital humanitarianism, mapping activities are now widely undertaken by smaller organizations and networks of volunteers who produce spatial data and maps on the ground and remotely via the use of Web mapping and mobile phone technologies. Big data based on location and behavioral attributes produced online and through interaction with digital systems and networks can also be exploited to enhance information environments. Together, these new developments signal new possibilities for improved risk and crisis management, based on up-to-date high resolution spatial and temporal evidence. Research in human geography, geographic information science, and related disciplines focuses on tracing benefits such as increased speed and low costs, as well as the risks of relying on distributed volunteers and new sources of data of questionable accuracy and validity.","Jonathan Cinnamon","https:\/\/doi.org\/10.1016\/B978-0-08-102295-5.10559-1","2020","Humanitarian Mapping"],["The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict\/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where \u201ctimestamp\u201d was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.","Nasim Sadat Mosavi and Francisco Freitas and Rog\u00e9rio Pires and C\u00e9sar Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais","https:\/\/doi.org\/10.1016\/j.procs.2022.03.065","2022","Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities"],["Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel \/a\/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%\u201319.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel \/a\/ and serving as big data pool.","Pak Ho Leung and Kwok Tai Chui and Kenneth Lo and Patricia Ord\u00f3\u00f1ez {de Pablos}","https:\/\/doi.org\/10.1016\/B978-0-12-822060-3.00014-0","2021","Chapter 13 - A support vector machine\u2013based voice disorders detection using human voice signal"],["Rapid urbanization, population increase, emerging contaminants and increasing water scarcity have put a major constraint on the wastewater treatment system. Scarcity of water is steering current way of water recycle, and the drive focus towards resource recovery. Zero waste pathway in circular bioeconomy can bring transformation of wastewater commercialization by adding value with resource recovery. The complex biological reactions, unforeseen microbial behaviours, lack of reliable on-line instrumentation, complex modelling, lack of visualize techniques, low-quality industrial measurements and highly time-varying intensive data-driven operations call for the intelligence techniques and operations. The study is a review of sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant. Water surveillance and monitoring, circular economy and sustainability, automation pyramid, digital transformation, artificial intelligence, data pipeline, digital twin, data mining, and data-driven visualization, cyber-physical systems and water-energy-health management were reviewed. The deployment of the digital systems has evidently proven to bridges the gap between the data-driven soft sensor, operation and control systems in WWTP. Accurate prediction of the WWTP variables can support process design and control, reduce operation cost, improve system reliability, predictive maintenance and troubleshooting, increase water quality, increase stakeholder's engagement and endorse optimization of the plant performance. This procures the best compliance with international standards and diversification. The inclusion of life cycle environmental or cost management technologies in optimization models is an interesting pathway towards sustainable water treatment in-line with sustainable development goals, circular bioeconomy and industry 4.0.","Anthony Njuguna Matheri and Belaid Mohamed and Freeman Ntuli and Esther Nabadda and Jane Catherine Ngila","https:\/\/doi.org\/10.1016\/j.pce.2022.103152","2022","Sustainable circularity and intelligent data-driven operations and control of the wastewater treatment plant"],["The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.","Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar","https:\/\/doi.org\/10.1016\/j.giq.2021.101624","2022","Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy"],["The term big data has come into use in recent years. It is used to refer to the ever-increasing amount of data that organizations are storing, processing and analyzing. An Interesting fact with bigdata is that it differ in Volume, Variety, Velocity characteristics which makes it difficult to process using the conventional Database Management System. Hence there is a need of schema less Management Systems even this will never be complete solution to bigdata analysis since the processing has no focus on the semantic information as they consider only the structural information. Content Management System like Wikipedia stores and links huge amount of documents and files. There is lack of semantic linking and analysis in such systems even though this kind of CMS uses clusters and distributed framework for storing big data. The retrieved references for a particular article are random and enormous. In order to reduce the number of references for a selected content there is a need for semantic matching. In this paper we propose framework which make use of the distributed parallel processing capability of Hadoop Distributed File System (HDFS) to perform semantic analysis over the volume of documents (bigdata) to find the best matched source document from the collection source documents for the same virtual document.","R. Priyadarshini and Latha Tamilselvan and T. Khuthbudin and S. Saravanan and S. Satish","https:\/\/doi.org\/10.1016\/j.procs.2015.06.043","2015","Semantic Retrieval of Relevant Sources for Large Scale Virtual Documents"],["In online review systems, a participant's level of knowledge impacts his\/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.","Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu","https:\/\/doi.org\/10.1016\/j.ins.2019.09.053","2020","Clicking position and user posting behavior in online review systems: A data-driven agent-based modeling approach"],["Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.","Maryam Ghasemaghaei and Goran Calic","https:\/\/doi.org\/10.1016\/j.jbusres.2019.07.006","2019","Does big data enhance firm innovation competency? The mediating role of data-driven insights"],["Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.","Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan}","https:\/\/doi.org\/10.1016\/j.is.2014.07.006","2015","The rise of \u201cbig data\u201d on cloud computing: Review and open research issues"],["Introduction\nMachine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning\/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.\nMethods\nDatabases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.\nResults\nResults indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.\nConclusions\nMachine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.","Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre","https:\/\/doi.org\/10.1016\/j.artmed.2019.101704","2019","Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry"],["Big Data Governance and big data stewardship are not so different from what we\u2019ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of \u201cgetting it wrong\u201d due to not only the large quantities of data and metadata, but also the speed at which the data can change.","David Plotkin","https:\/\/doi.org\/10.1016\/B978-0-12-822132-7.00010-3","2021","Chapter 10 - Big Data Stewardship and Data Lakes"],[null,"Lynn E. Bayne","https:\/\/doi.org\/10.1016\/j.cnc.2018.07.005","2018","Big Data in Neonatal Health Care: Big Reach, Big Reward?"],["Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V\u2019s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.","Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi","https:\/\/doi.org\/10.1016\/j.procs.2019.01.005","2019","Assessing reliability of Big Data Knowledge Discovery process"],["Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.","Wang Kun and Liu Tong and Xie Xiaodan","https:\/\/doi.org\/10.1016\/j.procs.2019.01.221","2019","Application of Big Data Technology in Scientific Research Data Management of Military Enterprises"],["The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.","Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu","https:\/\/doi.org\/10.2478\/dim-2018-0014","2018","Big Data in Health Care: Applications and Challenges"],["Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.","Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi","https:\/\/doi.org\/10.1016\/j.jbusres.2016.08.007","2017","Factors influencing big data decision-making quality"],["Since the first version of the Entity\u2013Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER\u201918) held in Xi\u2019an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.","Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey","https:\/\/doi.org\/10.1016\/j.datak.2021.101911","2021","Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue"],["With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.","Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng","https:\/\/doi.org\/10.1016\/j.cities.2021.103298","2021","Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data"],["As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.","Shuangqi Li and Hongwen He and Jianwei Li","https:\/\/doi.org\/10.1016\/j.apenergy.2019.03.154","2019","Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology"],["Although big data analytics (BDA) is considered the next \u201cfrontier\u201d in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.","Nadine C\u00f4rte-Real and Pedro Ruivo and Tiago Oliveira and Ale\u0161 Popovi\u010d","https:\/\/doi.org\/10.1016\/j.jbusres.2018.12.072","2019","Unlocking the drivers of big data analytics value in firms"],["Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.","Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati","https:\/\/doi.org\/10.1016\/j.procs.2019.09.208","2019","A New Architecture for Cognitive Internet of Things and Big Data"],["Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.","Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida","https:\/\/doi.org\/10.1016\/j.jclepro.2018.11.025","2019","A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions"],["The recent explosion in ICT and digital data has led organizations, both private and public, to efficient decision-making. Nowadays organizations can store huge amounts of data, which can be accessible at any time. Big Data governance refers to the management of huge volumes of an organization\u2019s data, exploiting it in the organization\u2019s decision-making using different analytical tools. Big Data emergence provides great convenience, but it also brings challenges. Nevertheless, for Big Data governance, data has to be prepared in a timely manner, keeping in view the consistency and reliability of the data, and being able to trust its source and the meaningfulness of the result. Hence, a framework for Big Data governance would have many advantages. There are Big Data governance frameworks, which guide the management of Big Data. However, there are also limitations associated with these frameworks. Therefore, this study aims to explore the existing Big Data governance frameworks and their shortcomings, and propose a new framework. The proposed framework consists of eight components. As a framework validation, the proposed framework has been compared with the ISO 8000 data governance framework.","Ali Al-Badi and Ali Tarhini and Asharul Islam Khan","https:\/\/doi.org\/10.1016\/j.procs.2018.10.181","2018","Exploring Big Data Governance Frameworks"],["Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert\u00ae system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.","Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt","https:\/\/doi.org\/10.1016\/j.trip.2019.100071","2019","An adaptive big data weather system for surface transportation"],["The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.","Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan","https:\/\/doi.org\/10.1016\/j.tust.2021.104285","2022","Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning"],["In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.","Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao","https:\/\/doi.org\/10.1016\/j.ins.2018.11.029","2019","Data source selection for information integration in big data era"],["With the growing scale and complexity of city infrastructures, the need for data analysis and machine learning is becoming more and more prominent in the field of civil infrastructure sensing. This coupled with the explosion of available sensing data in smart cities and smart infrastructures has offered new opportunities like never before. Using big data tools at a structure level, we can understand important information about structural properties and damage states, city environmental and operational conditions, as well as an individual user or group patterns. In this chapter, we explore and provide guidance for big data analytics and its application to civil infrastructure problems. Furthermore, we discuss future directions and trends that will enable large-scale monitoring of civil infrastructure and smart cities.","Hae Young Noh and Jonathon Fagert","https:\/\/doi.org\/10.1016\/B978-0-08-102706-6.00007-6","2022","20 - Big data analysis for civil infrastructure sensing"],["Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.","Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain","https:\/\/doi.org\/10.1016\/j.array.2020.100033","2020","An efficient integration and indexing method based on feature patterns and semantic analysis for big data"],["This work discusses the use of big data and machine learning to predict the severity of a system breakdown caused by an n\u22121 transmission line condition. The contingency analysis is a key part of traditional energy management systems. The severity of the line is identified by computing the Line Voltage Stability Index The large amount of data handling will be involved during the contingency study. These data need to be processed and analyzed properly by data handling technique and the use of machine learning tools arrive required information in the system. The severity of transmission lines is predicted and compared using classification approaches. To create large data, MATLAB simulation results will be used and the machine learning tool, Weka is used to analyze the data and forecast transmission line. The standard IEEE 30 bus system is considered to understand the proposed methodology.","Ravi V. Angadi and Suresh Babu Daram and P.S. Venkataramu","https:\/\/doi.org\/10.1016\/B978-0-323-90789-7.00004-X","2022","Chapter Seven - Role of big data analytic and machine learning in power system contingency analysis"],["The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.","Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour}","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.07.005","2018","Management theory and big data literature: From a review to a research agenda"],["Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data \u2013 including its new methods and functions \u2013 must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.","H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer","https:\/\/doi.org\/10.1016\/j.giq.2018.10.011","2019","Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?"],["A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm\u2019s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.","Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou","https:\/\/doi.org\/10.1016\/j.im.2019.05.004","2020","Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities"],["The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.","Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole","https:\/\/doi.org\/10.1016\/j.future.2019.03.052","2019","Spatial urban data system: A cloud-enabled big data infrastructure for social and economic urban analytics"],["Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.","Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.05.017","2019","Healthcare big data processing mechanisms: The role of cloud computing"],[null,"Yoshiaki Ueda and Shuichi Yanagisawa","https:\/\/doi.org\/10.1016\/j.molp.2019.01.008","2019","Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era"],["This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.","Jeffrey Hughes and Kirstie Ball","https:\/\/doi.org\/10.1016\/j.techfore.2020.120300","2020","Sowing the seeds of value? Persuasive practices and the embedding of big data analytics"],["The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions\u2014data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.","Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti","https:\/\/doi.org\/10.1016\/j.jretai.2016.12.004","2017","The Role of Big Data and Predictive Analytics in Retailing"],["The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.","Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem","https:\/\/doi.org\/10.1016\/j.indmarman.2019.09.010","2020","Understanding market agility for new product success with big data analytics"],["The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.","P. Galetsi and K. Katsaliaki and S. Kumar","https:\/\/doi.org\/10.1016\/j.socscimed.2019.112533","2019","Values, challenges and future directions of big data analytics in healthcare: A systematic review"],[null,"Miltiadis D. Lytras and Anna Visvizi and Akila Sarirete and Kwok Tai Chui","https:\/\/doi.org\/10.1016\/B978-0-12-822060-3.00018-8","2021","Preface: artificial intelligence and big data analytics for smart healthcare: a digital transformation of healthcare primer"],["Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.","Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera","https:\/\/doi.org\/10.1016\/j.future.2019.04.020","2019","The role of big data analytics in industrial Internet of Things"],["This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, \u201cbig data\u201d is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary \u201cV's\u201d of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.","Debashis Das and Chinmay Chakraborty and Sourav Banerjee","https:\/\/doi.org\/10.1016\/B978-0-12-818556-8.00007-0","2020","Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare"],["Data quality evaluation is built upon data quality measurement results. \u201cData quality evaluation\u201d uses the \u201cdata quality rules\u201d representing the risk appetite of the organization to decide on the usability of the data; \u201cdata quality measurement\u201d uses the business rules describing the \u201cdata requirements\u201d or \u201cdata specifications\u201d to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO\/IEC 25012 and ISO\/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.","Ismael Caballero and Fernando Gualo and Mois\u00e9s Rodr\u00edguez and Mario Piattini","https:\/\/doi.org\/10.1016\/j.is.2022.102058","2022","BR4DQ: A methodology for grouping business rules for data quality evaluation"],["Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.","Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki","https:\/\/doi.org\/10.1016\/j.jbusres.2020.01.022","2020","Customer experience management in the age of big data analytics: A strategic framework"],["Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings\u2014clinical treatment, large-scale interventions, and language documentation\u2014where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.","Margaret Cychosz and Alejandrina Cristia","https:\/\/doi.org\/10.1016\/bs.acdb.2021.12.001","2022","Chapter One - Using big data from long-form recordings to study development and optimize societal impact"],["ABSTRACT\nObjective\nTo examine the data quality and usability of visual acuity (VA) data extracted from an electronic health record (EHR) system during ophthalmology encounters and provide recommendations for consideration of relevant VA endpoints in retrospective analyses.\nDesign\nRetrospective, EHR data analysis\nParticipants\nAll patients with eyecare office encounters at any one of the nine locations of a large academic medical center between August 1st 2013 and December 31st 2015.\nMethods\nData from 13 of the 21 VA fields (accounting for 93% VA data) in EHR encounters were extracted, categorized, recoded, and assessed for conformance and plausibility using an internal data dictionary, a 38-item listing of VA line measurements and observations including 28 line-measurements (e.g., 20\/30, 20\/400) and 10 observations (e.g., NLP [no light perception]). Entries were classified into usable and unusable data. Usable data were further categorized based on conformance to internal data dictionary: (1) exact match; (2) conditional conformance, letter count (e.g., 20\/30+2-3); (3) convertible conformance (e.g., 5\/200 to 20\/800); (4) plausible but cannot be conformed (e.g., 5\/400). Data were deemed unusable when they were not plausible.\nMain Outcome Measures\nProportions of usable and unusable VA entries at the overall and subspecialty levels.\nResults\nAll VA data from 513,036 encounters representing 166,212 patients were included. Of the 1,573,643 VA entries, 1,438,661 (91.4%) contained usable data. There were 1,196,720 (76.1%) exact match (category 1), 185,692 (11.8%) conditional conformance (category 2), 40,270 (2.6%) convertible conformance (category 3), and 15,979 (1.1%) plausible but not conformed entries. VA entries during visits with providers from retina (17.5%), glaucoma (14.0%), neuro-ophthalmology (8.9%), and low vision (8.8%) had the highest rates of unusable data. Documented VA entries with providers from comprehensive eye care (86.7%), oculoplastics (81.5%), and pediatrics\/strabismus (78.6%) yielded the highest proportions of exact match with the data dictionary.\nConclusions\nEHR VA data quality and usability vary across documented VA measures, observations, and eyecare subspecialty. We proposed a checklist of considerations and recommendations for planning, extracting, analyzing, and reporting retrospective study outcomes using EHR VA data. These are important first steps to standardize analyses enabling comparative research.","Judith E. Goldstein and Xinxing Guo and Michael V. Boland and Kerry E. Smith","https:\/\/doi.org\/10.1016\/j.xops.2022.100215","2022","Visual Acuity \u2013 Assessment of Data Quality and Usability in an Electronic Health Record System"],["Context:\nOver the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.\nObjective:\nAlthough a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).\nMethod:\nAs the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.\nResults:\nIn total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects\/bug fixing, testing, project management analytics, and application usage analytics.\nConclusions:\nAs BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.","Katarzyna Biesialska and Xavier Franch and Victor Munt\u00e9s-Mulero","https:\/\/doi.org\/10.1016\/j.infsof.2020.106448","2021","Big Data analytics in Agile software development: A systematic mapping study"],["Background\nBig data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.\nObjectives\nTo methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.\nMethods\nWe demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.\nResults\nThe results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.\nConclusions\nThe results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.","Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner","https:\/\/doi.org\/10.1016\/j.hlpt.2018.12.003","2019","Bringing big data analytics closer to practice: A methodological explanation and demonstration of classification algorithms"],["Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.","Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede","https:\/\/doi.org\/10.1016\/j.jclepro.2019.03.181","2019","Linking big data analytics and operational sustainability practices for sustainable business management"],["In this chapter we look at the need for oversight and governance for the data, especially when those developing big data applications often bypass traditional IT and data management channels. Some of the key issues involve the fact that for big data applications that consume massive amounts of data streamed from external sources, there is little or no control that can be exerted to ensure data quality and usability. We consider five key concepts, namely managing data consumer expectations, defining critical data quality dimensions, monitoring consistency of metadata, data repurposing and reinterpretation, and data enrichment when possible.","David Loshin","https:\/\/doi.org\/10.1016\/B978-0-12-417319-4.00005-3","2013","Chapter 5 - Data Governance for Big Data Analytics: Considerations for Data Policies and Processes"],["The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.","Valeriia Boldosova","https:\/\/doi.org\/10.1016\/j.indmarman.2019.12.004","2020","Telling stories that sell: The role of storytelling and big data analytics in smart service sales"],["This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728\u00a0days\u2019 continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n\u00a0+\u00a01th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.","Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing","https:\/\/doi.org\/10.1016\/j.tust.2020.103636","2021","Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning"],["In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China\u2019s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.","Zhihan Lv and Liang Qiao","https:\/\/doi.org\/10.1016\/j.future.2020.03.039","2020","Analysis of healthcare big data"],["The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.","Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough","https:\/\/doi.org\/10.1016\/j.jksuci.2019.05.009","2020","A fuzzy agent approach for smart data extraction in big data environments"],["\u201cBig data\u201d is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to \u201clong data,\u201d \u201cwide data,\u201d and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.","Tim Hulsen","https:\/\/doi.org\/10.1016\/B978-0-12-822884-5.00016-7","2021","Chapter 4 - Challenges and solutions for big data in personalized healthcare"],[null,"Emily L. Gill and Stephen R. Master","https:\/\/doi.org\/10.1016\/j.cll.2019.11.009","2020","Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model"],["The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders\u2019 interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.","Ant\u00f3nio A.C. Vieira and Lu\u00eds Dias and Maribel Y. Santos and Guilherme A.B. Pereira and Jos\u00e9 Oliveira","https:\/\/doi.org\/10.1016\/j.promfg.2020.02.093","2020","Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio"],["The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.","Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath","https:\/\/doi.org\/10.1016\/j.procs.2020.03.021","2020","IoT and Big Data Analytics for Smart Buildings: A Survey"],["Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.","Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi}","https:\/\/doi.org\/10.1016\/j.cie.2019.106099","2019","Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies"],["Disruptive innovations are usually identified as ideas that are created \u2018outside the box\u2019. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.","Esa H\u00e4m\u00e4l\u00e4inen and Tommi Inkinen","https:\/\/doi.org\/10.1016\/j.jii.2019.100105","2019","Industrial applications of big data in disruptive innovations supporting environmental reporting"],["The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).","Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li","https:\/\/doi.org\/10.1016\/j.procir.2019.05.023","2019","Big data driven decision-making for batch-based production systems"],["Understanding urban vibrancy aids policy-making to foster urban space and therefore has long been a goal of urban studies. Recently, the emerging urban big data and urban analytic methods have enabled us to portray citywide vibrancy. From the social sensing perspective, this study presents a comprehensive and comparative framework to cross-validate urban vibrancy and uncover associated spatial effects. Spatial patterns of urban vibrancy indicated by multisource urban sensing data (points-of-interest, social media check-ins, and mobile phone records) were investigated. A comprehensive urban vibrancy metric was formed by adaptively weighting these metrics. The association between urban vibrancy and demographic, economic, and built environmental factors was revealed with global regression models and local regression models. An empirical experiment was conducted in Shenzhen. The results demonstrate that four urban vibrancy metrics are all higher in the special economic zone (SEZ) and lower in non-SEZs but with different degrees of spatial aggregation. The influences of employment and road density on all vibrancy metrics are significant and positive. However, the effects of metro stations, land use mix, building footprints, and distance to district center depend on the vibrancy indicator and location. These findings unravel the commonalities and differences in urban vibrancy metrics derived from multisource urban big data and the hidden spatial dynamics of the influences of associated factors. They further suggest that urban policies should be proposed to foster vibrancy in Shenzhen therefore benefit social wellbeing and urban development in the long term. They also provide valuable insights into the reliability of urban big data-driven urban studies.","Wei Tu and Tingting Zhu and Jizhe Xia and Yulun Zhou and Yani Lai and Jincheng Jiang and Qingquan Li","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2019.101428","2020","Portraying the spatial dynamics of urban vibrancy using multisource urban big data"],["The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.","Shouyang Wang","https:\/\/doi.org\/10.1016\/j.dsm.2021.01.001","2021","An interview with Shouyang Wang: research frontier of big data-driven economic and financial forecasting"],["Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented \u03b5-constraint method is utilized. The model performance is investigated in a comprehensive computational study.","Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh","https:\/\/doi.org\/10.1016\/j.jclepro.2020.120640","2020","A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data"],["Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.","Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai","https:\/\/doi.org\/10.1016\/j.dss.2020.113382","2020","A note on big data analytics capability development in supply chain"],["In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4\u00a0bar and 25 to 100\u00a0\u00b0C, respectively. Approximately 400\u00a0million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.","Andr\u00e1s Urb\u00e1n and Axel Groniewsky and Milan Mal\u00fd and Viktor J\u00f3zsa and Jan Jedelsk\u00fd","https:\/\/doi.org\/10.1016\/j.fuel.2020.117792","2020","Application of big data analysis technique on high-velocity airblast atomization: Searching for optimum probability density function"],["Background and Objectives\nTo highlight the potential of multiple file record linkage. Linkage increases the value of existing information by supplying missing data or correcting errors in existing data, through generating important covariates, and by using family information to control for unmeasured variables and expand research opportunities.\nMethods\nRecent Manitoba papers highlight the use of linkage to produce better studies. Specific ways in which linkage helps deal with different substantive issues are described.\nResults\nWide data files\u2014files containing considerable amounts of information on each individual\u2014generated by linkage improve research by facilitating better design. Nonexperimental work in particular benefits from such linkages. Population registries are especially valuable in supplying family data to facilitate work across different substantive fields.\nConclusion\nSeveral examples show how record linkage magnifies the value of information from individual projects. The results of observational studies become more defensible through the better designs facilitated by such linkage.","Leslie L. Roos and Elizabeth Wall-Wieler and Charles Burchill and Naomi C. Hamm and Amani F. Hamad and Lisa M. Lix","https:\/\/doi.org\/10.1016\/j.jclinepi.2022.06.006","2022","Record linkage and big data\u2014enhancing information and improving design"],[null,"Aadia I. Rana and Michael J. Mugavero","https:\/\/doi.org\/10.1016\/j.idc.2019.05.009","2019","How Big Data Science Can Improve Linkage and Retention in Care"],["In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.","Shivam Bachhety and Shivani Kapania and Rachna Jain","https:\/\/doi.org\/10.1016\/B978-0-12-820203-6.00008-4","2021","2 - Big Data Analytics for healthcare: theory and applications"],["Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.","Jenifer Pedro and Irwin Brown and Mike Hart","https:\/\/doi.org\/10.1016\/j.procs.2019.12.147","2019","Capabilities and Readiness for Big Data Analytics"],["The consumption data from smart meters and complex questionnaires reveals the electricity consumers\u2019 willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30\u00a0min intervals, as well as data collected from complex questionnaires.","Simona-Vasilica Oprea and Adela B\u00e2ra and Bogdan George Tudoric\u0103 and Maria Ir\u00e8ne C\u0103linoiu and Mihai Alexandru Botezatu","https:\/\/doi.org\/10.1016\/j.compeleceng.2020.106902","2021","Insights into demand-side management with big data analytics in electricity consumers\u2019 behaviour"],["Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users\u2019 behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.","Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline","https:\/\/doi.org\/10.1016\/j.ipm.2019.102122","2020","Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics"],["Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.","Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu","https:\/\/doi.org\/10.1016\/j.jclepro.2019.119299","2020","Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits"],["An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3\u00a0km\u00a0\u00d7\u00a03\u00a0km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23\u201355%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5\u201317% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.","Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan","https:\/\/doi.org\/10.1016\/j.scitotenv.2020.144535","2021","An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms"],["With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.","Mouzhi Ge and Hind Bangui and Barbora Buhnova","https:\/\/doi.org\/10.1016\/j.future.2018.04.053","2018","Big Data for Internet of Things: A Survey"],["Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data\u2019s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.","Kumar Rahul and Rohitash Kumar Banyal","https:\/\/doi.org\/10.1016\/j.procs.2020.06.042","2020","Data Life Cycle Management in Big Data Analytics"],["Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.","Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie","https:\/\/doi.org\/10.1016\/j.jbusres.2019.01.044","2019","Big data analytics and firm performance: Findings from a mixed-method approach"],[null,"Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren","https:\/\/doi.org\/10.1053\/j.jvca.2019.11.012","2020","Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group"],["The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.","Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.08.006","2019","Real-time big data processing for anomaly detection: A Survey"],["This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.","Heiu-Jou Shaw and Cheng-Kuan Lin","https:\/\/doi.org\/10.1016\/j.oceaneng.2021.108953","2021","Marine big data analysis of ships for the energy efficiency changes of the hull and maintenance evaluation based on the ISO 19030 standard"],["In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between \u201cdata in principle\u201d as they are recorded, and the \u201cdata in practice\u201d as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.","Matthew Jones","https:\/\/doi.org\/10.1016\/j.jsis.2018.10.005","2019","What we talk about when we talk about (big) data"],["Cooking emission inventories always have poor spatial resolutions when applying with traditional methods, making their impacts on ambient air and human health remain obscure. In this study, we created a systematic dataset of cooking emission factors (CEFs) and applied it with a new data source, cooking-related point of interest (POI) data, to build up highly spatial resolved cooking emission inventories from the city scale. Averaged CEFs of six particulate and gaseous species (PM, OC, EC, NMHC, OVOCs, VOCs) were 5.92\u202f\u00b1\u202f6.28, 4.10\u202f\u00b1\u202f5.50, 0.05\u202f\u00b1\u202f0.05, 22.54\u202f\u00b1\u202f20.48, 1.56\u202f\u00b1\u202f1.44, and 7.94\u202f\u00b1\u202f6.27\u202fg\/h normalized in every cook stove, respectively. A three-field CEF index containing activity and emission factor species was created to identify and further build a connection with cooking-related POI data. A total of 95,034 cooking point sources were extracted from Beijing, as a study city. In downtown areas, four POI types were overlapped in the central part of the city and radiated into eight distinct directions from south to north. Estimated PM\/VOC emissions caused by cooking activities in Beijing were 4.81\/9.85\u202ft per day. A 3D emission map showed an extremely unbalanced emission density in the Beijing region. Emission hotspots were seen in Central Business District (CBD), Sanlitun, and Wangjing in Chaoyang District and Willow and Zhongguancun in Haidian District. PM\/VOC emissions could be as high as 16.6\/42.0\u202fkg\/d in the searching radius of 2\u202fkm. For PM, the total emissions were 417.4, 389.0, 466.9, and 443.0\u202ft between Q1 and Q4 2019 in Beijing, respectively. The proposed methodology is transferrable to other Chinese cities for deriving enhanced commercial cooking inventories and potentially highlighting the further importance of cooking emissions on air quality and human health.","Pengchuan Lin and Jian Gao and Yisheng Xu and James J. Schauer and Jiaqi Wang and Wanqing He and Lei Nie","https:\/\/doi.org\/10.1016\/j.envpol.2022.120320","2022","Enhanced commercial cooking inventories from the city scale through normalized emission factor dataset and big data"],["Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.","Aboobucker Ilmudeen","https:\/\/doi.org\/10.1016\/B978-0-12-821633-0.00003-9","2021","Chapter 3 - Big data-based frameworks for healthcare systems"],["This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.","Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han","https:\/\/doi.org\/10.1016\/j.indmarman.2020.10.012","2020","Linking big data analytical intelligence to customer relationship management performance"],["The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.","Iris Hausladen and Maximilian Schosser","https:\/\/doi.org\/10.1016\/j.jairtraman.2019.101721","2020","Towards a maturity model for big data analytics in airline network planning"],["Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining\/capturing\/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.","Lang Huang and Chao Wu and Bing Wang","https:\/\/doi.org\/10.1016\/j.jclepro.2019.05.245","2019","Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective"],["Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (\u2018big data\u2019), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015\u20132017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25\u201330% of Yp for ware, seed and starch potato and spring barley, and 35\u201340% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20\u201349%). Current levels of crop water productivity ranged between 13\u202fkg\u202fDM\u202fha\u22121\u202fmm\u22121 for spring barley, ca. 15 kg\u202fDM\u202fha\u22121\u202fmm\u22121 for seed potato, spring onion and winter wheat, 23\u202fkg\u202fDM\u202fha\u22121\u202fmm\u22121 for ware potato and ca. 25\u202fkg\u202fDM\u202fha\u22121\u202fmm\u22121 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that \u2018big data\u2019 are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.","Jo\u00e3o Vasco Silva and Tom\u00e1s R. Tenreiro and L\u00e9on Sp\u00e4tjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma","https:\/\/doi.org\/10.1016\/j.fcr.2020.107828","2020","Can big data explain yield variability and water productivity in intensive cropping systems?"],["Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.","Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan","https:\/\/doi.org\/10.1016\/j.apenergy.2019.02.002","2019","Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?"],["As the demand for software to support the processing and analysis of massive radio astronomy datasets increases in the era of the SKA, we demonstrate the interactive workflow building, data mining, processing, and visualisation capabilities of DUG Insight. We test the performance and flexibility of DUG Insight by processing almost 68,000 full sky radio images produced from the Engineering Development Array (EDA2) over the course of a three day period. The goal of the processing was to passively detect and identify known Resident Space Objects (RSOs: satellites and debris in orbit) and investigate how radio interferometry could be used to passively monitor aircraft traffic. These signals are observable due to both terrestrial FM radio signals reflected back to Earth and out-of-band transmission from RSOs. This surveillance of the low Earth orbit and airspace environment is useful as a contribution to space situational awareness and aircraft tracking technology. From the observations, we made 40 detections of 19 unique RSOs within a range of 1,500 km from the EDA2. This is a significant improvement on a previously published study of the same dataset and showcases the flexible features of DUG Insight that allow the processing of complex datasets at scale. Future enhancements of our DUG Insight workflow will aim to realise real-time acquisition, detect unknown RSOs, and continue to process data from SKA-relevant facilities.","D. Grigg and S.J. Tingay and M. Sokolowski and R.B. Wayth","https:\/\/doi.org\/10.1016\/j.ascom.2022.100619","2022","DUG Insight: A software package for big-data analysis and visualisation, and its demonstration for passive radar space situational awareness using radio telescopes"],["From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.","Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin","https:\/\/doi.org\/10.1016\/j.rcim.2020.102026","2021","A big data-driven framework for sustainable and smart additive manufacturing"],["Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.","Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox","https:\/\/doi.org\/10.2478\/dim-2020-0003","2020","Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning"],["In the \u201cInternet+\u201d era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP\u2019s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a \u201cwin\u2013win\u201d situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer\u2019s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP\u2019s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP\u2019s overconfidence and cost-sharing strategies may damage the supplier\u2019s profit, the total profit of the CLSC increases.","Zehua Xiang and Minli Xu","https:\/\/doi.org\/10.1016\/j.cie.2020.106538","2020","Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence"],["The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.","Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben","https:\/\/doi.org\/10.1016\/j.ssresearch.2016.04.015","2016","The role of administrative data in the big data revolution in social science research"],["Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.","Sara Barja-Martinez and M\u00f2nica Arag\u00fc\u00e9s-Pe\u00f1alba and \u00cdngrid Munn\u00e9-Collado and Pau Lloret-Gallego and Eduard Bullich-Massagu\u00e9 and Roberto Villafafila-Robles","https:\/\/doi.org\/10.1016\/j.rser.2021.111459","2021","Artificial intelligence techniques for enabling Big Data services in distribution networks: A review"],["Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.","Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran","https:\/\/doi.org\/10.1016\/j.comcom.2020.01.016","2020","Deep learning and big data technologies for IoT security"],["In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient\u2019s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.","Blagoj Ristevski and Snezana Savoska","https:\/\/doi.org\/10.1016\/B978-0-12-820203-6.00005-9","2021","4 - Healthcare and medical Big Data analytics"],["As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) \u2013 broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and\/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.","Mark D. McCoy","https:\/\/doi.org\/10.1016\/j.jas.2017.06.003","2017","Geospatial Big Data and archaeology: Prospects and problems too great to ignore"],["The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers\u2019 evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.","Marcello M. Mariani and Samuel {Fosso Wamba}","https:\/\/doi.org\/10.1016\/j.jbusres.2020.09.012","2020","Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies"],[null,"A.L. Caissie and M.L. Mierzwa and C.D. Fuller and M. Rajaraman and A. Lin and A.M. McDonald and R.A. Popple and Y. Xiao and L. {van Dijk} and P. Balter and H. Fong and H. Ping and M. Kovoor and J. Lee and A. Rao and M.K. Martel and R.F. Thompson and B. Merz and J. Yao and C. Mayo","https:\/\/doi.org\/10.1016\/j.ijrobp.2020.07.224","2020","Radiotherapy (RT) Patterns Of Practice Variability Identified As A Challenge To Real-World Big Data: Recommendations From The Learning From Analysis Of Multicenter Big Data Aggregation (LAMBDA) Consortium"],["Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers\u2019 limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.","Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}","https:\/\/doi.org\/10.1016\/j.njas.2019.04.003","2019","Key questions on the use of big data in farming: An activity theory approach"],["With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data \u2013 MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.","Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo","https:\/\/doi.org\/10.1016\/j.jnca.2018.09.014","2018","Multimedia big data computing and Internet of Things applications: A taxonomy and process model"],["The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.","Xiang Li and Ning Guo and Quanzheng Li","https:\/\/doi.org\/10.1016\/j.gpb.2018.11.005","2019","Functional Neuroimaging in the New Era of Big Data"],["Deep learning has played a crucial role in the field of smart agriculture and been widely used in various applications. However, the deep learning models are constrained by data quality, which means poor data quality and unreliable data annotation will seriously restrict the performance of smart applications. In this paper, we proposed two methods to assess data quality, named Bound-DE and Multi-Branch. In experiments, the IP06 dataset and the ResNet-18 backbone network were adopted. The results show that the redundancy of the used public dataset is so large that about 50% of the data can achieve the similar test accuracy. Furthermore, we also analyzed the high contributive samples and summarized the rules of those selected informative samples, which is significant for the design of high-efficiency datasets. In summary, this study guides and promotes the following data-centric research in the field of smart agriculture.","Jiachen Yang and Guipeng Lan and Yang Li and Yicheng Gong and Zhuo Zhang and Sezai Ercisli","https:\/\/doi.org\/10.1016\/j.compeleceng.2022.108322","2022","Data quality assessment and analysis for pest identification in smart agriculture"],["Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company\u2019s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.","Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang","https:\/\/doi.org\/10.1016\/j.ins.2019.10.019","2020","Privacy-preserving clustering for big data in cyber-physical-social systems: Survey and perspectives"],["Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.","Ant\u00f3nio A.C. Vieira and Lu\u00eds Dias and Maribel Y. Santos and Guilherme A.B. Pereira and Jos\u00e9 Oliveira","https:\/\/doi.org\/10.1016\/j.promfg.2020.02.033","2020","Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context"],[null,"Lars Lundberg and H\u00e5kan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab","https:\/\/doi.org\/10.1016\/j.bdr.2021.100244","2021","Editorial to the Special Issue on Big Data in Industrial and Commercial Applications"],["Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).","Jos\u00e9 Ricardo L\u00f3pez-Robles and Marisela Rodr\u00edguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jes\u00fas Cobo","https:\/\/doi.org\/10.1016\/j.procs.2019.12.044","2019","The last five years of Big Data Research in Economics, Econometrics and Finance: Identification and conceptual analysis"],["In spite of their development in different communities, either astro-informatics or geo-informatics, data management and analytics of astronomical and geospatial data share the same characteristics, and raise the same challenges when it comes to access, query, or analysis of the spatial features over Big Data. The very first challenge is to deal with the data volume, which is tremendous in many geo and astro datasets. In this chapter, we highlight their main specificity and outline the main steps of query processing in big geospatial and astronomical data servers. Through the review of the state of the art, we show the advance in the topic of Big Data management in both contexts of geospatial and sky surveying, while highlighting their similarity. This progress notwithstanding, several issues remain to deal with the variety (such as multidimensional arrays) of the data.","Karine Zeitouni and Mariem Brahem and Laurent Yeh and Atanas Hristov","https:\/\/doi.org\/10.1016\/B978-0-12-819154-5.00018-7","2020","Chapter 8 - Query Processing and Access Methods for Big Astro and Geo Databases"],["Background\nChronic diseases, such as opioid use disorder (OUD) require a multifaceted scientific approach to address their evolving complexity. The Council for the Advancement of Nursing Science's (Council) four nursing science priority areas (precision health; global health, determinants of health, and big data\/data analytics) were established to provide a framework to address current complex health problems.\nPurpose\nTo examine OUD research through the nursing science priority areas and evaluate the appropriateness of the priority areas as a framework for research on complex health conditions.\nMethod\nOUD was used as an exemplar to explore the relevance of the nursing science priorities for future research.\nFindings\nResearch in the four priority areas is advancing knowledge in OUD identification, prevention, and treatment. Intersection of OUD research population focus and methodological approach was identified among the priority areas.\nDiscussion\nThe Council priorities provide a relevant framework for nurse scientists to address complex health problems like OUD.","Patricia Eckardt and Donald Bailey and Holli A. DeVon and Cynthia Dougherty and Pamela Ginex and Cheryl A. Krause-Parello and Rita H. Pickler and Therese S. Richmond and Eleanor Rivera and Carol F. Roye and Nancy Redeker","https:\/\/doi.org\/10.1016\/j.outlook.2020.02.001","2020","Opioid use disorder research and the Council for the Advancement of Nursing Science priority areas"],["Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999\u20132018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.","Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei","https:\/\/doi.org\/10.1016\/j.tmp.2020.100715","2020","Knowledge mapping of tourism demand forecasting research"],["This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access \u2018big data\u2019 of highway traffic and accidents in England for the period 2012\u20132014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the \u2018rubbernecking\u2019 effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.","Ilias Pasidis","https:\/\/doi.org\/10.1016\/j.jtrangeo.2017.10.006","2019","Congestion by accident? A two-way relationship for highways in England"],["Big data of online product purchases is an emerging source for obtaining customers\u2019 preferences of product features for new product development. This paper proposes a framework and associated method for product features characterization and customers\u2019 preference prediction based on online product purchase data. Specifications and components of products are firstly analyzed and the relationships between product specifications and components are then established for features characterization. The customers preferred specifications, features and their combinations are predicted for development of new products. The features characterization and customers\u2019 preferences prediction of toy cars were used as an example of illustrating the proposed method.","Jian Zhang and Alessandro Simeone and Peihua Gu and Bo Hong","https:\/\/doi.org\/10.1016\/j.cirp.2018.04.020","2018","Product features characterization and customers\u2019 preferences prediction based on purchasing data"],["As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.","Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son","https:\/\/doi.org\/10.1016\/j.compag.2018.08.045","2018","Evaluating fidelity of lossy compression on spatiotemporal data from an IoT enabled smart farm"],["Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for water resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of hydroinformatics applications.","Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal","https:\/\/doi.org\/10.1016\/j.envsoft.2022.105396","2022","An overview of visualization and visual analytics applications in water resources management"],["In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.","Jingjing Li and Ling Tang and Shouyang Wang","https:\/\/doi.org\/10.1016\/j.physa.2020.124178","2020","Forecasting crude oil price with multilingual search engine data"],["The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.","Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski","https:\/\/doi.org\/10.1016\/j.giq.2020.101493","2020","Data governance: Organizing data for trustworthy Artificial Intelligence"],["It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers\u2019 choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution\/system\/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.","Danette McGilvray","https:\/\/doi.org\/10.1016\/B978-0-12-818015-0.00001-3","2021","Chapter 5 - Structuring Your Project"],["The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.","Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim","https:\/\/doi.org\/10.1016\/j.is.2021.101874","2022","Assessing and improving measurability of process performance indicators based on quality of logs"],["Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator \u201cThermal Anomalies\u201d, these data, were acquired from \u201cMODIS\u201d (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https:\/\/github.com\/ouladsayadyounes\/Wildfires). Experiments were made using the big data platform \u201cDatabricks\u201d. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.","Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}","https:\/\/doi.org\/10.1016\/j.firesaf.2019.01.006","2019","Predictive modeling of wildfires: A new dataset and machine learning approach"],["Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10\u00a0m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.","Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou","https:\/\/doi.org\/10.1016\/j.isprsjprs.2020.07.013","2020","Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples"],["Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS \u2013 integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.","Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts","https:\/\/doi.org\/10.1016\/j.isprsjprs.2020.02.009","2020","An integrated environmental analytics system (IDEAS) based on a DGGS"],["This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand \u201cdata as data.\u201d But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.","Laura Sebastian-Coleman","https:\/\/doi.org\/10.1016\/B978-0-12-821737-5.00004-3","2022","Chapter 4 - The Data Challenge: The Mechanics of Meaning"],["Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions.","Aiswarya Raj Munappy and Jan Bosch and Helena Holmstr\u00f6m Olsson and Anders Arpteg and Bj\u00f6rn Brinne","https:\/\/doi.org\/10.1016\/j.jss.2022.111359","2022","Data management for production quality deep learning models: Challenges and solutions"],["The complexity of building biological databases is well-known and ontologies play an extremely important role in biological databases. However, much of the emphasis on the role of ontologies in biological databases has been on the construction of databases. In this paper, we explore a somewhat overlooked aspect regarding ontologies in biological databases, namely, how ontologies can be used to assist better database retrieval. In particular, we show how ontologies can be used to revise user submitted queries for query relaxation. In addition, since our research is conducted at today's \u201cbig data\u201d era, our investigation is centered on NoSQL databases which serve as a kind of \u201crepresentatives\u201d of big data. This paper contains two major parts: First we describe our methodology of building two NoSQL application databases (MongoDB and AllegroGraph) using GO ontology, and then discuss how to achieve query relaxation through GO ontology. We report our experiments and show sample queries and results. Our research on query relaxation on NoSQL databases is complementary to existing work in big data and in biological databases and deserves further exploration.","Naresh Kumar Gundla and Zhengxin Chen","https:\/\/doi.org\/10.1016\/j.procs.2016.07.120","2016","Creating NoSQL Biological Databases with Ontologies for Query Relaxation"],["Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and\/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.","Fadi Al-Turjman","https:\/\/doi.org\/10.1016\/j.future.2019.08.009","2020","Intelligence and security in big 5G-oriented IoNT: An overview"],["Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.","Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles","https:\/\/doi.org\/10.1016\/j.imu.2019.100194","2019","An automated workflow for MALDI-ToF mass spectra pattern identification on large data sets: An application to detect aneuploidies from pregnancy urine"],["The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth\/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.","Kyoung Sook Jeong and Suejin Kim and Woo\u00a0Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha","https:\/\/doi.org\/10.1016\/j.envres.2018.12.009","2019","Cohort profile: Beyond birth cohort study \u2013 The Korean CHildren's ENvironmental health Study (Ko-CHENS)"],["Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.","Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba","https:\/\/doi.org\/10.1016\/j.csbj.2020.06.031","2020","Assessment of vector-host-pathogen relationships using data mining and machine learning"],["In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.","Serkan Ayvaz and Koray Alpay","https:\/\/doi.org\/10.1016\/j.eswa.2021.114598","2021","Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time"],["Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.","Humberto Ferreira and Pedro Ruivo and Carolina Reis","https:\/\/doi.org\/10.1016\/j.procs.2021.01.228","2021","How do data scientists and managers influence machine learning value creation?"],["Scientific research is one area of applications and usage of big data where we can generate lots of data in a single experiment and perform complex analytics on the same in the outcome of that experiment. The most famous example that we can talk about is the usage of all infrastructure technologies in the discovery of the \u201cGod particle\u201d or \u201cHiggs boson particle\u201d which is leading us to uncover more exploration around the universe.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-815746-6.00004-1","2020","4 - Scientific research applications and usage"],["The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company\u2019s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization\u2019s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.","Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede}","https:\/\/doi.org\/10.1016\/j.ssci.2019.04.045","2019","A systematic approach for discovering causal dependencies between observations and incidents in the health and safety domain"],["For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.","Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar","https:\/\/doi.org\/10.1016\/j.compind.2022.103666","2022","Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions"],["In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.","Michal Moran and Goren Gordon","https:\/\/doi.org\/10.1016\/j.ins.2019.02.009","2019","Curious Feature Selection"],["Abstract:\nDecision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) \u201cMaintenance Descriptive Analytics (monitoring)\u201d; 2) \u201cMaintenance Diagnostic Analytics\u201d; 3) \u201cMaintenance Predictive Analytics\u201d; and 4) \u201cMaintenance Prescriptive analytics\u201d.","Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar","https:\/\/doi.org\/10.1016\/j.ifacol.2016.11.037","2016","Maintenance Analytics \u2013 The New Know in Maintenance"],["Abstract:\nAs Lucy Suchmann observed in 2011, through L\u00e9vi-Strauss, \u201c\u2026we are our tools\u2026\u201d and our personal health data are an integral part of us. In these circumstances, it becomes necessary to question the value of Big Data in the health sphere.","J\u00e9r\u00f4me B\u00e9ranger","https:\/\/doi.org\/10.1016\/B978-1-78548-025-6.50002-6","2016","2 - Ethical Development of the Medical Datasphere"],["Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.","Mouzhi Ge and W\u0142odzimierz Lewoniewski","https:\/\/doi.org\/10.1016\/j.procs.2020.09.228","2020","Developing the Quality Model for Collaborative Open Data"],["Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.","Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang","https:\/\/doi.org\/10.1016\/j.tmp.2021.100860","2021","Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers"],["There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.","Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato","https:\/\/doi.org\/10.1016\/j.jnca.2021.103107","2021","A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0"],["The Semantic Web technologies are being increasingly used for exploiting relations between data. In addition, new tendencies of real-time systems, such as social networks, sensors, cameras or weather information, are continuously generating data. This implies that data and links between them are becoming extremely vast. Such huge quantity of data needs to be analyzed, processed, as well as stored if necessary. In this position paper, we will introduce recent work on Real-Time Business Intelligence combined with semantic data stream management. We will present underlying approaches such as continuous queries, data summarization and matching, and stream reasoning.","Marie-Aude Aufaure and Raja Chiky and Olivier Cur\u00e9 and Houda Khrouf and Gabriel Kepeklian","https:\/\/doi.org\/10.1016\/j.future.2015.11.015","2016","From Business Intelligence to semantic data stream management"],["Recently, with the development of \u201cIndustry 4.0\u201d, \u201cOil and Gas 4.0\u201d has also been put on the agenda in the past two years. Some companies and experts believe that \u201cOil and Gas 4.0\u201d can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the \u201cOil and Gas 4.0\u201d is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of \u201cOil and Gas 4.0\u201d, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of \u201cOil and Gas 4.0\u201d is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the \u201cOil and Gas 4.0\u201d era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the \u201cOil and Gas 4.0\u201d era. Finally, relevant policy recommendations are proposed.","Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang","https:\/\/doi.org\/10.1016\/j.compind.2019.06.007","2019","Oil and Gas 4.0 era: A systematic review and outlook"],["Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.","Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam","https:\/\/doi.org\/10.1016\/j.ipm.2021.102736","2022","IoT data visualization for business intelligence in corporate finance"],["In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.","Xun Wang and Yahan Yang and Yuxuan Wu and Wenbin Wei and Li Dong and Yang Li and Xingping Tan and Hankun Cao and Hong Zhang and Xiaodan Ma and Qin Jiang and Yunfan Zhou and Weihua Yang and Chaoyu Li and Yu Gu and Lin Ding and Yanli Qin and Qi Chen and Lili Li and Mingyue Lian and Jin Ma and Dongmei Cui and Yuanzhou Huang and Wenyan Liu and Xiao Yang and Shuiming Yu and Jingjing Chen and Dongni Wang and Zhenzhe Lin and Pisong Yan and Haotian Lin","https:\/\/doi.org\/10.1016\/j.imed.2021.05.001","2021","The national multi-center artificial intelligent myopia prevention and control project"],["Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.","Kerem Kayabay and Mert Onuralp G\u00f6kalp and Ebru G\u00f6kalp and P. {Erhan Eren} and Altan Ko\u00e7yi\u011fit","https:\/\/doi.org\/10.1016\/j.techfore.2021.121264","2022","Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization"],["In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.","Junfeng Wang","https:\/\/doi.org\/10.1016\/B978-0-12-813304-0.00008-6","2018","Chapter 8 - Real-Time Monitoring and Early Warning of a Train\u2019s Running State and Operation Behavior"],["The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.","Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan","https:\/\/doi.org\/10.1016\/j.inffus.2019.05.004","2019","A survey of data fusion in smart city applications"],["Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.","Hermanus J Smidt and Osden Jokonya","https:\/\/doi.org\/10.1016\/j.procs.2021.01.281","2021","The challenge of privacy and security when using technology to track people in times of COVID-19 pandemic"],["Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.","Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jes\u00fas Peral","https:\/\/doi.org\/10.1016\/j.csi.2019.103378","2020","Adding value to Linked Open Data using a multidimensional model approach based on the RDF Data Cube vocabulary"],["The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization\u2019s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.","J.R. L\u00f3pez-Robles and J.R. Otegi-Olaso and I. {Porto G\u00f3mez} and M.J. Cobo","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.01.013","2019","30 years of intelligence models in management and business: A bibliometric review"],["The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50\u202fm above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7\u20132.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10\u202fm AGL). We show that pollen concentrations measured at >10\u202fm are representative for background near-ground levels.","Jes\u00fas Rojo and Jose Oteros and Rosa P\u00e9rez-Badia and Patricia Cervig\u00f3n and Zuzana Ferencova and A. Monserrat Guti\u00e9rrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodr\u00edguez-De la Cruz} and Estefan\u00eda S\u00e1nchez-Reyes and Jos\u00e9 S\u00e1nchez-S\u00e1nchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calder\u00f3n and C\u00e9sar Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fern\u00e1ndez-Gonz\u00e1lez and Jos\u00e9 Garc\u00eda and M. Mar Trigo and Dorota Myszkowska and Santiago Fern\u00e1ndez-Rodr\u00edguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Don\u00e1t Magyar and L\u00e1szl\u00f3 Makra and Orsolya Udvardy and Purificaci\u00f3n Alc\u00e1zar and Carmen Gal\u00e1n and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skj\u00f8th and Predrag Radi\u0161i\u0107 and Branko \u0160ikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters","https:\/\/doi.org\/10.1016\/j.envres.2019.04.027","2019","Near-ground effect of height on pollen exposure"],["Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.","Deanne Larson and Victor Chang","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.04.013","2016","A review and future direction of agile, business intelligence, analytics and data science"],["Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.","Evelyn Hovenga and Heather Grain","https:\/\/doi.org\/10.1016\/B978-0-12-823413-6.00013-6","2022","Chapter 9 - Quality data, design, implementation, and governance"],["The smart city vision was born by the integration of ICT in the day to day city management operations and citizens lives, owing to the need for novel and smart ways to manage the cities resources; making them more efficient, sustainable and transparent. However, the understanding of the crucial elements to this integration and how they can benefit from each other proves difficult and unclear. In this article, we investigate the intricate synergies between different technologies and paradigms involved in the smart city vision, to help design a robust framework, capable of handling the challenges impeding its successful implementation. To this end, we propose a context-aware centered approach to present a holistic view of a smart city as viewed from the different angles (Cloud, IoT, Big Data). We also propose a framework encompassing elements from the different enablers, leveraging their strengths to build and develop smart-x applications and services.","Soufiane Faieq and Rajaa Saidi and Hamid Elghazi and Moulay Driss Rahmani","https:\/\/doi.org\/10.1016\/j.procs.2017.06.072","2017","C2IoT: A framework for Cloud-based Context-aware Internet of Things services for smart cities"],["The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.","Igor \u0160vab","https:\/\/doi.org\/10.1016\/B978-0-12-801238-3.11590-9","2021","Complexity of Patient Data in Primary Care Practice"],["The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.","Richard Vidgen and Sarah Shaw and David B. Grant","https:\/\/doi.org\/10.1016\/j.ejor.2017.02.023","2017","Management challenges in creating value from business analytics"],["Building the big data application is very interesting and can provide multiple users with multiple perspectives, data discovery to end state analytics and beyond is very much what everybody wants to achieve. Enterprises are ready to spend millions of dollars to get a share of your wallet, they want to be a part of your life and be present at every event that gets your attention. They want to leverage their partnerships and influence you, how do they make this all happen? The most successful companies will tell you their story is built on governance. The aspect of governance is very critical to the success of this journey whether internal or external. What governance are we talking about? How do we implement the same? This chapter will focus on those aspects.","Krish Krishnan","https:\/\/doi.org\/10.1016\/B978-0-12-815746-6.00009-0","2020","9 - Governance"],["In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.","Gregory Vial","https:\/\/doi.org\/10.1016\/j.dss.2019.113133","2019","Reflections on quality requirements for digital trace data in IS research"],["This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.","Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-Mar\u00eda Mora-Valent\u00edn","https:\/\/doi.org\/10.1016\/j.giq.2018.10.008","2019","Knowledge areas, themes and future research on open data: A co-word analysis"],["High quality, reliable data and robust models are central to the development and appraisal of transportation planning and policy. Although conventional data may offer good \u2018content\u2019, it is widely observed that it lacks context i.e. who and why people are travelling. Transportation modelling has developed within these boundaries, with implications for the planning, design and management of transportation systems and policy-making. This paper establishes the potential of passively collected GPS-based \u201cTrack & Trace\u201d (T&T) datasets of individual mobility profiles towards enhancing transportation modelling and policy-making. T&T is a type of New and Emerging Data Form (NEDF), lying within the broader \u2018Big Data\u2019 paradigm, and is typically collected using mobile phone sensors and related technologies. These capture highly grained mobility content and can be linked to the phone owner\/user behavioural choices and other individual context. Our meta-analysis of existing literature related to spatio-temporal mobile phone data demonstrates that NEDF\u2019s, and in particular T&T data, have had little mention to date within an applied transportation planning and policy context. We thus establish there is an opportunity for policy-makers, transportation modellers, researchers and a wide range of stakeholders to collaborate in developing new analytic approaches, revise existing models and build the skills and related capacity needed to lever greatest value from the data, as well as to adopt new business models that could revolutionise citizen participation in policy-making. This is of particular importance due to the growing awareness in many countries for a need to develop and monitor efficient cross-sectoral policies to deliver sustainable communities.","Gillian Harrison and Susan M. Grant-Muller and Frances C. Hodgson","https:\/\/doi.org\/10.1016\/j.trc.2020.102672","2020","New and emerging data forms in transportation planning and policy: Opportunities and challenges for \u201cTrack and Trace\u201d data"],["Process safety is playing an important role with the rapid development of industry. With the advent of the Big Data era, various and massive data from the Internet of Things can be used for process safety. In this chapter, we aim to provide the reader with a comprehensive understanding of rapidly growing data-driven process safety approaches in the chemical industry. Data-driven approaches primarily use past process data without a complex mechanism model of chemical properties or processes; hence, they have advantages in practical industrial applications. In this chapter, first, we describe the importance of data in process safety. Then, we briefly introduce the ideas and methods of data pre-processing. We follow this with a discussion on statistical-based and artificial intelligence-based data-driven approaches. Then, we elaborate on the application of data-driven methods in the field of chemical process safety. Finally, we provide a summary and outlook for advancing data-driven methods.","Yiming Bai and Shuaiyu Xiang and Zeheng Zhao and Borui Yang and Jinsong Zhao","https:\/\/doi.org\/10.1016\/bs.mcps.2022.04.002","2022","Chapter Three - Data-driven approaches: Use of digitized operational data in process safety"],["Life cycle assessment (LCA) is widely used for assessing the environmental impacts of a product or service. Collecting reliable data is a major challenge in LCA due to the complexities involved in the tracking and quantifying inputs and outputs at multiple supply chain stages. Blockchain technology offers an ideal solution to overcome the challenge in sustainable supply chain management. Its use in combination with internet-of-things (IoT) and big data analytics and visualization can help organizations achieve operational excellence in conducting LCA for improving supply chain sustainability. This research develops a framework to guide the implementation of Blockchain-based LCA. It proposes a system architecture that integrates the use of Blockchain, IoT, and big data analytics and visualization. The proposed implementation framework and system architecture were validated by practitioners who were experienced with Blockchain applications. The research also analyzes system implementation costs and discusses potential issues and solutions, as well as managerial and policy implications.","Abraham Zhang and Ray Y Zhong and Muhammad Farooque and Kai Kang and V G Venkatesh","https:\/\/doi.org\/10.1016\/j.resconrec.2019.104512","2020","Blockchain-based life cycle assessment: An implementation framework and system architecture"],["The year 2017 has seen many EU and UK legislative initiatives and proposals to consider and address the impact of artificial intelligence on society, covering questions of liability, legal personality and other ethical and legal issues, including in the context of data processing. In March 2017, the Information Commissioner's Office (UK) updated its big data guidance to address the development of artificial intelligence and machine learning, and to provide (GDPR), which will apply from 25 May 2018. This paper situates the ICO's guidance in the context of wider legal and ethical considerations and provides a critique of the position adopted by the ICO. On the ICO's analysis, the key challenge for artificial intelligence processing personal data is in establishing that such processing is fair. This shift reflects the potential for artificial intelligence to have negative social consequences (whether intended or unintended) that are not otherwise addressed by the GDPR. The question of \u2018fairness\u2019 is an important one, to address the imbalance between big data organisations and individual data subjects, with a number of ethical and social impacts that need to be evaluated.","Michael Butterworth","https:\/\/doi.org\/10.1016\/j.clsr.2018.01.004","2018","The ICO and artificial intelligence: The role of fairness in the GDPR framework"],["The utilisation of Artificial Intelligence (AI) applications in the energy sector is gaining momentum, with increasingly intensive search for suitable, high-quality and trustworthy solutions that displayed promising results in research. The growing interest comes from decision makers of both the industry and policy domains, searching for applications to increase companies\u2019 profitability, raise efficiency and facilitate the energy transition. This paper aims to provide a novel three-dimensional (3D) indicator for AI applications in the energy sector, based on their respective maturity level, regulatory risks and potential benefits. Case studies are used to exemplify the application of the 3D indicator, showcasing how the developed framework can be used to filter promising AI applications eligible for governmental funding or business development. In addition, the 3D indicator is used to rank AI applications considering different stakeholder preferences (risk-avoidance, profit-seeking, balanced). These results allow AI applications to be better categorised in the face of rapidly emerging national and intergovernmental AI strategies and regulations that constrain the use of AI applications in critical infrastructures.","Hugo Quest and Marine Cauz and Fabian Heymann and Christian Rod and Lionel Perret and Christophe Ballif and Alessandro Virtuani and Nicolas Wyrsch","https:\/\/doi.org\/10.1016\/j.egyai.2022.100167","2022","A 3D indicator for guiding AI applications in the energy sector"],["Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.","Carlos A. Escobar and Debejyo Chakraborty and Megan McGovern and Daniela Macias and Ruben Morales-Menendez","https:\/\/doi.org\/10.1016\/j.promfg.2021.06.085","2021","Quality 4.0 \u2014 Green, Black and Master Black Belt Curricula"],["From very beginning, research and practice of database management systems (DBMSs) have been cantered on handling granulation and granularities at various levels, thus sharing common interests with granular computing (GrC). Although DBMS and GrC have different focuses, the advent of Big Data has brought these two research areas closer to each other, because Big Data requires integrated study of data storage and analysis. In this paper, we explore this issue. Starting with an examination of granularities from a database perspective, we discuss new challenges of Big Data. We then turn to data management issues related to GrC. As an example of possible cross-fertilization of these two fields, we examine the recent development of database keyword search (DBKWS). Even research in DBKWS is largely independent to GrC, DBKWS has to handle various issues related to granularity handling. In particular, aggregation of DBKWS results is closely related to studies in granularities and granulation, which echoes L. Zadeh's famous formula: Granulation = Summarization. We present our proposed approach, termed as extended keyword search, which illustrates that an integrated study of data management and data mining\/analysis is not restricted to GrC or rough set theory","Zhengxin Chen","https:\/\/doi.org\/10.1016\/j.procs.2015.07.117","2015","Towards Integrated Study of Data Management and Data Mining"],["In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.","Francisco J. Mont\u00e1ns and Francisco Chinesta and Rafael G\u00f3mez-Bombarelli and J. Nathan Kutz","https:\/\/doi.org\/10.1016\/j.crme.2019.11.009","2019","Data-driven modeling and learning in science and engineering"],["Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.","Alexander G. Rumson and Stephen H. Hallett","https:\/\/doi.org\/10.1016\/j.scitotenv.2019.01.114","2019","Innovations in the use of data facilitating insurance as a resilience mechanism for coastal flood risk"],["The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.","Manmeet Mahinderjit Singh and Anizah Abu Bakar","https:\/\/doi.org\/10.1016\/j.procs.2019.11.227","2019","A Systemic Cybercrime Stakeholders Architectural Model"],["Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as \u201cMIDR-Angelization\u201d to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.","Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq","https:\/\/doi.org\/10.1016\/j.scs.2018.04.014","2018","Privacy preserving data by conceptualizing smart cities using MIDR-Angelization"],["Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n\u202f=\u202f106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40\u202fL\/100\u202fkm (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.","Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang","https:\/\/doi.org\/10.1016\/j.energy.2019.116388","2020","Impact factors of the real-world fuel consumption rate of light duty vehicles in China"],["Data warehouses are constantly evolving to support new technologies and business requirements\u2014and remain relevant when it comes to big data and analytics. Regardless of how new or sophisticated your data warehouse is, it likely needs modernization. Data warehousing, along with data modeling, and side by side with data analytic capability gives us the upper hand with our knowledge by collecting the right information at the right time with the right data coming from all directions, whether or not these data are structured or unstructured. We should be able to have proper tools in hand to be able to take this information and knowledge to be in a position of resilience based on predictive analysis driven by data. This chapter will discuss data warehousing, data modeling, and consequently, data analytics where, in combination, they all are variables functioning within the process of predictive analytic modeling. This process allows us to have the knowledge we are looking for. Getting reliable information from data warehouses is resource-intensive; missing one step can result in wasted processing time and\/or bad data.","Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia","https:\/\/doi.org\/10.1016\/B978-0-323-95112-8.00001-5","2022","Chapter 3 - Data warehousing, data mining, data modeling, and data analytics"],["With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.","Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam","https:\/\/doi.org\/10.1016\/bs.adcom.2021.09.006","2022","Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications"],["In this chapter we look at the technology and tooling available for data management. We start with a discussion on the different components of an IT infrastructure and how to describe them. We provide a short taxonomy of tools from analytics and data distribution to data governance and end-user tools. This is followed by a discussion on data models and storage models, from traditional relational databases to different challenger technologies including NoSQL databases. We discuss data quality management and curation processes and data storage at different scales from data marts and warehouses to data lakes. We then discuss data analytics and big data technologies and what some of the use cases and implications for financial services firms are. After discussing privacy and security aspects, and the promising application areas of blockchain technology in master data, we discuss cloud storage models and what the cloud trend means for banks and asset managers. We end with a discussion on IT sourcing options, IT management, and IT maturity models before concluding with a look ahead.","Martijn Groot","https:\/\/doi.org\/10.1016\/B978-0-12-809776-2.00005-3","2017","Chapter 5 - Data Management Tools and Techniques"],["In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component\u2019s lifetime.","Christian Dalheim \u00d8ien and Sebastian Dransfeld","https:\/\/doi.org\/10.1016\/j.procir.2021.11.224","2021","An approach to data structuring and predictive analysis in discrete manufacturing"],["Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers\u2019 fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.","Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo","https:\/\/doi.org\/10.1016\/j.comcom.2021.09.026","2021","Towards high quality mobile crowdsensing: Incentive mechanism design based on fine-grained ability reputation"],["Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil\/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil\/gas industry professionals.","Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed","https:\/\/doi.org\/10.1016\/j.petrol.2020.107627","2021","Automated pressure transient analysis: A cloud-based approach"],["Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate \u201cheterogeneous treatment effects.\u201d We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.","Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene","https:\/\/doi.org\/10.1016\/j.leaqua.2020.101426","2020","Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science"],["In the era of Big Data, data assets have become a strategic resource which cannot be overlooked by both society and enterprises. However, data is not equal to the assets. This paper first introduces the necessary conditions of data assetization and discriminates the concepts of data governance, data management and data asset management. Then it focuses on the unique connotation and characteristics of grid data assets. With reference to the mainstream theory of data management, the framework for the grid data asset management is set up in the combination of the characteristics of data assets, business needs and the actual situation in the power supply enterprises. Finally, this paper puts forward higher system requirements and technical requirements for China\u2019s power supply enterprises to conduct data asset management.","Jun Wang and Yun-si Li and Wei Song and Ai-hua Li","https:\/\/doi.org\/10.1016\/j.procs.2018.10.258","2018","Research on the Theory and Method of Grid Data Asset Management"],["With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the Coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.","Ming Zhang and Danyun Dai and Siliang Hou and Wei Liu and Feng Gao and Dong Xu and Yu Hu","https:\/\/doi.org\/10.1016\/j.imed.2021.03.004","2021","Thinking on the informatization development of China's healthcare system in the post-COVID-19 era"],["The aerospace sector is one of the many sectors in which large amounts of data are generated. Thanks to the evolution of technology, these data can be exploited in several ways to improve the operation and management of industrial processes. However, to achieve this goal, it is necessary to define architectures and data models that allow to manage and homogenise the heterogeneous data collected. In this paper, we present an Airport Digital Twin Reference Conceptualisation\u2019s and data model based on FIWARE Generic Enablers and the Next Generation Service Interfaces-Linked Data standard. Concretely, we particularise the Airport Digital Twin to improve the efficiency of flight turnaround events. The architecture proposed is validated in the Aberdeen International Airport with the aim of reducing delays in commercial flights. The implementation includes an application that shows the real state of the airport, combining two-dimensional and three-dimensional virtual reality representations of the stands, and a mobile application that helps ground operators to schedule departure and arrival flights.","Javier Conde and Andres Munoz-Arcentales and Mario Romero and Javier Rojo and Joaqu\u00edn Salvach\u00faa and Gabriel Huecas and \u00c1lvaro Alonso","https:\/\/doi.org\/10.1016\/j.aei.2022.101723","2022","Applying digital twins for the management of information in turnaround event operations in commercial airports"],["We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.","Christina G. Skarpathiotaki and Konstantinos E. Psannis","https:\/\/doi.org\/10.1016\/j.bdr.2021.100274","2022","Cross-Industry Process Standardization for Text Analytics"],["At the time of writing, the world is facing the new coronavirus pandemic, which has been declared one of the most dangerous disasters of the 21st century. All nations and communities have applied many countermeasures to control the spread of the epidemic. In terms of countermeasures, lockdowns and reductions of social activities are meant to flatten the curve of infection. Nevertheless, to date, there has been no evaluation of the effectiveness of these methods. Thus, the present study aims to interpret the change in the population density of Sapporo city in the emergency's period declaration using big data obtained from mobile spatial statistics. The results indicate that, in the time of refraining from traveling, the city's residents have been more likely to stay home and less likely to travel to the center area. This has led to a decrease of up to 90% of the population density in crowded areas. The study's outcomes partly explain the statement of reducing 70%\u201380% of contact between people in line with the purpose of the emergency declaration. Moreover, these findings establish the primary step for further analysis of estimating the efficiency of policy in controlling the epidemic.","Mikiharu Arimura and Tran Vinh Ha and Kota Okumura and Takumi Asada","https:\/\/doi.org\/10.1016\/j.trip.2020.100212","2020","Changes in urban mobility in Sapporo city, Japan due to the Covid-19 emergency declarations"],["In recent years, many governments have launched various smart city or smart infrastructure initiatives to improve the quality of citizens\u2019 life and help city managers \/ planners optimize the operation and management of urban infrastructures. By deploying internet of things (IoT) to infrastructure systems, high-volume and high-variety of data pertinent to the condition and performance of infrastructure systems along with the behaviors of citizens can be gathered, processed, integrated and analyzed through cloud-based infrastructure asset management systems, ubiquitous mobile applications and big data analytics platforms. Nonetheless, how to fully exploit the value of \u2018big infrastructure data\u2019 is still a key challenge facing most stakeholders. Unless data is shared by different infrastructure systems in an interoperable and consistent manner, it is difficult to realize the smart infrastructure concept for efficient smart city planning, not to mention about developing appropriate resilience and sustainable programs. To unlock the value of big infrastructure data for smart, sustainable and resilient city planning, a master data management (MDM) solution is proposed in this paper. MDM has been adopted in the business sector to orchestrate operational and analytical big data applications. In order to derive a suitable MDM solution for smart, sustainable and resilient city planning, commercial and open source MDM systems, smart city standards, smart city concept models, smart community infrastructure frameworks, semantic web technologies will be critically reviewed, and feedback and requirements will be gathered from experts who are responsible for developing smart, sustainable and resilient city programs. A case study which focuses on the building and transportation infrastructures of a selected community in Hong Kong will be conducted to pilot the proposed MDM solution.","S. Thomas Ng and Frank J. Xu and Yifan Yang and Mengxue Lu","https:\/\/doi.org\/10.1016\/j.proeng.2017.08.034","2017","A Master Data Management Solution to Unlock the Value of Big Infrastructure Data for Smart, Sustainable and Resilient City Planning"],["Flood susceptibility assessment for identifying flood-prone areas plays a significant role in flood hazard mitigation. Machine learning is an optional assessment method because of its high objectivity and computational efficiency, but how to get enough and accurate information of historical flood locations to train the machine learning models has been a key problem. In recent years, news media data from both news websites and social media accounts has emerged as a promising source for natural science studies. However, the application of news media data in urban flood susceptibility assessment is still inadequate. This study proposed an approach to fill this gap. Firstly, flood locations were extracted from news media data based on a named entity recognition (NER) model. Then, a frequency or distance-based data quality control method was employed to improve the representativeness of the extracted flooded locations. Finally, flood conditioning factors with information of historical flood locations were input into a Support Vector Machine (SVM) model for flood susceptibility assessment. We took the central city of Dalian, China as a case study. The T-test results show that there was no significant difference between the distributions of most flood conditioning factors at the flood locations from the news media data and the official planning report. In the obtained flood susceptibility map, the high flood susceptibility areas got a recall of 90% compared with the high flood hazard areas in the planning report. Performing data quality control in the frequency-based method can improve the precision of the flood susceptibility map by up to 5%, while the distance-based method is ineffective. This study provides an example and offers the value of applying new data sources and modern deep learning techniques for urban flood management.","Shengnan Fu and Heng Lyu and Ze Wang and Xin Hao and Chi Zhang","https:\/\/doi.org\/10.1016\/j.jhydrol.2022.128312","2022","Extracting historical flood locations from news media data by the named entity recognition (NER) model to assess urban flood susceptibility"],["The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author\u2019s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.","Lucy Fortson","https:\/\/doi.org\/10.1016\/B978-0-12-817390-9.00009-9","2021","Chapter 10 - From Green Peas to STEVE: Citizen Science Engagement in Space Science"],["Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800\u20132013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.","Demetris T. Christopoulos","https:\/\/doi.org\/10.1016\/j.jastp.2015.03.009","2015","Extraction of the global absolute temperature for Northern Hemisphere using a set of 6190 meteorological stations from 1800 to 2013"],["Cloud computing and the mobile Internet have been the two most influential information technology revolutions, which intersect in mobile cloud computing (MCC). The burgeoning MCC enables the large-scale collection and processing of big data, which demand trusted, authentic, and accurate data to ensure an important but often overlooked aspect of big data \u2014 data veracity. Troublesome internal attacks launched by internal malicious users is one key problem that reduces data veracity and remains difficult to handle. To enhance data veracity and thus improve the performance of big data computing in MCC, this paper proposes a Data Trustworthiness enhanced Reputation Mechanism (DTRM) which can be used to defend against internal attacks. In the DTRM, the sensitivity-level based data category, Metagraph theory based user group division, and reputation transferring methods are integrated into the reputation query and evaluation process. The extensive simulation results based on real datasets show that the DTRM outperforms existing classic reputation mechanisms under bad mouthing attacks and mobile attacks.","Hui Lin and Jia Hu and Chuanfeng Xu and Jianfeng Ma and Mengyang Yu","https:\/\/doi.org\/10.1016\/j.future.2018.01.026","2018","DTRM: A new reputation mechanism to enhance data trustworthiness for high-performance cloud computing"],["Abstract:\nMarine engine operating regions under principal component analysis (PCA) to evaluate ship performance and navigation behavior are presented in this study. A data set with ship performance and navigation information (i.e. a selected vessel) is considered to identify its hidden structure with respect to a selected operating region of the marine engine. Firstly, the data set is classified with respect to the engine operating points (i.e. operating modes), identifying three operating regions for the main engine. Secondly, one engine operating region (i.e. a data cluster) is analyzed to calculate the respective principal components (PCs). These PCs represent various relationships among ship performance and navigation parameters of the vessel and those relationships with respect to the marine engine operating region are used to evaluate ship performance and navigation behavior. Furthermore, such knowledge (i.e. PCs and parameter behavior) can also be used for sensor fault identification and data compression\/expansion types of applications as a big data solution in shipping.","Lokukaluge P. Perera and Brage Mo","https:\/\/doi.org\/10.1016\/j.ifacol.2016.10.487","2016","Marine Engine Operating Regions under Principal Component Analysis to evaluate Ship Performance and Navigation Behavior"],["Background\nIn the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.\nMethod\nIn this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.\nResults\nTo test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.\nConclusions\nThe obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.","Sarwar Kamal and Shamim Hasnat Ripon and Nilanjan Dey and Amira S. Ashour and V. Santhi","https:\/\/doi.org\/10.1016\/j.cmpb.2016.04.005","2016","A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset"],["This work assesses the quality of Internet of Things data not only as an intrinsic quality on how well it represents the related phenomenon but also, on how much information it contains to educate an artificial entity. The quality metrics here proposed are tested with real datasets. Also, they are implemented on OpenCPU, so the open data repositories can use them off-the-shelf to rate their datasets without computational cost and minimum human intervention, making them more attractive to potential users and gaining visibility and impact.","Aurora Gonz\u00e1lez-Vidal and Alfonso P. Ramallo-Gonz\u00e1lez and Antonio F. Skarmeta","https:\/\/doi.org\/10.1016\/j.icte.2022.06.001","2022","Intrinsic and extrinsic quality of data for open data repositories"],["The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.","Juan Yebenes and Marta Zorrilla","https:\/\/doi.org\/10.1016\/j.procs.2019.04.082","2019","Towards a Data Governance Framework for Third Generation Platforms"],["Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.","James C.L. Chow","https:\/\/doi.org\/10.1016\/j.rpor.2017.08.005","2017","Internet-based computer technology on radiotherapy"],["This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.","Feng Ming Tsai and Tat-Dat Bui and Ming-Lang Tseng and Mohd Helmi Ali and Ming K. Lim and Anthony SF Chiu","https:\/\/doi.org\/10.1016\/j.resconrec.2021.105421","2021","Sustainable supply chain management trends in world regions: A data-driven analysis"],["This paper investigates the usability of Future Internet technologies (aka \u201cGeneric Enablers of the Future Internet\u201d) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the \u201cEnvironmental Observation Web\u201d as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, \u201cthe cloud\u201d can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.","Carlos Granell and Denis Havlik and Sven Schade and Zoheir Sabeur and Conor Delaney and Jasmin Pielorz and Thomas Usl\u00e4nder and Paolo Mazzetti and Katharina Schleidt and Mike Kobernus and Fuada Havlik and Nils Rune Bodsberg and Arne Berre and Jose Lorenzo Mon","https:\/\/doi.org\/10.1016\/j.envsoft.2015.12.015","2016","Future Internet technologies for environmental applications"],["Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.","Suchitra Kataria and Vinod Ravindran","https:\/\/doi.org\/10.1016\/j.berh.2019.101429","2019","Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases"],["This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.","Mohammad I. Merhi","https:\/\/doi.org\/10.1016\/j.techfore.2021.121180","2021","Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process"],["Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.","Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger","https:\/\/doi.org\/10.1016\/j.geoforum.2016.07.019","2016","Evidence and future potential of mobile phone data for disease disaster management"],["The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on \u201cinformation quality\u201d and \u201cdata quality\u201d identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.","Michael Riesener and Christian D\u00f6lle and G\u00fcnther Schuh and Christian T\u00f6nnes","https:\/\/doi.org\/10.1016\/j.procir.2019.03.131","2019","Framework for defining information quality based on data attributes within the digital shadow using LDA"],["Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.","Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath","https:\/\/doi.org\/10.1016\/j.watres.2019.03.030","2019","Data-driven performance analyses of wastewater treatment plants: A review"],["While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.","Diana Moise and Denis Shestakov","https:\/\/doi.org\/10.1016\/B978-0-444-63492-4.00012-5","2015","Chapter 12 - Terabyte-Scale Image Similarity Search"],["The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.","Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi","https:\/\/doi.org\/10.1016\/j.jbusres.2022.01.039","2022","Social media use: A review of innovation management practices"],["The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented.","Marilex Rea Llave","https:\/\/doi.org\/10.1016\/j.procs.2018.10.071","2018","Data lakes in business intelligence: reporting from the trenches"],["The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.","Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag \u00d8ivind Madsen","https:\/\/doi.org\/10.1016\/j.techfore.2021.121111","2021","The science of statistics versus data science: What is the future?"],["Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.","Ricardo Matheus and Marijn Janssen and Devender Maheshwari","https:\/\/doi.org\/10.1016\/j.giq.2018.01.006","2020","Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities"],["This chapter presents the origin of medical data that comprises the core of any federated cloud platform that deals with medical data sharing and analytics. The different types and sources of medical data are extensively described along with applications and standard data acquisition protocols. Emphasis is given on the definition and the impact of the cohorts in clinical research, as well as the importance of the big data in medicine. The impact of the big data in medicine is also discussed along with emerging opportunities and challenges. The need to develop data standardization protocols across heterogeneous and dispersed data sources is finally highlighted, to enable the analysis of different types of medical big data.","Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis","https:\/\/doi.org\/10.1016\/B978-0-12-816507-2.00002-5","2020","Chapter 2 - Types and sources of medical and other related data"],["The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.","Raoudha Gaha and Alexandre Durupt and Benoit Eynard","https:\/\/doi.org\/10.1016\/j.promfg.2021.07.033","2021","Towards the implementation of the Digital Twin in CMM inspection process: opportunities, challenges and proposals"],["The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.","Tanveer Ahmad and Rafal Madonski and Dongdong Zhang and Chao Huang and Asad Mujeeb","https:\/\/doi.org\/10.1016\/j.rser.2022.112128","2022","Data-driven probabilistic machine learning in sustainable smart energy\/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm"],["Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution\u2013business value\u2013of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the \u201c3As Data Quality-in-Use model\u201d, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.","Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini","https:\/\/doi.org\/10.1016\/j.future.2015.11.024","2016","A Data Quality in Use model for Big Data"],[null,"Xintao Liu and Min Chen and Christophe Claramunt and Michael Batty and Mei-Po Kwan and Ahmad M. Senousi and Tao Cheng and Josef Strobl and Arzu C\u00f6ltekin and John Wilson and Temenoujka Bandrova and Milan Konecny and Paul M. Torrens and Fengyuan Zhang and Li He and Jinfeng Wang and Carlo Ratti and Olaf Kolditz and Alexander Klippel and Songnian Li and Hui Lin and Guonian L\u00fc","https:\/\/doi.org\/10.1016\/j.xinn.2022.100279","2022","Geographic information science in the era of geospatial big data: A\u00a0cyberspace perspective"],["As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.","Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov","https:\/\/doi.org\/10.1016\/j.ifacol.2020.12.103","2020","Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0"],["Background\nThe need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.\nMethods\nElectronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006\u20132016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.\nResults\nData were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34\/1000. Statistical differences (P\u00a0<\u00a00.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017\u20132018, in which 10 (3.5%) tables were corrected.\nConclusion\nSignificant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.","Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang","https:\/\/doi.org\/10.1016\/j.jfma.2021.12.024","2022","Quality assurance of integrative big data for medical research within a multihospital system"],["Big data is transforming biomedical research by integrating massive amounts of data from laboratory experiments, clinical investigations, healthcare records, and the internet of things. Specifically, the increasing rate at which information is obtained from omics technologies (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and pharmacogenomics) is providing an opportunity for future advances in personalized medicine that are paving the way to improved patient care. The recent advances in omics technologies are profoundly contributing to big data in biomedicine and are anticipated to aid in disease diagnosis and patient care management. Herein, we critically review the major computational techniques, algorithms, and their outcomes that have contributed to recent advances in big data generated from biomedical research in various complex human diseases, such as cancer and infectious diseases. Finally, we discuss trends in the field and the future directions that must be considered to advance the influence of big data on biomedical research and its translation in the healthcare industry.","Conor John Cremin and Sabyasachi Dash and Xiaofeng Huang","https:\/\/doi.org\/10.1016\/j.crbiot.2022.02.004","2022","Big data: Historic advances and emerging trends in biomedical research"],["ABSTRACT\nThe field of vascular surgery is in constant evolution. Administrative data and registries can provide important contemporary evidence to inform clinical decision making and the delivery of health services. The following review outlines some important considerations for retrospective studies using administrative health databases and registries. First, these data sources have advantages (e.g. real-world applicability, timely data access, relatively lower research cost) and disadvantages (e.g. potential missing data, selection bias, confounding bias) that may be more or less relevant to different administrative databases or registries. Second, we discuss a framework to guide data source selection and provide a summary of frequently used data sources in vascular surgery research. Third, a retrospective study design warrants pre-planned exposure, outcome and covariate definitions and, when studying an exposure-outcome association, careful consideration of confounders through direct acyclic graphs. Finally, investigators must plan the most appropriate analytic approach and we distinguish descriptive, explanatory, and predictive analyses.","Jean Jacob-Brassard and Charles {de Mestral}","https:\/\/doi.org\/10.1053\/j.semvascsurg.2022.09.002","2022","Big Data: Using Databases and Registries"],["The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as \u201cInductive (Prediction paradigm: Data mining\/Theory building) vs. Deductive (Explanation paradigm: Theory testing)\u201d. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.","Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang","https:\/\/doi.org\/10.1016\/j.jbusres.2021.04.019","2021","Big data and human resource management research: An integrative review and new directions for future research"],["In recent years, the just-in-time (JIT) predictive models have attracted considerable attention due to their ability to prevent degradation of prediction accuracy. However, one of their practical limitations is expensive computation, which becomes a major factor that prevents them from being used for big data quality prediction. This is because the JIT modeling methods need to update the local regression model using the relevant samples that are searched through the lineal scan of the database during online operation. To solve this issue, the present work proposes a novel hashing-based JIT (HbJIT) modeling method that is suitable for big data quality prediction. In HbJIT, a family of locality-sensitive hash functions is firstly used to hash big data into a set of buckets, in which similar samples are grouped on themselves. During online prediction, HbJIT looks up multiple buckets that have a high probability of containing similar samples of a query object through the intelligent probing scheme, uses the data objects in the buckets as the candidate set of the results, and then filters the candidate objects using a linear scan. After filtering, the most relevant samples are used to construct the local regression model to yield the prediction of the query object. By integrating the multi-probe hashing strategy into the JIT learning framework, HbJIT can not only deal with process nonlinearity and time-varying characteristics but also is applicable to large-scale industrial processes. Experimental results on real-world dataset have demonstrated that the proposed HbJIT is time-efficient in processing large-scale datasets, and greatly reduces the online prediction time without compromising on the prediction accuracy.","Xinmin Zhang and Jiang Zhai and Zhihuan Song and Yuan Li","https:\/\/doi.org\/10.1016\/B978-0-323-85159-6.50280-3","2022","Hashing-based just-in-time learning for big data quality prediction"],["Summary\nDiarrhoea is an important cause of morbidity and mortality in children from low-income and middle-income countries (LMICs), despite advances in the management of this condition. Understanding of the causes of diarrhoea in children in LMICs has advanced owing to large multinational studies and big data analytics computing the disease burden, identifying the important variables that have contributed to reducing this burden. The advent of the mobile phone has further enabled the management of childhood diarrhoea by providing both clinical support to health-care workers (such as diagnosis and management) and communicating preventive measures to carers (such as breastfeeding and vaccination reminders) in some settings. There are still challenges in addressing the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented geographical areas, concerns about patient confidentiality, unequal partnerships between study investigators, and the reactive approach to outbreaks. A transparent approach to promote the inclusion of researchers in LMICs could address partnership imbalances. A big data umbrella encompassing cloud-based centralised databases to analyse interlinked human, animal, agricultural, social, and climate data would provide an informative solution to the development of appropriate management protocols in LMICs.","Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke","https:\/\/doi.org\/10.1016\/S1473-3099(21)00585-5","2022","Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications"],["This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.","Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat","https:\/\/doi.org\/10.1016\/j.techfore.2021.121171","2021","The convergence of big data and accounting: innovative research opportunities"],["The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.","Scott A. Brave and R. Andrew Butters and Michael Fogarty","https:\/\/doi.org\/10.1016\/j.bushor.2021.06.004","2022","The perils of working with big data, and a SMALL checklist you can use to recognize them"],[null,"Mark Plazier and Vincent Raymaekers and Wim Duyvendak and Sacha Meeuws and Maarten Wissels and Steven Vanvolsem and Gert Roosen and Sven Bamps and Salah-Edine Achabar and Stefan Schu and Anna Keil and Bj\u00f6rn Carsten Schultheis and Philipp Slotty and Dirk {De Ridder} and Jan Vesper","https:\/\/doi.org\/10.1016\/j.neurom.2022.08.222","2022","PO048 \/ #665 THE BIG CHANGE IS COMMING: BIG DATA: E-POSTER VIEWING"],["Objective\nThe National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.\nMethods\nWe designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.\nResults\nWe identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process\u2013specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation\u00a0= 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.\nConclusions\nWe present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.","Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms","https:\/\/doi.org\/10.1016\/j.wneu.2022.02.113","2022","The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review"],["Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users\u2019 application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.","Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng","https:\/\/doi.org\/10.1016\/j.comnet.2021.107994","2021","A blockchain-based trading system for big data"],["Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.","Md. Arfanuzzaman","https:\/\/doi.org\/10.1016\/j.indic.2021.100127","2021","Harnessing artificial intelligence and big data for SDGs and prosperous urban future in South Asia"],["Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.","Maryam Ghasemaghaei and Goran Calic","https:\/\/doi.org\/10.1016\/j.dss.2019.03.008","2019","Can big data improve firm decision quality? The role of data quality and data diagnosticity"],["Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations\u2019 capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.","Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah","https:\/\/doi.org\/10.1016\/j.chb.2021.106777","2021","Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings"],["The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.","Ana Le\u00f3n and \u00d3scar Pastor","https:\/\/doi.org\/10.1016\/j.bdr.2021.100253","2021","Enhancing Precision Medicine: A Big Data-Driven Approach for the Management of Genomic Data"],["In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.","Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang","https:\/\/doi.org\/10.1016\/j.jpdc.2021.05.012","2021","SparkDQ: Efficient generic big data quality management on distributed data-parallel computation"],["The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.","M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI","https:\/\/doi.org\/10.1016\/j.procs.2019.04.127","2019","Big Data: Trade-off between Data Quality and Data Security"],["Big data analytics (BDA) adoption has gained attention in both practical and theoretical circles owing to the opportunities and advantages that can be reaped from it. In theory, the majority of researchers have evidenced the benefits of BDA, although barriers to its adoption have also been mentioned. This study draws upon the technology-organisation-environment framework and resource-based view theory to propose an integrated model that examines the drivers and impact of BDA adoption in the retail industry in Jordan. The proposed single model encapsulates the aspects of BDA adoption and performance. The study makes use of an online questionnaire survey to collect the required data, and the research model is eventually validated based on 132 responses gathered from the retail industry in Jordan. The findings highlight two major observations. The first is that relative advantage, organisational readiness, top management support, government support, data variety and data velocity all have a significant influence over BDA adoption. The second observation is that a significant association exists between BDA adoption and firm performance, providing information on the way firms can enhance their BDA adoption for enhanced performance. This study contributes to literature dedicated to examining BDA in terms of its drivers and impact on performance and can be used as a reference in developing nations.","Abdalwali Lutfi and Mahmaod Alrawad and Adi Alsyouf and Mohammed Amin Almaiah and Ahmad Al-Khasawneh and Akif Lutfi Al-Khasawneh and Ahmad Farhan Alshira'h and Malek Hamed Alshirah and Mohamed Saad and Nahla Ibrahim","https:\/\/doi.org\/10.1016\/j.jretconser.2022.103129","2023","Drivers and impact of big data analytic adoption in the retail industry: A quantitative investigation applying structural equation modeling"],["Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G\/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world\u2019s roadways, where each of which can produce up to 30\u00a0terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citro\u00ebn). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).","Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and Fran\u00e7ois Dessables","https:\/\/doi.org\/10.1016\/j.future.2022.04.020","2022","Big data architecture for connected vehicles: Feedback and application examples from an automotive group"],["Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to \u201cmine\u201d laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.","Dan Yang and Zihan Su and Min Zhao","https:\/\/doi.org\/10.1016\/j.cca.2022.01.001","2022","Big data and reference intervals"],["Battery is one of the most important and costly devices in electric vehicles (EVs). Developing an efficient battery management method is of great significance to enhancing vehicle safety and economy. Recently developed big-data and cloud platform computing technologies bring a bright perspective for efficient utilization and protection of vehicle batteries. However, a reliable data transmission network and a high-quality cloud battery dataset are indispensable to enable this benefit. This paper makes the first effort to systematically solve data quality problems in cloud-based vehicle battery monitoring and management by developing a novel integrated battery data cleaning framework. In the first stage, the outlier samples are detected by analyzing the temporal features in the battery data time series. The outlier data in the dataset can be accurately detected to avoid their impacts on battery monitoring and management. Then, the abnormal samples, including the noise polluted data and missing value, are restored by a novel future fusion data restoring model. The real electric bus operation data collected by a cloud-based battery monitoring and management platform are used to verify the performance of the developed data cleaning method. More than 93.3% of outlier samples can be detected, and the data restoring error can be limited to 2.11%, which validates the effectiveness of the developed methods. The proposed data cleaning method provides an effective data quality assessment tool in cloud-based vehicle battery management, which can further boost the practical application of the vehicle big data platform and Internet of vehicle.","Shuangqi Li and Hongwen He and Pengfei Zhao and Shuang Cheng","https:\/\/doi.org\/10.1016\/j.apenergy.2022.119292","2022","Data cleaning and restoring method for vehicle battery big data platform"],["The application of big data in the energy sector is considered as one of the main elements of Energy Internet. Crucial and promising challenges exist especially with the integration of renewable energy sources and smart grids. The ability to collect data and to properly use it for better decision-making is a key feature; in this work, the benefits and challenges of implementing big data analytics for renewable energy power stations are addressed. A framework was developed for the potential implementation of big data analytics for smart grids and renewable energy power utilities. A five-step approach is proposed for predicting the smart grid stability by using five different machine learning methods. Data from a decentralized smart grid data system consisting of 60,000 instances and 12 attributes was used to predict the stability of the system through three different machine learning methods. The results of fitting the penalized linear regression model show an accuracy of 96% for the model implemented using 70% of the data as a training set. Using the random forest tree model has shown 84% accuracy, and the decision tree model has shown 78% accuracy. Both the convolutional neural network model and the gradient boosted decision tree model yielded 87% for the classification model. The main limitation of this work is that the amount of data available in the dataset is considered relatively small for big data analytics; however the cloud computing and real-time event analysis provided was suitable for big data analytics framework. Future research should include bigger datasets with variety of renewable energy sources and demand across more countries.","Noha Mostafa and Haitham Saad Mohamed Ramadan and Omar Elfarouk","https:\/\/doi.org\/10.1016\/j.mlwa.2022.100363","2022","Renewable energy management in smart grids by using big data analytics and machine learning"],["In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called \"ontology-based data matching\", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.","Alexander Binder and Eva-Maria Iwer and Werner Quint","https:\/\/doi.org\/10.1016\/j.promfg.2020.11.051","2020","Big Data Management Using Ontologies for CPQ Solutions"],["The main purpose of this study is to examine the factors that are critical to create business value from business analytics (BA). Therefore, we conduct a meta-analysis of 125 firm-level studies spanning ten years of research from across 26 countries. We found evidence that the social factors of BA, such as human resources, management capabilities, and organizational culture show a greater impact on business value, whereas technical aspects play a minor role in enhancing firm performance. Through these findings, we contribute to the ongoing debate concerning BA business value by synthesizing and validating the findings of the body of knowledge.","Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg","https:\/\/doi.org\/10.1016\/j.im.2022.103685","2022","What translates big data into business value? A meta-analysis of the impacts of business analytics on firm performance"],["The implementation of the GDPR that aims at protecting European citizens\u2019 privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR\u2019s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.","Mouna Rhahla and Sahar Allegue and Takoua Abdellatif","https:\/\/doi.org\/10.1016\/j.jisa.2021.102896","2021","Guidelines for GDPR compliance in Big Data systems"],["Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.","Carla Wilkin and Ald\u00f3nio Ferreira and Kristian Rotaru and Luigi Red Gaerlan","https:\/\/doi.org\/10.1016\/j.accinf.2020.100470","2020","Big data prioritization in SCM decision-making: Its role and performance implications"],["The introduction and use of \u2018big data and analytics\u2019 is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.","Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin","https:\/\/doi.org\/10.1016\/j.techfore.2021.121222","2022","Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice"],["Spatially contiguous aerosol type grids were rarely available for air quality management in the past. To bridge the gap, we developed an integrated remote sensing and big data analytics framework to generate spatially gap-free aerosol type grids between 2000 and 2020 in China. The effect of emission control via environmental management on haze reduction was fully realized for the first time with the aid of satellite-based gap-free aerosol type data. Daily gap-free aerosol fine mode fraction (FMF) data were first derived via a data-driven regression model based on remote sensing big data. According to empirically determined FMF probability distributions over regions with typical emission sources, aerosols in China were classified into eight major types, including typical\/atypical\/mixed anthropogenic aerosols, typical\/atypical\/mixed dust, and typical mixed and multiple modes. The results indicated that the gridded FMF estimates derived in this study agreed well with FMF retrievals from AERONET, with correlation coefficient of 0.81 and root mean square error of 0.13. The long-term variations in major aerosol types showed that in China the territory covered by typical anthropogenic aerosols was reduced from 21.38% to 11.76% over the past two decades, while dust aerosols decreased from 6.99% to 2.15%. The declining trend in anthropogenic aerosols could be attributed to reduced coal consumption and\/or favorable dispersion conditions, whereas decreasing dust aerosols were largely associated with increased vegetation cover and\/or weakened wind speed in the west. Overall, such advancements provide fresh evidence to improve our understanding of the emission control effect on haze pollution variations in China.","Ke Li and Kaixu Bai and Mingliang Ma and Jianping Guo and Zhengqiang Li and Gehui Wang and Ni-Bin Chang","https:\/\/doi.org\/10.1016\/j.isprsjprs.2022.09.001","2022","Spatially gap free analysis of aerosol type grids in China: First retrieval via satellite remote sensing and big data analytics"],["Angelica sinensis is a kind of traditional Chinese medicine with very good blood nourishing effect, and it is cultivated in many regions of China. But with the increasingly severe climate, angelica cultivation has become a big problem. Therefore, this paper starts from the soil microorganisms of angelica planting, and studies the influence of soil biodiversity and angelica planting in the context of big data. This paper proposes a Hadoop system for big data analysis, combining the biocommunity characteristics and metagenomes of soil microorganisms. It then calculates the distance between samples and generates a dissimilarity matrix. Finally, this paper proposes a soil optimization method for angelica planting based on big data analysis of soil microorganisms. In order to optimize the Angelica planting soil designed in this paper, a soil microbial genome comparison experiment and a big data concurrent control test experiment were designed in this paper. It then analyzes the data obtained from the experiment, and the results of the analysis are used to optimize the soil for angelica planting. It finally compares the soil method of Angelica planting designed in this paper with the traditional Angelica planting method. The experimental results show that the survival rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms has increased by 16.09% compared with the traditional Angelica sinensis planting site. The growth rate of Angelica sinensis planted by the soil optimization method based on big data analysis of soil microorganisms increased by 9.64% compared with the traditional Angelica sinensis planting area.","Yinan Peng and Ze Ye and Peng Xi and Hongshan Qi and Bin Ji and Zhiye Wang","https:\/\/doi.org\/10.1016\/j.seta.2022.102674","2022","The relationship between soil microbial diversity and angelica planting based on network big data"],["Abstract\nIn the last years, the term Big Data (BD) is becoming ubiquitous in almost every scientific field given the information era we are living. In this respect, power systems is not the exception. This overflood of information can be overwhelming for the different energy stakeholders, nonetheless, numerous opportunities can also be derived from this BD. To do so, it is necessary to perform the corresponding knowledge extraction to this large data. This has been referred as Big Data Analytics (BDA) in the recent literature. In this respect, this work provides a comprehensive understanding of the attributes of BD in the power sector, along with the most representative tools, techniques, applications and challenges related to the incorporation of BDA in this field.","Xavier Dominguez and Alvaro Prado and Pablo Arboleya","https:\/\/doi.org\/10.1016\/B978-0-12-821204-2.00073-8","2022","Big Data applications in power systems"],["In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users\u2019 privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.","Jaqueline de Godoy and Kathrin Otrel-Cass and Kristian H\u00f8yer Toft","https:\/\/doi.org\/10.1016\/j.egyai.2021.100079","2021","Transformations of trust in society: A systematic review of how access to big data in energy systems challenges Scandinavian culture"],["Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.","Danilo Ardagna and Cinzia Cappiello and Walter Sam\u00e1 and Monica Vitali","https:\/\/doi.org\/10.1016\/j.future.2018.07.014","2018","Context-aware data quality assessment for big data"],["Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase\/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.","Faheem Ullah and M. Ali Babar","https:\/\/doi.org\/10.1016\/j.jnca.2021.103294","2022","On the scalability of Big Data Cyber Security Analytics systems"],["Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems\u2019 accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.","Imane El Alaoui and Youssef Gahi","https:\/\/doi.org\/10.1016\/j.procs.2019.11.007","2019","The Impact of Big Data Quality on Sentiment Analysis Approaches"],["Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.","Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo\u2019ai A. Tawalbeh and Mais Tawalbeh","https:\/\/doi.org\/10.1016\/j.procs.2021.12.206","2022","Emerging Concepts Using Blockchain and Big Data"],["Autonomous driving is assisted by different sensors, each providing information about certain parameters. What we are looking for is an integrated perspective of all these parameters to drive us into better decisions. To achieve this goal, a system that can handle these Big Data issues regarding volume, velocity and variety is needed. This paper aims to design and develop a real-time Big Data Warehouse repository, integrating the data generated by the multiple sensors developed in the context of IVS (In-Vehicle Sensing) systems; the data to be stored in this repository should be merged, which will imply its processing, consolidation and preparation for the analytical mechanisms that will be required. This multisensory fusion is important because it allows the integration of different perspectives in terms of sensor data, since they complement each other. Therefore, it can enrich the entire analysis process at the decision-making level, for instance, understanding what is going on inside the cockpit.","Ana Xavier Fernandes and Pedro Guimar\u00e3es and Maribel Yasmina Santos","https:\/\/doi.org\/10.1016\/j.procs.2022.08.099","2022","Big Data Analytics for Vehicle Multisensory Anomalies Detection"],["With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching\/visiting, online\/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.","Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang","https:\/\/doi.org\/10.1016\/j.bdr.2021.100289","2022","Big Data in Forecasting Research: A Literature Review"],["Over the past twodecades, healthcare providers across the world have adopted digital methods for capturing clinical and administrative information. Clinicians take contemporaneous records of their interactions with patients, so many health service providers have accrued vast repositories of longitudinally collected data. These data, coupled with advances in data extraction methods, computer processing power, and linkage to nonhealth public services data, now provide child and adolescent mental health researchers unique opportunities for tackling a broad range of clinical questions; especially those where the considerations of scale and generalizability make individually funded studies unaffordable. However, these \u201cbig\u201d data have their limitations. Best practice requires clinicians, informaticians, and data scientists to work together, so assumptions over data quality or validity are not misplaced. This chapter explains why the evidence base for child and adolescent mental healthcare needs big data applications as well as conventional research, to move the field forward. This chapter provides illustrations of big data applications to child and adolescent mental healthcare, primarily from England and the United Kingdom, but also offers a section on the global perspective. This chapter also reviews the methodological strengths and weaknesses of big data and describes the ethical and governance implications for their use.","Alice Wickersham and Johnny Downs","https:\/\/doi.org\/10.1016\/B978-0-323-91709-4.00005-6","2023","Chapter 3 - Clinical applications of big data to child and adolescent mental health care"],["The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.","Vinaya Keskar and Jyoti Yadav and Ajay Kumar","https:\/\/doi.org\/10.1016\/j.matpr.2021.05.597","2022","Perspective of anomaly detection in big data for data quality improvement"],["Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.","Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris","https:\/\/doi.org\/10.1016\/j.eswa.2021.114858","2021","Experimenting with big data computing for scaling data quality-aware query processing"],["With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.","Jian Wang","https:\/\/doi.org\/10.1016\/j.egyr.2022.02.022","2022","A novel oscillation identification method for grid-connected renewable energy based on big data technology"],["Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.","Georgios Georgiadis and Geert Poels","https:\/\/doi.org\/10.1016\/j.clsr.2021.105640","2022","Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review"],["Big data analytics (BDA) has recently gained importance as an emerging technology for handling big data. The use of advanced techniques with differing levels of intelligence, such as descriptive, predictive, prescriptive, and autonomous analytics, is expected to create value for firms. By viewing BDA as a sociotechnical system, we conduct a meta-analysis of 107 individual studies to integrate prior evidence on the role of the technical and social factors of BDA in creating BDA business value. The findings underline the predominant role of the social components in enhancing firm performance, such as the BDA system\u2019s human factors and a nurturing organizational structure, in contrast to the minor role of the technological factors. However, both the technical and social factors are found to be strong determinants of BDA business value. Through the combined lens of sociotechnical theory and the IS business value framework, we contribute to research and practice by enhancing the understanding of the main technical and social determinants of BDA business value at the firm level.","Thuy Duong Oesterreich and Eduard Anton and Frank Teuteberg and Yogesh K Dwivedi","https:\/\/doi.org\/10.1016\/j.jbusres.2022.08.028","2022","The role of the social and technical factors in creating business value from big data analytics: A meta-analysis"],["Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.","Ohbyung Kwon and Namyeon Lee and Bongsik Shin","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2014.02.002","2014","Data quality management, data usage experience and acquisition intention of big data analytics"],["The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of \u201cbig data\u201d have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of \u201cbig errors\u201d in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific \u201cstories\u201d, as well as explore and develop techniques and methods to mitigate or rectify those \u2018big-errors\u2019 brought by big data.","Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu","https:\/\/doi.org\/10.1016\/j.isprsjprs.2015.11.006","2016","Rethinking big data: A review on the data quality and usage issues"],["Background\nActivating prior medical knowledge in diagnosis and treatment is an important basis for clinicians to improve their care ability. However, it has not been systematically explained whether and how various big data resources affect the activation of prior knowledge in the big data environment faced by clinicians.\nObjective\nThe aim of this study is to contribute to a better understanding on how the activation of prior knowledge of clinicians is affected by a wide range of shared and private big data resources, to reveal the impact of big data resources on clinical competence and professional development of clinicians.\nMethod\nThrough the comprehensive analysis of extant research results, big data resources are classified as big data itself, big data technology and big data services at the public and institutional levels. A survey was conducted on clinicians and IT personnel in Chinese hospitals. A total of 616 surveys are completed, involving 308 medical institutions. Each medical institution includes a clinician and an IT personnel. SmartPLS version 2.0 software package was used to test the direct impact of big data resources on the activation of prior knowledge. We further analyze their indirect impact of those big data resources without direct impact.\nResults\n(1) Big data quality environment at the institutional level and the big data sharing environment at the public level directly affect activation of prior medical knowledge; (2) Big data service environment at the institutional level directly affects activation of prior medical knowledge; (3) Big data deployment environment at the institutional level and big data service environment at the public level have no direct impact on activation of prior knowledge of clinicians, but they have an indirect impact through big data quality environment and service environment at the institutional level and the big data sharing environment at the public level.\nConclusions\nBig data technology, big data itself and big data service at the public level and institutional level interact and influence each other to activate prior medical knowledge. This study highlights the implications of big data resources on improvement of clinicians\u2019 diagnosis and treatment ability.","Sufen Wang and Junyi Yuan and Changqing Pan","https:\/\/doi.org\/10.1016\/j.heliyon.2022.e10312","2022","Impact of big data resources on clinicians\u2019 activation of prior medical knowledge"],["Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.","Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino","https:\/\/doi.org\/10.1016\/j.bdr.2021.100234","2021","Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data"],["With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.","Le Yao and Zhiqiang Ge","https:\/\/doi.org\/10.1016\/j.jprocont.2018.04.004","2018","Big data quality prediction in the process industry: A distributed parallel modeling framework"],["Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.","Nadine C\u00f4rte-Real and Pedro Ruivo and Tiago Oliveira","https:\/\/doi.org\/10.1016\/j.im.2019.01.003","2020","Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?"],["To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.","Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu","https:\/\/doi.org\/10.1016\/j.jhtm.2022.02.026","2022","Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view"],["Intensive big data nanoindentation (BDNi) characterization was performed to reveal the cross-scale mechanical properties of, and hence distinguish the different phases in, inorganic\u2013organic hybrid oilwell cement-elastomer composites, hydrothermally cured at 160\u00a0\u00b0C and 20\u00a0MPa for 28\u00a0days. Totally-three emulsified and particulate elastomers, including styrene-butadiene latex (SBL) emulsion (6, 12, and 14\u00a0wt.%), polypropylene (PP) powder (12\u00a0wt.%), and nitrile rubber (NR) powder (6\u00a0wt.%), and a weighting agent, hematite (50\u00a0wt.%), were used as additives to finely adjust the mechanical properties and microstructure of the hybrid composites, which were respectively examined by the BDNi and mercury intrusion porosimetry and scanning electron microscopy. BDNi data were statistically deconvoluted by the Gaussian mixture modeling (GMM) to discern mechanically distinct phases and their Young\u2019s moduli and hardness at the micro\/nano scale and the bulk composites\u2019 properties at the macro scale. Results show that the SBL emulsion can be more homogeneously dispersed into the cement matrix, due to its emulsified soft consistency and hydrophilicity, resulting in the formation of soft coatings on, and softer infills intermixed with, the cement hydration products (CHPs). In contrast, the two hydrophobic, inert, particulate elastomers, PP and NR powders, only act as isolated soft inclusions embedded in the hydrated cement matrix. The NR melts at high temperatures and permeates into the pores of the cement matrix, leading to the formation of complex intervened micromorphology and hence functions better than the PP. All elastomers can effectively reduce the composites\u2019 Young\u2019s moduli: with increasing the elastomer contents, while the modulus of a BDNi-identified major CHP phase decreases from 20.9 to 11.3 GPa, the bulk composites\u2019 counterpart from 17.3 to 10.7 GPa. The BDNi enables the identification of multiple mechanically distinct phases in the hybrid composites and quantification of the property changes of these phases.","Yucheng Li and Yunhu Lu and Li Liu and Shengmin Luo and Li He and Yongfeng Deng and Guoping Zhang","https:\/\/doi.org\/10.1016\/j.conbuildmat.2022.129190","2022","Big data nanoindentation characterization of cross-scale mechanical properties of oilwell cement-elastomer composites"],["It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.","Hamza Ali and Ryad Titah","https:\/\/doi.org\/10.1016\/j.giq.2021.101600","2021","Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities"],["A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications.\u00a0This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms\u2019 strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms\u2019 activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.","Shivam Gupta and Th\u00e9o Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen","https:\/\/doi.org\/10.1016\/j.techfore.2021.120986","2021","Big data and firm marketing performance: Findings from knowledge-based view"],["Industrial big data technology has become one of the important driving forces to intelligent manufacturing in the steel industry. In this study, the characteristics of data in steel production are analyzed and an industrial big data platform for steeling process is developed to extract the quality-related parameters. A data-driven approach to construct prediction intervals (PIs) of mechanical performances for hot-rolling strips is proposed to represent the uncertainty and reliability of the prediction results. The proposed method employs a new manifold visualization method, SLISEMAP, to reduce the feature dimensions with interpretability, utilizes lower upper bound estimation (LUBE) method to obtain the PIs, in which the broad learning system (BLS) is used as the basic training network model and the artificial bee colony (ABC) algorithm is applied to optimize the weighting parameters of BLS under the LUBE framework. A hot-rolling steel coil dataset consisting of 39 variables and 1335 coil samples is used to validate the proposed method. Two Delta-based approaches, namely back propagation neural network (BPNN) and extreme learning machine (ELM); and three LUBE-based approaches, namely ABC-BPNN, ABC-ELM, and ABC-support vector regression (SVR) are compared with the proposed method. Results show that the proposed ABC-BLS in LUBE is effective and efficient in constructing the PIs with a higher coverage probability and a narrower interval width.","Gongzhuang Peng and Yinliang Cheng and Yufei Zhang and Jian Shao and Hongwei Wang and Weiming Shen","https:\/\/doi.org\/10.1016\/j.jmsy.2022.08.014","2022","Industrial big data-driven mechanical performance prediction for hot-rolling steel using lower upper bound estimation method"],["ABSTRACT\nSystematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and\/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.","Nadja B. Cech and Marnix H. Medema and Jon Clardy","https:\/\/doi.org\/10.1039\/d1np00061f","2021","Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality"],["Today\u05f3s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.","Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer","https:\/\/doi.org\/10.1016\/j.ijpe.2014.04.018","2014","Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications"],["The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always \u201ccleaner\u201d and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.","El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon","https:\/\/doi.org\/10.1016\/j.seta.2022.102093","2022","AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities"],["The first paradigm of plant breeding involves direct selection based phenotypic observation, followed by predictive breeding using statistical models constructed for quantitative traits based on genetic experimental design and more recently by incorporating molecular marker genotypes. However, plant performance or phenotype (P) is determined by the combining effects of genotype (G), envirotype (E) and genotype by environment interaction (GEI). Phenotypes can be predicted more precisely by training a model using data collected from multiple sources, including spatiotemporal omics (genomics, phenomics and enviromics across time and space). Integration of 3D information profiles (G-P-E), each with multidimensionality, provides predictive breeding with both tremendous opportunities and great challenges. Here, we first review innovative technologies for predictive breeding. We then evaluate multidimensional information profiles that can be integrated with a predictive breeding strategy, particularly envirotypic data, which have largely been neglected in data collection and nearly untouched in model construction. We propose a smart breeding scheme, integrated genomic-enviromic prediction (iGEP), as an extension of genomic prediction, using integrated multiomics information, big data technology and artificial intelligence (mainly focus on machine and deep learning). How to implement iGEP was discussed, including spatiotemporal models, environmental indices, factorial and spatiotemporal structure of plant breeding data, and cross-species prediction. A strategy is then proposed for prediction-based crop redesign at both the macro (individual, population and species) and micro (gene, metabolism and network) scales. Finally, we provide perspectives on translating the smart breeding into genetic gain through integrative breeding platforms and open-source breeding initiatives. We call for coordinated efforts in smart breeding through iGEP, institutional partnerships, and innovative technological support.","Yunbi Xu and Xingping Zhang and Huihui Li and Hongjian Zheng and Jianan Zhang and Michael S. Olsen and Rajeev K. Varshney and Boddupalli M. Prasanna and Qian Qian","https:\/\/doi.org\/10.1016\/j.molp.2022.09.001","2022","Smart breeding driven by big data, artificial intelligence and integrated genomic-enviromic prediction"],["Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.","Antonio Ben\u00edtez-Hidalgo and Crist\u00f3bal Barba-Gonz\u00e1lez and Jos\u00e9 Garc\u00eda-Nieto and Pedro Guti\u00e9rrez-Moncayo and Manuel Paneque and Antonio J. Nebro and Mar\u00eda del Mar Rold\u00e1n-Garc\u00eda and Jos\u00e9 F. Aldana-Montes and Ismael Navas-Delgado","https:\/\/doi.org\/10.1016\/j.knosys.2021.107489","2021","TITAN: A knowledge-based platform for Big Data workflow management"],["We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and\/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.","Lei Yang and Anqian Jiang and Jiahua Zhang","https:\/\/doi.org\/10.1016\/j.cie.2021.107550","2021","Optimal timing of big data application in a two-period decision model with new product sales"],["Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.","Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede","https:\/\/doi.org\/10.1016\/j.compind.2020.103368","2021","Big data analytics: Implementation challenges in Indian manufacturing supply chains"],["Today, as technologies mature and people are encouraged to contribute data to organizations\u2019 databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.","Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fern\u00e1ndez","https:\/\/doi.org\/10.1016\/j.procs.2019.11.052","2019","Risk Analysis of Using Big Data in Computer Sciences"],["Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.","Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui","https:\/\/doi.org\/10.1016\/j.giq.2021.101626","2022","Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development"],["In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.","Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU","https:\/\/doi.org\/10.1016\/j.im.2022.103622","2022","scenario modeling for government big data governance decision-making: Chinese experience with public safety services"],["The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.","Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley","https:\/\/doi.org\/10.1016\/j.mporth.2021.01.004","2021","Introduction to Big Data in trauma and orthopaedics"],["Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks\u2014i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network\u2014are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.","Zhimei Lei and Yandan Chen and Ming K. Lim","https:\/\/doi.org\/10.1016\/j.techsoc.2021.101570","2021","Modelling and analysis of big data platform group adoption behaviour based on social network analysis"],["Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a \u201cone-size-fits-all\u201d approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.","Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag","https:\/\/doi.org\/10.1016\/j.jretconser.2021.102827","2022","Cross-national differences in big data analytics adoption in the retail industry"],["This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.","Jean-S\u00e9bastien Lacam and David Salvetat","https:\/\/doi.org\/10.1016\/j.hitech.2021.100406","2021","Big data and Smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop"],["The rapid development of building energy consumption monitoring platforms makes engineering data more diverse, which facilitates the goal of reducing emissions. It is increasingly acknowledged that data preprocessing deserves the same attention as intelligent algorithms. In this work, the data quality issue of the engineering big data from non-demonstration complexes in China are analyzed thoroughly, and the analysis is based on clustering-based algorithms. We can conclude that the data of the hourly power of equipment groups are quality and stable, which is suitable for the benchmark to check other data. The quality of the data about pipes is acceptable. The number of data types about cooling towers is less, and the quality is worse. Regarding other data, the quality is unstable, so researchers should deal with those case-by-case. According to the above analysis, we proposed a convenient, rule-based data preprocessing framework that utilizes the law of physics, ensuring the strong coupling of multi-variants. After the data preprocessing, these engineering data are more reliable and can be used to improve performance or train models. Additionally, the proposed framework is more suitable for preprocessing multi-variant engineering data.","Ruikai He and Tong Xiao and Shunian Qiu and Jiefan Gu and Minchen Wei and Peng Xu","https:\/\/doi.org\/10.1016\/j.enbuild.2022.112372","2022","A rule-based data preprocessing framework for chiller rooms inspired by the analysis of engineering big data"],["An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and\/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.","Anita Lee-Post and Ram Pakath","https:\/\/doi.org\/10.1016\/j.dss.2019.113135","2019","Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development"],["Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.","N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana","https:\/\/doi.org\/10.1016\/j.future.2022.01.017","2022","A survey on blockchain for big data: Approaches, opportunities, and future directions"],["Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.","Haitham Ghallab and Hanan Fahmy and Mona Nasr","https:\/\/doi.org\/10.1016\/j.eij.2019.12.001","2020","Detection outliers on internet of things using big data technology"],["With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project\u2019s Governance, Architecture, Data Governance and the BDA Project\u2019s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.","Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}","https:\/\/doi.org\/10.1016\/j.jksuci.2020.11.024","2022","Big data analytics in telecommunications: Governance, architecture and use cases"],["2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.","Michela Piccarozzi and Barbara Aquilani","https:\/\/doi.org\/10.1016\/j.procs.2022.01.375","2022","The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies"],["The rapid development of communication technologies and expert systems have resulted in a large volume of medical data. Big data such as clinical data, omics data, and electronic health data are difficult to manage in real-time due to noise, large size, different formats, missing values and large features. Hence, it is more difficult for the health monitoring system to extract the correct information. Low quality and noisy data can lead to unnecessary treatment. To overcome these issues, we proposed Enriched Salp Swarm Optimization based Bidirectional Long Short Term Memory (ESSOBiLSTM) to monitor health. This method consists of four layers, such as the data collection layer, data storage layer, data analytics, and presentation layer. The initial layer handles a variety of information from main sources: wearable sensor devices (WSD), social network data, and medical records (MR). The second layer stores all the collected data from WSD, MR, and social network data to the cloud server through the wireless network. The proposed framework for performing big data analytics steps like preprocessing, filtering, dimensionality reduction, and classification is performed in the third layer. In the final layer, the doctor analyzes the patient's condition based on the classification results of the enriched SSO-BiLSTM. Based on the evaluation report, the proposed ESSOBiLSTM gives an accuracy of 85%, precision of 80%, RMSE of 0.6, MAE of 0.58, recall of 85% and F-measure of 79%. As a result, ESSOBiLSTM has proven to be more effective in monitoring health in large datasets.","Geetika Dhand and Kavita Sheoran and Parul Agarwal and Siddhartha Sankar Biswas","https:\/\/doi.org\/10.1016\/j.imu.2022.101010","2022","Deep enriched salp swarm optimization based bidirectional -long short term memory model for healthcare monitoring system in big data"],["Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.","Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}","https:\/\/doi.org\/10.1016\/j.procs.2021.12.189","2022","Diagnostic analysis for outlier detection in big data analytics"],["Decision making is the lifeblood of the enterprise \u2014 from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being \u201cdata driven\u201d is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.","Lilia Sfaxi and Mohamed Mehdi Ben\u00a0Aissa","https:\/\/doi.org\/10.1016\/j.datak.2020.101862","2020","DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects"],["Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.","Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong","https:\/\/doi.org\/10.1016\/j.tibtech.2022.02.005","2022","Are batch effects still relevant in the age of big data?"],["Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.","Mark Birkin","https:\/\/doi.org\/10.1016\/B978-0-08-102295-5.10616-X","2020","Big Data"],["The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.","Shuangqi Li and Pengfei Zhao","https:\/\/doi.org\/10.1016\/j.est.2020.102064","2021","Big data driven vehicle battery management method: A novel cyber-physical system perspective"],["Purpose\/Objective\nOutside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this \u201cBig Data\u201d study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.\nMaterials\/Methods\nDosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.\nResults\nPlans for 4664 patients treated to a mean plan dose of 64.4 \u00b1 13.2 Gy in 32 \u00b1 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.\nConclusion\nQuantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.","Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo","https:\/\/doi.org\/10.1016\/j.adro.2022.100925","2022","Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium"],["Information systems (IS) research has explored \u201ceffective use\u201d in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.","Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq","https:\/\/doi.org\/10.1016\/j.im.2019.02.001","2020","Factors influencing effective use of big data: A research framework"],["Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.","Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}","https:\/\/doi.org\/10.1016\/j.procs.2019.11.177","2019","A Review on Data Cleansing Methods for Big Data"],["With the widespread use of digital technologies such as big data, cloud computing and artificial intelligence in higher education, how to establish a scientific and systematic evaluation system to turn the traditional classroom with the one-way transmission of knowledge into an interactive space for exchanging ideas and inspiring wisdom has become an essential task for human resource management in universities, and a key to improving teaching quality. However, due to the debate between scientism and humanism in teaching evaluation, studies related to teaching performance have been isolated from human resource management, resulting in the lack of a systematic vision and framework for such studies. Relevant studies are still limited to the evaluation contents of different evaluation subjects. Evaluations also tend to focus only on the teaching process, ignoring the objectives of talent training, making it difficult for evaluations to play a goal-oriented role and hindering the further development of relevant studies. Therefore, this paper draws on human resource management methodologies and analyzes knowledge teaching evaluation system characteristics in colleges and universities in a big data context to construct a \u201cmultiple evaluations, trinity and four-step closed-loop\u201d big data-based knowledge teaching evaluation system. \u201cTrinity\u201d represents evaluation from three performance dimensions: teaching effect, teaching behavior and teaching ability. \u201cMultiple evaluations\u201d represents the design of teaching performance indicators based on teaching data, breaking the barriers between different evaluation subjects. \u201cFour-step closed-loop\u201d draws on performance management theory to standardize the teaching performance management process from four aspects: planning, implementation, evaluation, and feedback. This evaluation system provides a systematic methodology for unifying the theory and practice of innovative knowledge teaching evaluation system in universities in a big data context.","Xu Xin and Yu Shu-Jiang and Pang Nan and Dou ChenXu and Li Dan","https:\/\/doi.org\/10.1016\/j.jik.2022.100197","2022","Review on A big data-based innovative knowledge teaching evaluation system in universities"],["With buildings consuming nearly 40% of energy in developed countries, it is important to accurately estimate and understand the building energy efficiency in a city. A better understanding of building energy efficiency is beneficial for reducing overall household energy use and providing guidance for future housing improvement and retrofit. In this research, we propose a deep learning-based multi-source data fusion framework to estimate building energy efficiency. We consider the traditional factors associated with the building energy efficiency from the Energy Performance Certificate (EPC) for 160,000 properties (30,000 buildings) in Glasgow, UK (e.g., property structural attributes and morphological attributes), as well as the Google Street View (GSV) building fa\u00e7ade images as a complement. We compare the performance improvements between our data-fusion framework with traditional morphological attributes and image-only models. The results show that including the building fa\u00e7ade images from GSV, the overall model accuracy increases from 79.7% to 86.8%. A further investigation and explanation of the deep learning model are conducted to understand the relationships between building features and building energy efficiency by using SHapley Additive exPlanations (SHAP). Our research demonstrates the potential of using multi-source data in building energy efficiency prediction with high accuracy and short inference time. Our paper also helps understand building energy efficiency at the city level to help achieve the net-zero target by 2050.","Maoran Sun and Changyu Han and Quan Nie and Jingying Xu and Fan Zhang and Qunshan Zhao","https:\/\/doi.org\/10.1016\/j.enbuild.2022.112331","2022","Understanding building energy efficiency with administrative and emerging urban big data by deep learning in Glasgow"],["Purpose\nWe present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.\nMethod\nThe method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.\nResults\nThe HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450\u00a0ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3\u00d728TB.\nConclusion\nThe experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.","Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang","https:\/\/doi.org\/10.1016\/j.cmpb.2021.106293","2021","Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis"],["Transport modes and big data considers the characteristics of \u201cBig Data\u201d as described by the 5 \u201cVs,\u201d Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.","Hannah D Budnitz and Emmanouil Tranos and Lee Chapman","https:\/\/doi.org\/10.1016\/B978-0-08-102671-7.10601-3","2021","Transport Modes and Big Data"],["Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.","Chunquan Li and Yaqiong Chen and Yuling Shang","https:\/\/doi.org\/10.1016\/j.jestch.2021.06.001","2022","A review of industrial big data for decision making in intelligent manufacturing"],["Notwithstanding the potential of big data analytics technology for alliance management, there is a lack of understanding of how such digital technology influences alliance relationship stability (ARS). Drawing on the information technology-enabled organizational capabilities (IT-enabled OCs) perspective, this study empirically verifies that big data analytics promotes ARS and risk management capability. Moreover, market risk management capability (MRM) enhances ARS, and data quality moderates the relationship between big data analytics usage (BDU) and MRM. This research reveals the impact mechanism of BDU on the ARS. Implications for management and future research are presented as well.","Shunzhi Lin and Jiabao Lin and Feiyun Han and Xin (Robert) Luo","https:\/\/doi.org\/10.1016\/j.im.2022.103680","2022","How big data analytics enables the alliance relationship stability of contract farming in the age of digital transformation"],["The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.","Lingqiang Kong and Zhifeng Liu and Jianguo Wu","https:\/\/doi.org\/10.1016\/j.jclepro.2020.123142","2020","A systematic review of big data-based urban sustainability research: State-of-the-science and future directions"],["Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on \u201cfitness for requirement\u201d, can arouse the user\u2019s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user\u2019s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user\u2019s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.","Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu","https:\/\/doi.org\/10.1016\/j.future.2019.07.063","2020","Semantic-aware data quality assessment for image big data"],["Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.","Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo","https:\/\/doi.org\/10.1016\/j.techfore.2021.121355","2022","Evaluating the impact of big data analytics usage on the decision-making quality of organizations"],["Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.","Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi","https:\/\/doi.org\/10.1016\/j.rcim.2022.102331","2022","Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study"],["In this paper we aim to classify digital data sources for the measurement of tourist mobility, to establish a set of assessment indicators, and to compare two Big Data sources to gain empirical insights into how we can measure tourism with Big Data. For three holiday destinations in Germany, passive mobile data and passive global positioning systems (GPS) data are compared with reference data from the destinations for twelve weeks in the summer of 2019. Results show that mobile network data are on a plausible level compared to the local reference data and are able to predict the temporal pattern to a very high degree. GPS app-based data also perform well, but are less plausible and precise than mobile network data.","Dirk Schm\u00fccker and Julian Reif","https:\/\/doi.org\/10.1016\/j.annale.2022.100061","2022","Measuring tourism with big data? Empirical insights from comparing passive GPS data and passive mobile data"],["Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures\u2019 elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach\u2019s scalability with that of Argo Workflows \u2013 one of the most prominent tools in the area of Big Data workflows \u2013 and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.","Nikolay Nikolov and Yared Dejene Dessalk and Akif Quddus Khan and Ahmet Soylu and Mihhail Matskin and Amir H. Payberah and Dumitru Roman","https:\/\/doi.org\/10.1016\/j.iot.2021.100440","2021","Conceptualization and scalable execution of big data workflows using domain-specific languages and software containers"],["Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.","Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang","https:\/\/doi.org\/10.1016\/j.cose.2021.102529","2022","Personal big data pricing method based on differential privacy"],["The demand for electricity is increasing exponentially day by day, especially with the arrival of electric vehicles. In the smart community neighborhood project, electricity should be produced at the household or community level and sold or bought according to the demands. Since the actors can produce, sell, and buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways, such as machine learning for analyzing the household data for customer demand and peak hours for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensuring data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here, in this article, we will show the importance of the TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.","Dhanya Therese Jose and J\u00f8rgen Holme and Antorweep Chakravorty and Chunming Rong","https:\/\/doi.org\/10.1016\/j.bcra.2022.100081","2022","Integrating big data and blockchain to manage energy smart grids\u2014TOTEM framework"],["Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.","Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li","https:\/\/doi.org\/10.1016\/j.inffus.2017.10.006","2018","A survey on deep learning for big data"],["Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V\u2019s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.","Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.06.005","2018","Big data with cognitive computing: A review for the future"],["Purpose\nBig data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of \u201cbig data.\u201d Therefore, this paper attempts to thoroughly investigate \u201cbig data,\u201d its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of \u201cbig data\u201d and its extension into \u201cbig data II\u201d\/IoT\u2013value-adding perspectives by proposing a value-adding framework.\nMethodology\/research approach\nThe research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles\/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of \u201cbig data\u201d applications between 2010 and 2015.\nFindings\/results\nThe four main attributes or factors identified with \u201cbig data\u201d include \u2013 big data development sources (Variety \u2013 V1), big data acquisition (Velocity \u2013 V2), big data storage (Volume \u2013 V3), and finally big data analysis (Veracity \u2013 V4). However, the study of \u201cbig data\u201d has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding \u2013 V5) \u2013 \u201cBig Data cloud computing perspective\/Internet of Things (IoT)\u201d. Hence, the four Vs of \u201cbig data\u201d is now expanded into five Vs.\nOriginality\/value of research\nThis paper presents original literature review research discussing \u201cbig data\u201d issues, trends and perspectives in operations\/supply-chain management in order to propose \u201cBig data II\u201d (IoT \u2013 Value-adding) framework. This proposed framework is supposed or assumed to be an extension of \u201cbig data\u201d in a value-adding perspective, thus proposing that \u201cbig data\u201d be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.","Richard Addo-Tenkorang and Petri T. Helo","https:\/\/doi.org\/10.1016\/j.cie.2016.09.023","2016","Big data applications in operations\/supply-chain management: A literature review"],[null,"Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh","https:\/\/doi.org\/10.1016\/j.ocl.2016.05.009","2016","Big Data, Big Problems: Incorporating Mission, Values, and Culture in Provider Affiliations"],["A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.","D.R. Cox and Christiana Kartsonaki and Ruth H. Keogh","https:\/\/doi.org\/10.1016\/j.spl.2018.02.015","2018","Big data: Some statistical issues"],["In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.","Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu","https:\/\/doi.org\/10.1016\/j.bdr.2018.05.007","2018","Lossless Pruned Naive Bayes for Big Data Classifications"],["A cloud framework refers to the aggregation of components like development tools, middleware and database services, needed for cloud computing, which aids in developing, deploying and managing cloud based applications strenuously, consequently making it an efficacious paradigm for massive scaling of dynamically allocated resources and their complex computing. Big Data Analytics (BDA) delivers data management solutions in the cloud architecture for storing, analysing and processing a huge volume of data. This paper presents a survey for performance based comparative analysis of cloud-based big data frameworks from leading enterprises like Amazon, Google, IBM, and Microsoft, which will assist researcher, IT analysts, reader and business user in picking the framework best suited for their work ensuring success in terms of favourable outcomes.","Subia Saif and Samar Wazir","https:\/\/doi.org\/10.1016\/j.procs.2018.05.172","2018","Performance Analysis of Big Data and Cloud Computing Techniques: A Survey"],["The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.","Neha Sharma and Malini M. Patil and Madhavi Shamkuwar","https:\/\/doi.org\/10.1016\/B978-0-12-817356-5.00010-3","2019","Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey for the Medical Industry"],["Big Data Analytics (BDA) is increasingly becoming a trending practice that generates an enormous amount of data and provides a new opportunity that is helpful in relevant decision-making. The developments in Big Data Analytics provide a new paradigm and solutions for big data sources, storage, and advanced analytics. The BDA provide a nuanced view of big data development, and insights on how it can truly create value for firm and customer. This article presents a comprehensive, well-informed examination, and realistic analysis of deploying big data analytics successfully in companies. It provides an overview of the architecture of BDA including six components, namely: (i) data generation, (ii) data acquisition, (iii) data storage, (iv) advanced data analytics, (v) data visualization, and (vi) decision-making for value-creation. In this paper, seven V's characteristics of BDA namely Volume, Velocity, Variety, Valence, Veracity, Variability, and Value are explored. The various big data analytics tools, techniques and technologies have been described. Furthermore, it presents a methodical analysis for the usage of Big Data Analytics in various applications such as agriculture, healthcare, cyber security, and smart city. This paper also highlights the previous research, challenges, current status, and future directions of big data analytics for various application platforms. This overview highlights three issues, namely (i) concepts, characteristics and processing paradigms of Big Data Analytics; (ii) the state-of-the-art framework for decision-making in BDA for companies to insight value-creation; and (iii) the current challenges of Big Data Analytics as well as possible future directions.","Mandeep Kaur Saggi and Sushma Jain","https:\/\/doi.org\/10.1016\/j.ipm.2018.01.010","2018","A survey towards an integration of big data analytics to big insights for value-creation"],["Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.","Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza","https:\/\/doi.org\/10.1016\/j.seta.2018.11.008","2019","A Systematic Literature Review on big data for solar photovoltaic electricity generation forecasting"],[null,"Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil","https:\/\/doi.org\/10.1016\/j.bdr.2019.100123","2019","Big Data Exploration, Visualization and Analytics"],["The era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision-making and knowledge discovery activities. In this paper, the five Vs of big data, volume, velocity, variety, veracity, and value, are reviewed, as well as new technologies, including NoSQL databases that have emerged to accommodate the needs of big data initiatives. The role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data.","Veda C. Storey and Il-Yeol Song","https:\/\/doi.org\/10.1016\/j.datak.2017.01.001","2017","Big data technologies and Management: What conceptual modeling can do"],["In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.","Domingo Llorente Rivera and Markus R. Scholz and Christoph B\u00fchl and Markus Krauss and Klaus Schilling","https:\/\/doi.org\/10.1016\/j.ifacol.2019.12.364","2019","Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study"],["Cloud computing and big data analytics are, without a doubt, two of the most important technologies to enter the mainstream IT industry in recent years. Surprisingly, the two technologies are coming together to deliver powerful results and benefits for businesses. Cloud computing is already changing the way IT services are provided by so called cloud companies and how businesses and users interact with IT resources. Big Data is a data analysis methodology enables by recent advances in information and communications technology. However, big data analysis requires a huge amount of computing resources making adoption costs of big data technology is not affordable for many small to medium enterprises. In this paper, we outline the the benefits and challenges involved in deploying big data analytics through cloud computing. We argue that cloud computing can support the storage and computing requirements of big data analytics. We discuss how the consolidation of these two dominant technologies can enhance the process of big data mining enabling businesses to improve decision-making processes. We also highlight the issues and risks that should be addressed when using a so called CLaaS, cloud-based service model.","Bala M. Balachandran and Shivika Prasad","https:\/\/doi.org\/10.1016\/j.procs.2017.08.138","2017","Challenges and Benefits of Deploying Big Data Analytics in the Cloud for Business Intelligence"],["It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.","Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen","https:\/\/doi.org\/10.1016\/j.jcss.2014.04.022","2014","A spatiotemporal compression based approach for efficient big data processing on Cloud"],["Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.","Wendy Arianne G\u00fcnther and Mohammad H. {Rezazade Mehrizi} and Marleen Huysman and Frans Feldberg","https:\/\/doi.org\/10.1016\/j.jsis.2017.07.003","2017","Debating big data: A literature review on realizing value from big data"],["This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics\u2014DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.","Wu Qing","https:\/\/doi.org\/10.1016\/B978-0-12-820714-7.00009-1","2021","Chapter 9 - Global Practice of AI and Big Data in Oil and Gas Industry"],["This paper investigates big data analytics research and application in supply chain management between 2010 and 2016 and provides insights to industries. In recent years, the amount of data produced from end-to-end supply chain management practices has increased exponentially. Moreover, in current competitive environment supply chain professionals are struggling in handling the huge data. They are surveying new techniques to investigate how data are produced, captured, organized and analyzed to give valuable insights to industries. Big Data analytics is one of the best techniques which can help them in overcoming their problem. Realizing the promising benefits of big data analytics in the supply chain has motivated us to write a review on the importance\/impact of big data analytics and its application in supply chain management. First, we discuss big data analytics individually, and then we discuss the role of big data analytics in supply chain management (supply chain analytics). Current research and application are also explored. Finally, we outline the insights to industries. Observations and insights from this paper could provide the guideline for academia and practitioners in implementing big data analytics in different aspects of supply chain management.","Sunil Tiwari and H.M. Wee and Yosef Daryanto","https:\/\/doi.org\/10.1016\/j.cie.2017.11.017","2018","Big data analytics in supply chain management between 2010 and 2016: Insights to industries"],["To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.","Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd","https:\/\/doi.org\/10.1016\/j.techfore.2015.12.019","2018","Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations"],["The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors\u2019 aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.","Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo","https:\/\/doi.org\/10.1016\/j.surg.2018.06.022","2018","Big data: More than big data sets"],["The world is witnessing an unprecedented interest in big data. Big data is data that is big in size (volume), big in variety (structured; semi-structured; unstructured), and big in speed of change (velocity). It was reported that almost 90% of the data worldwide was just created in the past 2 years. Therefore, this paper is an attempt to align ERP systems with big data. The objective is to suggest a future research agenda to bring together big data and ERP. While almost everyone is talking about big data at the product or tool level, relationship with social media, relationship with Internet of things, etc. no one has tried to integrate big data and ERP. A research agenda is discussed and introduced in this paper.","Ahmed Elragal","https:\/\/doi.org\/10.1016\/j.protcy.2014.10.089","2014","ERP and Big Data: The Inept Couple"],["This document presents an introduction to the ISCTSC Special Issue of Transport Research Procedia. It synthesizes the discussions held at the 11th International Conference on Transport Survey Methods, and describes the contents of the selected contributions. This conference has been held in different countries from all over the world, involving an increasing group of enthusiastic and generous specialists, willing to share their knowledge. This 11th conference was an opportunity to discuss the state of the art on transport survey methods, but also to question the way transport surveys are conducted in the era of big data. We took the opportunity to identify the main challenges, and the most important questions.","Patrick Bonnel and Marcela A. Munizaga","https:\/\/doi.org\/10.1016\/j.trpro.2018.10.001","2018","Transport survey methods - in the era of big data facing new and old challenges"],["Based on our GEOSS Clearinghouse operating experience, we summarized the three Earth Observation (EO) Big Data access challenges, namely, fast access, accurate service estimation and global access, and two essential research questions: are there any spatiotemporal patterns when end users access EO data, and how can these spatiotemporal patterns be utilized to better facilitate EO Big Data access? To tackle these two research questions, we conducted a two-year pattern analysis with 2+ million user access records. The spatial pattern, temporal pattern and spatiotemporal pattern of user-data interactions were explored. For the second research question, we developed three spatiotemporal optimization strategies to respond to the three access challenges: a) spatiotemporal indexing to accelerate data access, b) spatiotemporal service modeling to improve data access accuracy and c) spatiotemporal cloud computing to enhance global access. This research is a pioneering framework for spatiotemporal optimization of EO Big Data access and valuable for other multidisciplinary geographic data and information research.","Jizhe Xia and Chaowei Yang and Qingquan Li","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2018.06.010","2018","Using spatiotemporal patterns to optimize Earth Observation Big Data access: Novel approaches of indexing, service modeling and cloud computing"],["The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.","Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin","https:\/\/doi.org\/10.1016\/j.jnca.2016.04.008","2016","A survey of big data management: Taxonomy and state-of-the-art"],["The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.","Deepak Kumar Sharma and Bhanu Tokas and Leo Adlakha","https:\/\/doi.org\/10.1016\/B978-0-12-822226-3.00002-7","2021","Chapter 2 - Deep learning in big data and data mining"],["The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called \u201cbig data\u201d field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.","Gr\u00e9goire Rey and Karim Bounebache and Claire Rondet","https:\/\/doi.org\/10.1016\/j.jflm.2016.12.004","2018","Causes of deaths data, linkages and big data perspectives"],["This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users\u05f3 adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.","Dong-Hee Shin","https:\/\/doi.org\/10.1016\/j.telpol.2015.03.007","2016","Demystifying big data: Anatomy of big data developmental process"],["Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core\u2122i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.","Awais Ahmad and Anand Paul and M. Mazhar Rathore","https:\/\/doi.org\/10.1016\/j.neucom.2015.04.109","2016","An efficient divide-and-conquer approach for big data analytics in machine-to-machine communication"],["External cause injuries are defined as intentionally or unintentionally harm or injury to a person, which may be caused by trauma, poisoning, assault, accidents, etc., being fatal (fatal injury) or not leading to death (non-fatal injury). External injuries have been considered a global health problem for two decades. This work aims to determine criminal patterns using data mining techniques to a sample of patients from Mumbai city in India.","Jesus Silva and Jack Zilberman and Ligia Romero and Omar Bonerge Pineda and Yaneth Herazo-Beltran","https:\/\/doi.org\/10.1016\/j.procs.2020.03.114","2020","Identification of Patterns of Fatal Injuries in Humans through Big Data"],["Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.","Shannon Haymond and Randall K. Julian and Emily L. Gill and Stephen R. Master","https:\/\/doi.org\/10.1016\/B978-0-12-817962-8.00018-4","2021","Chapter 3 - Machine learning and big data in pediatric laboratory medicine"],["As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.","Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo","https:\/\/doi.org\/10.1016\/j.jss.2019.09.053","2020","What Can We Learn About Drug Safety and Other Effects in the Era of Electronic Health Records and Big Data That We Would Not Be Able to Learn From Classic Epidemiology?"],["People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia \u2013 Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).","Maribel Yasmina Santos and Jorge {Oliveira e S\u00e1} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and Jo\u00e3o Galv\u00e3o","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2017.07.012","2017","A Big Data system supporting Bosch Braga Industry 4.0 strategy"],["Participatory sensing has become an emerging technology of this era owing to its low cost in big sensor data collection. Prior to participatory sensing, large-scale deployment complexities were found in wireless sensor networks when collecting data from widespread resources. Participatory sensing systems employ handheld devices as sensors to collect data from communities and transmit to the cloud, where data are further analyzed by expert systems. The processes involved in participatory sensing, such as data collection, transmission, analysis, and visualization, exhibit certain management issues. This study aims to identify big data management issues that must be addressed at the cloud side during data processing and storing and at the participant side during data collection and visualization. It then proposes a framework for big data management in participatory sensing to resolve the contemporary big data management issues on the basis of suggested principles. Moreover, this work presents case studies to elaborate the existence of the highlighted issues. Finally, the limitations, recommendations, and future research directions for academia and industry in the domain of participatory sensing are discussed.","Ahmad Karim and Aisha Siddiqa and Zanab Safdar and Maham Razzaq and Syeda Anum Gillani and Huma Tahir and Sana Kiran and Ejaz Ahmed and Muhammad Imran","https:\/\/doi.org\/10.1016\/j.future.2017.10.007","2020","Big data management in participatory sensing: Issues, trends and future directions"],["Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.","Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li","https:\/\/doi.org\/10.1016\/j.tourman.2018.03.009","2018","Big data in tourism research: A literature review"],["A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled \u201cHarnessing Big Data\u201d, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.","Gabriel E. S\u00e1nchez-Mart\u00ednez and Marcela Munizaga","https:\/\/doi.org\/10.1016\/j.retrec.2016.10.008","2016","Workshop 5 report: Harnessing big data"],["Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.","Tijs {van den Broek} and Anne Fleur {van Veenstra}","https:\/\/doi.org\/10.1016\/j.techfore.2017.09.040","2018","Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation"],["The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.","Venketesh Palanisamy and Ramkumar Thirunavukarasu","https:\/\/doi.org\/10.1016\/j.jksuci.2017.12.007","2019","Implications of big data analytics in developing healthcare frameworks \u2013 A review"],["The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.","Yunji Liang and Xiaolong Zheng and Daniel D. Zeng","https:\/\/doi.org\/10.1016\/j.inffus.2019.04.001","2019","A survey on big data-driven digital phenotyping of mental health"],["In the era of Big Data, many organisations have successfully leveraged Big Data Analytics (BDA) capabilities to improve their performance. However, past literature on BDA have put limited focus on understanding the capabilities required to extract value from big data. In this context, this paper aims to provide a systematic literature review of BDA capabilities in supply chain and develop the capabilities maturity model. The paper presents the bibliometric and thematic analysis of research papers from 2008 to 2016. This paper contributes in theorizing BDA capabilities in context of supply chain, and provides future direction of research in this field.","Deepak Arunachalam and Niraj Kumar and John Paul Kawalek","https:\/\/doi.org\/10.1016\/j.tre.2017.04.001","2018","Understanding big data analytics capabilities in supply chain management: Unravelling the issues, challenges and implications for practice"],["The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.","Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng","https:\/\/doi.org\/10.1016\/j.scs.2020.102024","2020","Analyzing the Influencing Factors of Urban Thermal Field Intensity Using Big-Data-Based GIS"],["With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.","Mart\u00ed Cuquet and Anna Fensel","https:\/\/doi.org\/10.1016\/j.techsoc.2018.03.005","2018","The societal impact of big data: A research roadmap for Europe"],["Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.","Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.07.009","2016","Big data: From beginning to future"],["Data analytics play an active role in medical applications to extract relevant information from heaps of data samples. Internet of things (IoT) technology has slowly captured the market and is entering the health care sector too. With the help of big data analytics, various IoT-based devices can auto-monitor the health conditions of patients and can send the status to concerned physicians and family members. Thus, the integration of big data analytics with IoT technology forms a favorable combination in the health care domain. In this chapter, we discuss the two latest trends that include big data analytics and IoT with respect to its relevance in medical fields. We also analyze a health care monitoring system, which is an IoT-based model integrated with big data analytics. The system integrates patient specific information over the cloud. In the more developed model, the implementation was made to monitor the health status of the patients. The developed model was found to be faster and thus it can be easily implemented into a real time patient health monitoring and status management system. Medical experts can take advantage of this system model, thereby providing appropriate information to appropriate patient and doctors at appropriate time.","Sushruta Mishra and Brojo Kishore Mishra and Hrudaya Kumar Tripathy and Arijit Dutta","https:\/\/doi.org\/10.1016\/B978-0-12-818318-2.00001-5","2020","Chapter 1 - Analysis of the role and scope of big data analytics with IoT in health care domain"],["The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.","Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood","https:\/\/doi.org\/10.1016\/j.jbusres.2018.04.029","2018","On big data-guided upstream business research and its knowledge management"],["This case increases your understanding of data governance in an era of sophisticated analytics and Big Data where corporate data integrity and data quality may be at risk. KrauseMcMahon, a large certified public accounting and business consulting firm, faces a tradeoff of increasing control of the company\u2019s data assets versus unleashing end user innovation due to the proliferation of self-service business intelligence tools. You are required to analyze the issues in the case from organizational, financial, and technical perspectives to propose alternatives the organization should consider and make specific recommendations on how the company should proceed. By completing this case, you will demonstrate cross-disciplinary abilities related to foundational business, accounting, and broad management competencies. By addressing such competencies, the case requires your use of accounting, MIS, and upper-level business skills.","Frederick J. Riggins and Bonnie K. Klamm","https:\/\/doi.org\/10.1016\/j.jaccedu.2016.12.002","2017","Data governance case at KrauseMcMahon LLP in an era of self-service BI and Big Data"],["Big Data is now readily available for analysis, and analysts trained to conduct effective analysis in this area are in high demand. However, there is a dearth of discussion in the literature related to identifying the important cognitive skills required for accountants to conduct effective Big Data analysis. Here we argue that accountants need to approach Big Data analysis as informed skeptics, being ever ready to challenge the analysis by asking good questions in appropriate topical areas. These areas include understanding the limits of measurement and representation, the subjectiveness of insight, the challenges of statistics and integrating data sets, and the effects of underdetermination and inductive reasoning. Accordingly, we develop a framework and an illustrative example to facilitate the training of accounting students to become informed skeptics in the era of Big Data by explaining the conceptual relevance of each of the topical areas to Big Data analysis. In addition, example questions are identified that accountants conducting Big Data analysis should be asking regarding each topic. Further, for each topic, references to additional resources are provided that students can access to learn more about effectively conducting Big Data analysis.","Earl McKinney and Charles J. Yoos and Ken Snead","https:\/\/doi.org\/10.1016\/j.jaccedu.2016.12.007","2017","The need for \u2018skeptical\u2019 accountants in the era of Big Data"],["Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.","Yichuan Wang and Nick Hajli","https:\/\/doi.org\/10.1016\/j.jbusres.2016.08.002","2017","Exploring the path to big data analytics success in healthcare"],["Big Data analytics in national security, law enforcement and the fight against fraud have the potential to reap great benefits for states, citizens and society but require extra safeguards to protect citizens' fundamental rights. This involves a crucial shift in emphasis from regulating Big Data collection to regulating the phases of analysis and use. In order to benefit from the use of Big Data analytics in the field of security, a framework has to be developed that adds new layers of protection for fundamental rights and safeguards against erroneous and malicious use. Additional regulation is needed at the levels of analysis and use, and the oversight regime is in need of strengthening. At the level of analysis \u2013 the algorithmic heart of Big Data processes \u2013 a duty of care should be introduced that is part of an internal audit and external review procedure. Big Data projects should also be subject to a sunset clause. At the level of use, profiles and (semi-) automated decision-making should be regulated more tightly. Moreover, the responsibility of the data processing party for accuracy of analysis \u2013 and decisions taken on its basis \u2013 should be anchored in legislation. The general and security-specific oversight functions should be strengthened in terms of technological expertise, access and resources. The possibilities for judicial review should be expanded to stimulate the development of case law.","Dennis Broeders and Erik Schrijvers and Bart {van der Sloot} and Rosamunde {van Brakel} and Josta {de Hoog} and Ernst {Hirsch Ballin}","https:\/\/doi.org\/10.1016\/j.clsr.2017.03.002","2017","Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data"],["Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.","Ekaterina Smirnova and Andrada Ivanescu and Jiawei Bai and Ciprian M. Crainiceanu","https:\/\/doi.org\/10.1016\/j.spl.2018.02.014","2018","A practical guide to big data"],["Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five \u201cVs\u201d: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.","Bonnie L. Westra and Jessica J. Peterson","https:\/\/doi.org\/10.1016\/j.aorn.2016.07.009","2016","Big Data and Perioperative Nursing"],[null,"Fabien Lareyre and C\u00e9dric Adam and Marion Carrier and Juliette Raffort","https:\/\/doi.org\/10.1016\/j.avsg.2020.04.022","2020","Artificial Intelligence in Vascular Surgery: Moving from Big Data to Smart Data"],["Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as \u201cbig data\u201d has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.","Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed","https:\/\/doi.org\/10.1016\/j.chb.2018.08.039","2019","Social media big data analytics: A survey"],["Information security governance is an important aspects for all organizations. Given the crucial importance of IT systems and the increasing range of threats these systems are facing, there is an increasing interest on the topic. On the other hand, Big Data environments are also beginning to be more pervasive as IT is increasing its importance for organizations worldwide. In order to better know which aspects are the most important for the intersection of Big Data and information security governance, authors present in this paper a systematic mapping on this topic. Authors illustrate challenges and gaps concerning the topic and clarify these challenges by means of a classification of the environments they take place, the security risk spectrums they concern, and the security governance measures they take to mitigate them; by providing solutions as in a framework, model, software or tool, wherever possible. Results are expected to be useful for IT security professionals and information systems practitioners as a whole.","Reza Saneei Moghadam and Ricardo Colomo-Palacios","https:\/\/doi.org\/10.1016\/j.procs.2018.10.057","2018","Information security governance in big data environments: A systematic mapping"],["Big data has become omnipresent and crucial for many application domains. Big data makes reference to the explosive quantity of data generated in today\u2019s society that might contain personally identifiable information (PII). That\u2019s why the challenge from the point of view of data privacy is one of the major hurdles for the application of big data. In that situation, several techniques were exposed in order to ensure privacy in big data including generalization, randomization and cryptographic techniques as well. It is well known that there exist two main types of attributes in the literature, quasi identifier and sensitive attributes. In this paper, we are going to focus on quasi identifier attributes. Over the years, k-anonymity has been treated with great interest as an anonymization technique ensuring privacy in big data when we are dealing with quasi identifier attributes. Despite the fact that many algorithms of k-anonymity have been proposed, most of them admit that the threshold k of k-anonymity has to be known before anonymizing the data set. Here, a novel way in applying k-anonymity for quasi identifier attributes is presented. It\u2019s a new algorithm called \u201ck-anonymity without prior value of the threshold k\u201d. Our proposed algorithm was experimentally evaluated using a test table of quasi identifier attributes. Furthermore, we highlight all the steps of our proposed algorithm with detailed comments.","Zakariae {El Ouazzani} and Hanan {El Bakkali}","https:\/\/doi.org\/10.1016\/j.procs.2018.01.097","2018","A new technique ensuring privacy in big data: K-anonymity without prior value of the threshold k"],["An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5\u202f\u03bcm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3\u202f\u00b1\u202f27.1\u202f\u03bcg\/m3 (n\u202f=\u202f365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1\u202f\u00b1\u202f13.9\u202f\u03bcg\/m3 (n\u202f=\u202f365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.","YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong","https:\/\/doi.org\/10.1016\/j.envpol.2019.07.034","2019","A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources"],["Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.","Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad","https:\/\/doi.org\/10.1016\/j.future.2018.08.040","2019","IoT big data analytics for smart homes with fog and cloud computing"],["\u201cBig data\u201d is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to \u201clong data,\u201d \u201cwide data,\u201d and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.","Tim Hulsen","https:\/\/doi.org\/10.1016\/B978-0-12-822884-5.00016-7","2021","Chapter 4 - Challenges and solutions for big data in personalized healthcare"],["Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.","In Lee","https:\/\/doi.org\/10.1016\/j.bushor.2017.01.004","2017","Big data: Dimensions, evolution, impacts, and challenges"],["Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources\u2014physical, human, and organizational capital\u2014moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic\/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.","Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne","https:\/\/doi.org\/10.1016\/j.jbusres.2015.07.001","2016","Big Data consumer analytics and the transformation of marketing"],["A big data analytics-enabled transformation model based on practice-based view is developed, which reveals the causal relationships among big data analytics capabilities, IT-enabled transformation practices, benefit dimensions, and business values. This model was then tested in healthcare setting. By analyzing big data implementation cases, we sought to understand how big data analytics capabilities transform organizational practices, thereby generating potential benefits. In addition to conceptually defining four big data analytics capabilities, the model offers a strategic view of big data analytics. Three significant path-to-value chains were identified for healthcare organizations by applying the model, which provides practical insights for managers.","Yichuan Wang and LeeAnn Kung and William Yu Chung Wang and Casey G. Cegielski","https:\/\/doi.org\/10.1016\/j.im.2017.04.001","2018","An integrated big data analytics-enabled transformation model: Application to health care"],["Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the \u03bb-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the \u03bb-architecture, the Semantic Layer, Bolster\u00a0is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster\u00a0in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster\u00a0allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.","Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abell\u00f3 and Xavier Franch and Stijn Vansummeren and Danilo Valerio","https:\/\/doi.org\/10.1016\/j.infsof.2017.06.001","2017","A software reference architecture for semantic-aware Big Data systems"],["The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.","Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos","https:\/\/doi.org\/10.1016\/j.comnet.2017.06.013","2017","The role of big data analytics in Internet of Things"],["Recently, there has been a shifting focus of organizations and governments towards digitization of academic and technical documents, adding a new facet to the concept of digital libraries. The volume, variety and velocity of this generated data, satisfies the big data definition, as a result of which, this scholarly reserve is popularly referred to as big scholarly data. In order to facilitate data analytics for big scholarly data, architectures and services for the same need to be developed. The evolving nature of research problems has made them essentially interdisciplinary. As a result, there is a growing demand for scholarly applications like collaborator discovery, expert finding and research recommendation systems, in addition to several others. This research paper investigates the current trends and identifies the existing challenges in development of a big scholarly data platform, with specific focus on directions for future research and maps them to the different phases of the big data lifecycle.","Samiya Khan and Xiufeng Liu and Kashish A. Shakil and Mansaf Alam","https:\/\/doi.org\/10.1016\/j.ipm.2017.03.006","2017","A survey on scholarly data: From big data perspective"],["Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader\u2019s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.","Danette McGilvray","https:\/\/doi.org\/10.1016\/B978-0-12-818015-0.00021-9","2021","Chapter 1 - Data Quality and the Data-Dependent World"],["Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.","Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia","https:\/\/doi.org\/10.1016\/j.jnca.2019.102447","2020","Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals"],["According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.","Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne","https:\/\/doi.org\/10.1016\/j.comnet.2019.107035","2020","CYBELE \u2013 Fostering precision agriculture & livestock farming through secure access to large-scale HPC enabled virtual industrial experimentation environments fostering scalable big data analytics"],["Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.","Songnian Li and Suzana Dragicevic and Francesc Ant\u00f3n Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng","https:\/\/doi.org\/10.1016\/j.isprsjprs.2015.10.012","2016","Geospatial big data handling theory and methods: A review and research challenges"],["In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject\u2019s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Na\u00efve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.","Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi","https:\/\/doi.org\/10.1016\/j.future.2019.06.004","2019","Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques"],["The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.","Ersan Kabalci and Yasin Kabalci","https:\/\/doi.org\/10.1016\/B978-0-12-819710-3.00008-9","2019","Chapter 8 - Big data, privacy and security in smart grids"],["Data generation has increased drastically over the past few years. Data management has also grown in importance because extracting the significant value out of a huge pile of raw data is of prime important thing to make different decisions. One of the important sectors nowadays is construction sector, especially building energy efficiency field. Collecting big amount of data, using different kinds of big data analysis can help to improve construction process from the energy efficiency perspective. This article reviews the understanding of Big Data, methods used for Big Data analysis and the main problems with Big Data in the field of energy.","Natalija Koseleva and Guoda Ropaite","https:\/\/doi.org\/10.1016\/j.proeng.2017.02.064","2017","Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges"],["Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes\u2019 measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes\u2019 observations.","Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin","https:\/\/doi.org\/10.1016\/j.is.2021.101951","2022","Data quality challenges in large-scale cyber-physical systems: A systematic review"],["One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications\u2019 considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.","Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama","https:\/\/doi.org\/10.1016\/j.jpdc.2014.01.003","2014","Trends in big data analytics"],["Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.","Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi","https:\/\/doi.org\/10.1016\/j.jpdc.2014.08.007","2015","Adaptive, scalable and reliable monitoring of big data on clouds"],["The transferring of medical records into huge electronic databases has opened up opportunities for research but requires attention to data quality, study design and issues of bias and confounding.","Linda D. Sharples","https:\/\/doi.org\/10.1016\/j.spl.2018.02.044","2018","The role of statistics in the era of big data: Electronic health records for healthcare research"],["Next-generation sequencing (NGS) is considered to be a prominent example of \u201cbig data\u201d because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.","Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier","https:\/\/doi.org\/10.1016\/j.jval.2018.06.016","2018","Using \u201cBig Data\u201d in the Cost-Effectiveness Analysis of Next-Generation Sequencing Technologies: Challenges and Potential Solutions"],["Research on the adoption of systems for big data analytics has drawn enormous attention in Information Systems research. This study extends big data analytics adoption research by examining the effects of system characteristics on the attitude of managers towards the usage of big data analytics systems. A research model has been proposed in this study based on an extensive review of literature pertaining to the Technology Acceptance Model, with further validation by a survey of 150 big data analytics users. Results of this survey confirm that characteristics of the big data analytics system have significant direct and indirect effects on belief in the benefits of big data analytics systems and perceived usefulness, attitude and adoption. Moreover, there are mediation effects that exist among the system characteristics, benefits of big data analytics systems, perceived usefulness and the attitude towards using big data analytics system. This study expands the existing body of knowledge on the adoption of big data analytics systems, and benefits big data analytics providers and vendors while helping in the formulation of their business models.","Surabhi Verma and Som Sekhar Bhattacharyya and Saurav Kumar","https:\/\/doi.org\/10.1016\/j.ipm.2018.01.004","2018","An extension of the technology acceptance model in the big data analytics system implementation environment"],["The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.","Prableen Kaur and Manik Sharma and Mamta Mittal","https:\/\/doi.org\/10.1016\/j.procs.2018.05.020","2018","Big Data and Machine Learning Based Secure Healthcare Framework"],["It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.","Qiumei Ouyang and Chao Wu and Lang Huang","https:\/\/doi.org\/10.1016\/j.ssci.2017.08.012","2018","Methodologies, principles and prospects of applying big data in safety science research"],["A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.","Akemi Takeoka Chatfield and Christopher G. Reddick","https:\/\/doi.org\/10.1016\/j.giq.2017.11.002","2018","Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services"],["Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications\u2019 requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.","Ahmed Oussous and Fatima-Zahra Benjelloun and Ayoub {Ait Lahcen} and Samir Belfkih","https:\/\/doi.org\/10.1016\/j.jksuci.2017.06.001","2018","Big Data technologies: A survey"],["Drawing on a revelatory case study, we identify four big data analytics (BDA) actualization mechanisms: (1) enhancing, (2) constructing, (3) coordinating, and (4) integrating, which manifest in actions on three socio-technical system levels, i.e., the structure, actor, and technology levels. We investigate the actualization of four BDA affordances at an automotive manufacturing company, i.e., establishing customer-centric marketing, provisioning vehicle-data-driven services, data-driven vehicle developing, and optimizing production processes. This study introduces a theoretical perspective to BDA research that explains how organizational actions contribute to actualizing BDA affordances. We further provide practical implications that can help guide practitioners in BDA adoption.","Christian Dremel and Matthias M. Herterich and Jochen Wulf and Jan {vom Brocke}","https:\/\/doi.org\/10.1016\/j.im.2018.10.007","2020","Actualizing big data analytics affordances: A revelatory case study"],["This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.","Chiara Bachechi and Laura Po and Federica Rollo","https:\/\/doi.org\/10.1016\/j.bdr.2021.100292","2022","Big Data Analytics and Visualization in Traffic Monitoring"],["Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.","Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio","https:\/\/doi.org\/10.1016\/j.cities.2018.04.011","2018","Smart cities with big data: Reference models, challenges, and considerations"],["Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.","Zhihong Li","https:\/\/doi.org\/10.1016\/j.micpro.2021.103928","2021","Search query of English translation text based on embedded system and big data"],["Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.","Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang","https:\/\/doi.org\/10.1016\/j.egypro.2018.12.046","2019","Lithium-ion battery modeling based on Big Data"],["Background\nThe application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care.\nPurpose\nThis systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges.\nData sources\nA systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered.\nStudy selection\nDescriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected.\nData extraction\nTwo reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare.\nResults\nA total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare.\nConclusion\nThis review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.","Nishita Mehta and Anil Pandit","https:\/\/doi.org\/10.1016\/j.ijmedinf.2018.03.013","2018","Concurrence of big data analytics and healthcare: A systematic review"],["In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.","Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang","https:\/\/doi.org\/10.1016\/j.ijpe.2017.06.006","2017","A multidisciplinary perspective of big data in management research"],["Big Data has emerged as a significant area of study for both practitioners and researchers. Big Data is a term for massive data sets with large structure. In 2012, Big Data passed the top of the Gartner Hype Cycle, attesting the maturity level of this technology and its applications. The aim of this paper is to examine how do researchers grasp the big data concept? We will answer the following questions: How many research papers are produced? What is the annual trend of publications? What are the hot topics in big data research? What are the most investigated big data topics? Why the research is performed? What are the most frequently obtained research artefacts? What does big data research produces? Who are the active authors? Which journals include papers on Big Data? What are the active disciplines? For this purpose, we provide a framework identifying existing and emerging research areas of Big Data. This framework is based on eight dimensions, including the SMACIT (Social Mobile Analytics Cloud Internet of Things) perspective. Current and past research in Big Data are analyzed using a systematic mapping study of publications based on more than a decade of related academic publications. The results have shown that significant contributions have been made by the research community, attested by a continuous increase in the number of scientific publications that address Big Data. We found that researchers are increasingly involved in research combining Big Data and Analytics, Cloud, Internet of things, mobility or social media. As for quality objectives, besides an interest in performance, other topics as scalability is emerging. Moreover, security and quality aspects become important. Researchers on Big Data provide more algorithms, frameworks, and architectures than other artifacts. Finally, application domains such as earth, energy, medicine, ecology, marketing, and health attract more attention from researchers on big data. A complementary content analysis on a subset of papers sheds some light on the evolving field of big data research.","Jacky Akoka and Isabelle Comyn-Wattiau and Nabil Laoufi","https:\/\/doi.org\/10.1016\/j.csi.2017.01.004","2017","Research on Big Data \u2013 A systematic mapping study"],["The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.","Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan","https:\/\/doi.org\/10.1016\/j.jbi.2019.103149","2019","Experience and reflection from China\u2019s Xiangya medical big data project"],["Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.","Jo\u00e3o Canito and Pedro Ramos and S\u00e9rgio Moro and Paulo Rita","https:\/\/doi.org\/10.1016\/j.compind.2018.03.018","2018","Unfolding the relations between companies and technologies under the Big Data umbrella"],["Background and objective\nIn the medical field, data volume is increasingly growing, and traditional methods cannot manage it efficiently. In biomedical computation, the continuous challenges are: management, analysis, and storage of the biomedical data. Nowadays, big data technology plays a significant role in the management, organization, and analysis of data, using machine learning and artificial intelligence techniques. It also allows a quick access to data using the NoSQL database. Thus, big data technologies include new frameworks to process medical data in a manner similar to biomedical images. It becomes very important to develop methods and\/or architectures based on big data technologies, for a complete processing of biomedical image data.\nMethod\nThis paper describes big data analytics for biomedical images, shows examples reported in the literature, briefly discusses new methods used in processing, and offers conclusions. We argue for adapting and extending related work methods in the field of big data software, using Hadoop and Spark frameworks. These provide an optimal and efficient architecture for biomedical image analysis. This paper thus gives a broad overview of big data analytics to automate biomedical image diagnosis. A workflow with optimal methods and algorithm for each step is proposed.\nResults\nTwo architectures for image classification are suggested. We use the Hadoop framework to design the first, and the Spark framework for the second. The proposed Spark architecture allows us to develop appropriate and efficient methods to leverage a large number of images for classification, which can be customized with respect to each other.\nConclusions\nThe proposed architectures are more complete, easier, and are adaptable in all of the steps from conception. The obtained Spark architecture is the most complete, because it facilitates the implementation of algorithms with its embedded libraries.","Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and Ren\u00e9 Tchinda","https:\/\/doi.org\/10.1016\/j.imu.2018.05.001","2018","An optimal big data workflow for biomedical image analysis"],[null,"Daniel I. McIsaac","https:\/\/doi.org\/10.1016\/j.bja.2020.01.012","2020","Real-world evaluation of enhanced recovery after surgery: big data under the microscope"],["Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.","Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn","https:\/\/doi.org\/10.1016\/j.oceaneng.2018.08.050","2018","Energy-efficient shipping: An application of big data analysis for optimizing engine speed of inland ships considering multiple environmental factors"],["What is the role of Statistics in the era of big data, or\u00a0is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of \u2018bigness\u2019 of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.","E. Marian Scott","https:\/\/doi.org\/10.1016\/j.spl.2018.02.050","2018","The role of Statistics in the era of big data: Crucial, critical and under-valued"],["In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.","Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang","https:\/\/doi.org\/10.1016\/j.bdr.2015.01.006","2015","Significance and Challenges of Big Data Research"],["Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.","Ari Wibisono and Devvi Sarwinda","https:\/\/doi.org\/10.1016\/j.knosys.2019.03.019","2019","Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm for big data prediction"],["Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today\u2019s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.","Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad","https:\/\/doi.org\/10.1016\/j.egyr.2017.11.002","2018","Power systems big data analytics: An assessment of paradigm shift barriers and prospects"],["The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2\u20137.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.","Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi","https:\/\/doi.org\/10.1016\/j.egyr.2021.09.205","2021","Visual recognition processing of power monitoring data based on big data computing"],["Context\nBig Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.\nObjective\nWe aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.\nMethod\nWe used Systematic Literature Review (SLR) method for reviewing 74 papers.\nResult\nOur findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.\nConclusion\nOur study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.","Faheem Ullah and Muhammad {Ali Babar}","https:\/\/doi.org\/10.1016\/j.jss.2019.01.051","2019","Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review"],["Introduction\nEstimating PM2.5 concentrations and their prediction uncertainties at a high spatiotemporal resolution is important for air pollution health effect studies. This is particularly challenging for California, which has high variability in natural (e.g, wildfires, dust) and anthropogenic emissions, meteorology, topography (e.g. desert surfaces, mountains, snow cover) and land use.\nMethods\nUsing ensemble-based deep learning with big data fused from multiple sources we developed a PM2.5 prediction model with uncertainty estimates at a high spatial (1\u00a0km\u00a0\u00d7\u00a01\u00a0km) and temporal (weekly) resolution for a 10-year time span (2008\u20132017). We leveraged autoencoder-based full residual deep networks to model complex nonlinear interrelationships among PM2.5 emission, transport and dispersion factors and other influential features. These included remote sensing data (MAIAC aerosol optical depth (AOD), normalized difference vegetation index, impervious surface), MERRA-2 GMI Replay Simulation (M2GMI) output, wildfire smoke plume dispersion, meteorology, land cover, traffic, elevation, and spatiotemporal trends (geo-coordinates, temporal basis functions, time index). As one of the primary predictors of interest with substantial missing data in California related to bright surfaces, cloud cover and other known interferences, missing MAIAC AOD observations were imputed and adjusted for relative humidity and vertical distribution. Wildfire smoke contribution to PM2.5 was also calculated through HYSPLIT dispersion modeling of smoke emissions derived from MODIS fire radiative power using the Fire Energetics and Emissions Research version 1.0 model.\nResults\nEnsemble deep learning to predict PM2.5 achieved an overall mean training RMSE of 1.54\u00a0\u03bcg\/m3 (R2: 0.94) and test RMSE of 2.29\u00a0\u03bcg\/m3 (R2: 0.87). The top predictors included M2GMI carbon monoxide mixing ratio in the bottom layer, temporal basis functions, spatial location, air temperature, MAIAC AOD, and PM2.5 sea salt mass concentration. In an independent test using three long-term AQS sites and one short-term non-AQS site, our model achieved a high correlation (>0.8) and a low RMSE (<3 \u03bcg\/m3). Statewide predictions indicated that our model can capture the spatial distribution and temporal peaks in wildfire-related PM2.5. The coefficient of variation indicated highest uncertainty over deciduous and mixed forests and open water land covers.\nConclusion\nOur method can be generalized to other regions, including those having a mix of major urban areas, deserts, intensive smoke events, snow cover and complex terrains, where PM2.5 has previously been challenging to predict. Prediction uncertainty estimates can also inform further model development and measurement error evaluations in exposure and health studies.","Lianfa Li and Mariam Girguis and Frederick Lurmann and Nathan Pavlovic and Crystal McClure and Meredith Franklin and Jun Wu and Luke D. Oman and Carrie Breton and Frank Gilliland and Rima Habre","https:\/\/doi.org\/10.1016\/j.envint.2020.106143","2020","Ensemble-based deep learning for estimating PM2.5 over California with multisource big data including wildfire smoke"],["Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core \u2018techno-optimist\u2019 tenets from a more \u2018policy-pessimist\u2019 angle. In the conclusion we have these two narratives meet \u2018eye-to-eye\u2019, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.","Simon Vydra and Bram Klievink","https:\/\/doi.org\/10.1016\/j.giq.2019.05.010","2019","Techno-optimism and policy-pessimism in the public sector big data debate"],["All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.","Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow","https:\/\/doi.org\/10.1016\/j.envsci.2021.06.011","2021","A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong"],["This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 \u201cPartnerships for the Goals\u201d. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous \u2018exhaust trail\u2019 of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality \u2013 that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.","David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan","https:\/\/doi.org\/10.1016\/j.jbusres.2020.09.035","2021","Exploring future challenges for big data in the humanitarian domain"],["Big data analytics and machine learning are the promising fields of the present time and playing important role in the healthcare sector. Big data analytical techniques help in analyzing a huge volume of data which may be in structured, semistructured, or unstructured form, and extract meaningful information for effective decision-making. Machine learning techniques help in performing predictions with the trained models on the input datasets and perform classification, clustering of data. In this chapter, the author has performed data analysis on diabetic patients dataset categorical in nature using big data analytical techniques, i.e., MapReduce, Apache Pig, Apache Hive, Apache Spark, and their architectures are discussed. Apart from big data analytics, machine learning techniques, i.e., K-Nearest Neighbor, Decision Trees, Bagged Trees, are implemented on the female diabetic patient dataset which is categorical and numerical for performing predictions based on the attributes like Age, Body Mass Index, Glucose, Blood Pressure, etc. The sensitivity achieved by the decision tree is 61.2% which is higher compared to KNN and bagged tree, whereas the Specificity achieved by the KNN is 89.2% which is higher than the other two algorithms.","Pratiyush Guleria","https:\/\/doi.org\/10.1016\/B978-0-323-91907-4.00018-2","2022","Chapter 15 - Predictions on diabetic patient datasets using big data analytics and machine learning techniques"],["Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.","Pei Wang and Ming Luo","https:\/\/doi.org\/10.1016\/j.jmsy.2020.11.012","2021","A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing"],["Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.","Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang","https:\/\/doi.org\/10.1016\/j.inffus.2019.06.016","2020","Urban big data fusion based on deep learning: An overview"],["Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.","Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante","https:\/\/doi.org\/10.1016\/j.indmarman.2020.03.015","2020","A multi-dimension framework for value creation through Big Data"],["Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.","Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum","https:\/\/doi.org\/10.1053\/j.seminoncol.2020.02.006","2020","Using big data in pediatric oncology: Current applications and future directions"],["The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term \u201cBig Data\u201d can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.","Giorgia Carra and Jorge I.F. Salluh and Fernando Jos\u00e9 {da Silva Ramos} and Geert Meyfroidt","https:\/\/doi.org\/10.1016\/j.jcrc.2020.09.002","2020","Data-driven ICU management: Using Big Data and algorithms to improve outcomes"],["Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.","Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri}","https:\/\/doi.org\/10.1016\/j.comcom.2021.05.008","2021","A big data based architecture for collaborative networks: Supply chains mixed-network"],["The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.","Lihong Zhao","https:\/\/doi.org\/10.1016\/j.eti.2020.101196","2021","Prediction model of ecological environmental water demand based on big data analysis"],["The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.","\u00c1lvaro Valencia-Parra and \u00c1ngel Jes\u00fas Varela-Vaca and Luisa Parody and Mar\u00eda Teresa G\u00f3mez-L\u00f3pez","https:\/\/doi.org\/10.1016\/j.jocs.2020.101180","2020","Unleashing Constraint Optimisation Problem solving in Big Data environments"],["The growing ubiquity of location\/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage\/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from \u201cwhat happened\u201d and \u201cwhy did it happen\u201d to \u201cwhat's likely to happen in the future\u201d and \u201cwhat to do next\u201d. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume\/velocity\/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.","Haosheng Huang and Xiaobai Angela Yao and Jukka M. Krisp and Bin Jiang","https:\/\/doi.org\/10.1016\/j.compenvurbsys.2021.101712","2021","Analytics of location-based big data for smart cities: Opportunities, challenges, and future directions"],["Precision medicine is a medical model that recommends custom-tailored products, techniques, treatments, and decisions for a subgroup of patients having the same biological basis of diseases. Due to the huge size and complexity of omics data and dataset of patient features, they cannot be analyzed directly by doctors. Big data is a term used for complex or large datasets that cannot be accurately processed or stored by traditional management tools. Omics and electronic health record (EHR) data are essential big biomedical data having a strong association with precision medicine. In this chapter, we review the importance of analyzing EHR and omics data in precision medicine. Big data analytics has been applied to healthcare in biomarker discovery and disease subtyping, drug repurposing, and integrating omics data into EHR. This will provide the most appropriate and efficient treatment to every patient on the basis of their subtyping data.","Saurabh Biswas and Yasha Hasija","https:\/\/doi.org\/10.1016\/B978-0-323-91907-4.00005-4","2022","Chapter 6 - Big data analytics in precision medicine"],["While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.","Jos\u00e9 Carpio-Pinedo and Javier Guti\u00e9rrez","https:\/\/doi.org\/10.1016\/j.cities.2020.102859","2020","Consumption and symbolic capital in the metropolitan space: Integrating \u2018old\u2019 retail data sources with social big data"],[null,"A Miracolo and M Mills and P Kanavos","https:\/\/doi.org\/10.1016\/j.jval.2021.11.1002","2022","POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review"],["Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark\u2019s in-memory computing ability (as a master-slave distribution computing) and GA\u2019s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.","Ines Sbai and Saoussen Krichen","https:\/\/doi.org\/10.1016\/j.procs.2020.09.089","2020","A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems"],["The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.","Agnieszka Smalec","https:\/\/doi.org\/10.1016\/j.procs.2021.09.293","2021","Big Data as a tool helpful in communication management"],["AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.","Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui","https:\/\/doi.org\/10.1016\/j.envsci.2021.09.001","2021","AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making"],["We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the \u201cphysics of notations\u201d, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users.\u00a0Participants mostly agreed that BiDaML was straightforward to understand\/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.","Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He","https:\/\/doi.org\/10.1016\/j.cola.2020.100964","2020","An end-to-end model-based approach to support big data analytics development"],["Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human\u2019s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.","Devendra Kumar Mishra and Arvind Kumar Upadhyay and Sanjiv Sharma","https:\/\/doi.org\/10.1016\/j.matpr.2021.05.146","2021","An efficient approach for manufacturing process using Big data analytics"],["The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the \u201cbig data\u201d era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.","Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh","https:\/\/doi.org\/10.1016\/j.genrep.2020.100869","2020","Big data in biology: The hope and present-day challenges in it"],["Light-duty gasoline vehicles (LDGVs) have made up >90 % of vehicle fleets in China since 2019, moreover, with a high annual growth rate (> 10 %) since 2017. Hence, accurate estimates of air pollutant emissions of these fast-changing LDGVs are vital for air quality management, human healthcare, and ecological protection. However, this issue is poorly quantified due to insufficient reserves of timely updated LDGV emission factors, which are dependent on real-world activity levels. Here we constructed a big dataset of explicit emission profiles (e.g., emission factors and accumulated mileages) for 159,051 LDGVs based on an official I\/M database by matching real-time traffic dynamics via real-world traffic monitoring (e.g., traffic volumes and speeds). Consequently, we provide robust evidence that the emission factors of these LDGVs follow a clear heavy-tailed distribution. The top 10 % emitters contributed >60 % to the total fleet emissions, while the bottom 50 % contributed <10 %. Such emission factors were effectively reduced by 75.7\u201386.2 % as official emission standards upgraded gradually (i.e., from China 2 to China 5) within 13 years from 2004 to 2017. Nevertheless, such achievements would be offset once traffic congestion occurred. In the real world, the typical traffic congestions (i.e., vehicle speed <5 km\/h) can lead to emissions 5\u2013 9 times higher than those on non-congested roads (i.e., vehicle speed >50 km\/h). These empirical analyses enabled us to propose future traffic scenarios that could harmonize emission standards and traffic congestion. Practical approaches on vehicle emission controls under realistic conditions are proposed, which would provide new insights for future urban vehicle emission management.","Xue Chen and Linhui Jiang and Yan Xia and Lu Wang and Jianjie Ye and Tangyan Hou and Yibo Zhang and Mengying Li and Zhen Li and Zhe Song and Jiali Li and Yaping Jiang and Pengfei Li and Xiaoye Zhang and Yang Zhang and Daniel Rosenfeld and John H. Seinfeld and Shaocai Yu","https:\/\/doi.org\/10.1016\/j.scitotenv.2022.157581","2022","Quantifying on-road vehicle emissions during traffic congestion using updated emission factors of light-duty gasoline vehicles and real-world traffic monitoring big data"],["This chapter aims to discuss healthcare's development in China and the privacy and security risk factors in medical data under big data. First, the development status of China's healthcare sector is analyzed. The questionnaire is made to analyze the privacy and security risk factors of healthcare big data (HBD) and protection measures are proposed according to the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions and medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all increase annually. In 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The questionnaire results reveal that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system are all greater than 0.8 when HBD is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to emphasize data privacy protection and grasp using digital medical data to provide decision support for subsequent medical data analysis.","Zhihan Lv and Liang Qiao","https:\/\/doi.org\/10.1016\/B978-0-323-91907-4.00020-0","2022","Chapter 19 - Privacy security risks of big data processing in healthcare"],["Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.","Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu","https:\/\/doi.org\/10.1016\/j.drudis.2020.07.005","2020","Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling"],["This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the \u201cBig Data Journey\u201d status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.","Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli","https:\/\/doi.org\/10.1016\/j.bdr.2021.100192","2021","A Big Data Analytics Architecture for Smart Cities and Smart Companies"],["Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.","Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle","https:\/\/doi.org\/10.1016\/j.ecoinf.2020.101136","2020","A review of drought monitoring with big data: Issues, methods, challenges and research directions"],["Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on \u03c9-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.","Denglong Lv and Shibing Zhu","https:\/\/doi.org\/10.1016\/j.cose.2020.101937","2020","Achieving secure big data collection based on trust evaluation and true data discovery"],["The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser\u2013Meyer\u2013Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.","Chunhui Wen and Jinhai Yang and Liu Gan and Yang Pan","https:\/\/doi.org\/10.1016\/j.future.2021.06.003","2021","Big data driven Internet of Things for credit evaluation and early warning in finance"],["The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.","Karim Moharm","https:\/\/doi.org\/10.1016\/j.aei.2019.100945","2019","State of the art in big data applications in microgrid: A review"],["Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.","Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree","https:\/\/doi.org\/10.1016\/j.hlc.2021.04.023","2021","A Versatile Big Data Health System for Australia: Driving Improvements in Cardiovascular Health"],["The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis \u2026, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.","Harika Vanam and Jeberson {Retna Raj R}","https:\/\/doi.org\/10.1016\/j.matpr.2020.11.486","2021","Analysis of twitter data through big data based sentiment analysis approaches"],["Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.","Yuan Su and Yanni Yu and Ning Zhang","https:\/\/doi.org\/10.1016\/j.scitotenv.2020.138984","2020","Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis"],["In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.","Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar","https:\/\/doi.org\/10.1016\/j.ifacol.2021.08.100","2021","From Big Data to Smart Data: Application to performance management"],["For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.","Shadi Shahoud and Moritz Winter and Hatem Khalloof and Clemens Duepmeier and Veit Hagenmeyer","https:\/\/doi.org\/10.1016\/j.iot.2021.100432","2021","An extended Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualizationz Technologies"],["Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.","Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra","https:\/\/doi.org\/10.1016\/j.bdr.2019.03.001","2019","Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications"],["Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.","Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim","https:\/\/doi.org\/10.1016\/j.techfore.2018.07.043","2018","Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach"],["Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.","Vikrant Sharma and Atul Kumar and Mukesh Kumar","https:\/\/doi.org\/10.1016\/j.matpr.2021.03.374","2021","A framework based on BWM for big data analytics (BDA) barriers in manufacturing supply chains"],["In oncology, the term \u201cbig data\u201d broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.","Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez","https:\/\/doi.org\/10.1016\/j.semradonc.2019.05.002","2019","Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery"],["With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data\u2013driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.","Zikun Feng and Yan Li and Zhao Liu and Ryan Wen Liu","https:\/\/doi.org\/10.1016\/B978-0-12-822060-3.00006-1","2021","Chapter 10 - Spatiotemporal Big Data-Driven Vessel Traffic Risk Estimation for Promoting Maritime Healthcare: Lessons Learnt from Another Domain than Healthcare"],["The rapid growth of urban populations worldwide imposes new challenges on citizens\u2019 daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens\u2019 lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens\u2019 quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.","Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Gh\u00e9zala","https:\/\/doi.org\/10.1016\/j.cosrev.2020.100303","2020","Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions"],["Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.","Lidong Wang and Cheryl Alexander","https:\/\/doi.org\/10.1016\/B978-0-12-822884-5.00017-9","2021","Chapter 2 - Big data in personalized healthcare"],["Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.","Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu","https:\/\/doi.org\/10.1016\/j.jag.2021.102514","2021","Integrating remote sensing and geospatial big data for urban land use mapping: A review"],["This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.","Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia","https:\/\/doi.org\/10.1016\/j.techfore.2020.120315","2020","Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms"],[null,"Ahmed Salem and Kevin Franks and Alastair Greystoke and Gerard G. Hanna and Stephen Harrow and Matthew Hatton and Crispin Hiley and Fiona McDonald and Corinne Faivre-Finn","https:\/\/doi.org\/10.1016\/j.jtho.2022.02.010","2022","Unaccounted Confounders Limit the Ability to Draw Conclusions From Big Data Analysis Comparing Radiotherapy Fractionation Regimens in NSCLC"],["ABSTRACT\nThe significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.","Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi","https:\/\/doi.org\/10.1016\/j.techfore.2020.120420","2021","Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities"],["Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO\/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.","David G. Rosado and Julio Moreno and Luis E. S\u00e1nchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fern\u00e1ndez-Medina","https:\/\/doi.org\/10.1016\/j.cose.2020.102155","2021","MARISMA-BiDa pattern: Integrated risk analysis for big data"],["With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.","Uma Narayanan and Varghese Paul and Shelbi Joseph","https:\/\/doi.org\/10.1016\/j.jksuci.2020.05.005","2022","A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment"],["Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the \u201cBelt and Road\u201d International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.","Ruiyang Li and Yahan Yang and Haotian Lin","https:\/\/doi.org\/10.1016\/j.imed.2021.04.004","2021","The critical need to establish standards for data quality in intelligent medicine"],["This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.","Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan","https:\/\/doi.org\/10.1016\/j.im.2018.12.003","2019","Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view"],["With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.","Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimar\u00e3es","https:\/\/doi.org\/10.1016\/j.nimb.2020.05.027","2020","Ion beam analysis and big data: How data science can support next-generation instrumentation"],["Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.","Canchu Lin and Anand Kunnathur","https:\/\/doi.org\/10.1016\/j.jbusres.2019.07.016","2019","Strategic orientations, developmental culture, and big data capability"],["Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.","Qiuping Ma and Hongyan Li and Anders Thorstenson","https:\/\/doi.org\/10.1016\/j.cie.2021.107580","2021","A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving"],["In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.","Diego Garc\u00eda-Gil and Juli\u00e1n Luengo and Salvador Garc\u00eda and Francisco Herrera","https:\/\/doi.org\/10.1016\/j.ins.2018.12.002","2019","Enabling Smart Data: Noise filtering in Big Data classification"],["According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro\/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.","Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani","https:\/\/doi.org\/10.1016\/j.chemosphere.2021.133422","2022","Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family"],["The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.","Harpreet Singh","https:\/\/doi.org\/10.1016\/j.matpr.2020.07.170","2021","Big data, industry 4.0 and cyber-physical systems integration: A smart industry context"],["Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5\u00a0V\u2019s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals\/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.","Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii","https:\/\/doi.org\/10.1016\/j.tele.2020.101517","2021","Big data analytics meets social media: A systematic review of techniques, open issues, and future directions"],["Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. S\u00e3o Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.","Renato Arbex and Claudio B. Cunha","https:\/\/doi.org\/10.1016\/j.jtrangeo.2020.102671","2020","Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data"],["Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology\u2013Organization\u2013Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.","Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi","https:\/\/doi.org\/10.1016\/j.ipm.2019.102095","2019","Big data adoption: State of the art and research challenges"],["Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe\u2013Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http:\/\/github.org\/eubr-bigsea, https:\/\/hub.docker.com\/u\/eubrabigsea\/).","Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and N\u00e1dia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira","https:\/\/doi.org\/10.1016\/j.future.2019.02.011","2019","BIGSEA: A Big Data analytics platform for public transportation information"],["Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.","Balan Sundarakani and Aneesh Ajaykumar and Angappa Gunasekaran","https:\/\/doi.org\/10.1016\/j.omega.2021.102452","2021","Big data driven supply chain design and applications for blockchain: An action research using case study approach"],["Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.","Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante","https:\/\/doi.org\/10.1016\/j.indmarman.2019.08.004","2020","A multi-dimension framework for value creation through big data"],["Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.","Ant\u00f3nio AC Vieira and Lu\u00eds MS Dias and Maribel Y Santos and Guilherme AB Pereira and Jos\u00e9 A Oliveira","https:\/\/doi.org\/10.1016\/j.simpat.2019.101985","2020","On the use of simulation as a Big Data semantic validator for supply chain management"],["The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.","Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin","https:\/\/doi.org\/10.1016\/j.cofs.2020.11.006","2020","Big Data in food safety- A review"],["There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.","Nelson Lozada and Jose Arias-P\u00e9rez and Geovanny Perdomo-Charry","https:\/\/doi.org\/10.1016\/j.heliyon.2019.e02541","2019","Big data analytics capability and co-innovation: An empirical study"],["Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs\u2019 potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability\/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO\/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.","Mert Onuralp G\u00f6kalp and Ebru G\u00f6kalp and Kerem Kayabay and Selin G\u00f6kalp and Altan Ko\u00e7yi\u011fit and P. Erhan Eren","https:\/\/doi.org\/10.1016\/j.csi.2021.103585","2022","A process assessment model for big data analytics"],["This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.","Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa","https:\/\/doi.org\/10.1016\/j.epsr.2020.106788","2020","Big data analytics for future electricity grids"],["This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality\u2019s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.","Tibor Koltay","https:\/\/doi.org\/10.1016\/B978-0-12-824475-3.00004-7","2022","Chapter 3 - Data quality, the essential \u201cingredient\u201d"],["Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.","Yesheng Cui and Sami Kara and Ka C. Chan","https:\/\/doi.org\/10.1016\/j.rcim.2019.101861","2020","Manufacturing big data ecosystem: A systematic literature review"],["The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.","Patricia {L\u00f3pez Mart\u00ednez} and Ricardo Dint\u00e9n and Jos\u00e9 Mar\u00eda Drake and Marta Zorrilla","https:\/\/doi.org\/10.1016\/j.future.2021.06.020","2021","A big data-centric architecture metamodel for Industry 4.0"],["Has the rise of data-intensive science, or \u2018big data\u2019, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.","Robert Northcott","https:\/\/doi.org\/10.1016\/j.shpsa.2019.09.002","2020","Big data and prediction: Four case studies"],["The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.","Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi","https:\/\/doi.org\/10.1016\/j.matpr.2020.02.249","2020","Big-data driven approaches in materials science: A survey"],["This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.","Laura Sebastian-Coleman","https:\/\/doi.org\/10.1016\/B978-0-12-821737-5.00001-8","2022","Chapter 1 - The Importance of Data Quality Management"],["Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.","Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor","https:\/\/doi.org\/10.1016\/j.jii.2021.100236","2021","CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0"],["With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project\u2019s Governance, Architecture, Data Governance and the BDA Project\u2019s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.","Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}","https:\/\/doi.org\/10.1016\/j.jksuci.2020.11.024","2022","Big data analytics in telecommunications: Governance, architecture and use cases"],["The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.","Hugo Bergier and Lo\u00efc Duron and Christelle Sordet and Lou Kawka and Aur\u00e9lien Schlencker and Fran\u00e7ois Chasset and Laurent Arnaud","https:\/\/doi.org\/10.1016\/j.autrev.2021.102864","2021","Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?"],["The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm\u2019s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm\u2019s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC\u2019s and a firm\u2019s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.","Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie","https:\/\/doi.org\/10.1016\/j.im.2020.103361","2020","The role of information governance in big data analytics driven innovation"],["Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars\u2019 attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., \u201cDynamic Capabilities\u201d, \u201cResource-Based View\u201d, \u201cTechnology Acceptance Model\u201d, \u201cDiffusion of Innovation\u201d, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.","Mohamed Aboelmaged and Samar Mouakket","https:\/\/doi.org\/10.1016\/j.ipm.2020.102234","2020","Influencing models and determinants in big data analytics research: A bibliometric analysis"],["Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.","Gabriele Morra and David A. Yuen and Henry M. Tufo and Matthew G. Knepley","https:\/\/doi.org\/10.1016\/B978-0-08-102908-4.00111-9","2021","Fresh Outlook on Numerical Methods for Geodynamics. Part 2: Big Data, HPC, Education"],["Today, data is one of the most valuable assets on the planet. As a valuable resource, data may be used to develop a wide range of data applications, all of which are driven by creativity and innovation. In order to obtain information and provide services, data is also a critical component. In the recent years, big data has become a popular topic in global discussion. Big data is a new technology and knowledge generation phenomenon, that record, capture, and execute a significant amount of data for the usage in a variety of domains such as research, education, business, investing, health, and so on. The proliferation of data inspired by new methods of data gatherings such as via social media, wireless sensors, and data from government agencies which makes big data management an ultimate challenge. This study includes a thorough evaluation of existing theories and practical approach to address the public sector open data issues for determining the determinants of government public open data (GPOD) development of big data. To investigate the revolution of GPOD for health, the framework was dominantly used over architecture, infrastructures, followed by theoretical and conceptual framework, according to the review. This study revealed that most of the existing frameworks still lack consideration of the requirement for public open data in health. There is less number of existing research works that have sophisticated big data frameworks in GPOD for health. There also is still a lack of investment and adoption of big data in the public sector. The findings of this chapter will help academicians to empirically study the revealed requirement and provide decision-makers a better knowledge of how to leverage GPOD adoption in health by taking appropriate actions.","Najhan {Muhamad Ibrahim} and Nur Hidayah Ilham {Ahmad Azri} and Norbik Bashah Idris","https:\/\/doi.org\/10.1016\/B978-0-323-91907-4.00024-8","2022","Chapter 4 - Towards big data framework in government public open data (GPOD) for health"],["We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain\u2019s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.","Claudio A. Ardagna and Valerio Bellandi and Ernesto Damiani and Michele Bezzi and Cedric Hebert","https:\/\/doi.org\/10.1016\/j.compeleceng.2021.107215","2021","Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists"],["This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.","Xin Li and Rob Law","https:\/\/doi.org\/10.1016\/j.tmp.2019.100608","2020","Network analysis of big data research in tourism"],["The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.","Huamao Wang and Yumei Yao and Said Salhi","https:\/\/doi.org\/10.1016\/j.techfore.2020.120175","2020","Tension in big data using machine learning: Analysis and applications"],["The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of \u2018Big Data Analytics\u2019 (BDA) as a mediator between \u2018sustainable supply chain business performance\u2019 and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.","Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Manoj Dora and Mengqi Liu","https:\/\/doi.org\/10.1016\/j.tre.2020.102170","2021","Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains"],["With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010\u20132018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.","Mustafa Y\u0131ld\u0131r\u0131m and Feyza Y\u0131ld\u0131r\u0131m Okay and Suat \u00d6zdemir","https:\/\/doi.org\/10.1016\/j.eswa.2021.114840","2021","Big data analytics for default prediction using graph theory"],["In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs\u2019 ability to extract value from big data, helping IAFs to enhance their activities\u2019 efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs\u2019 DA usage. From the literature, we identify five main variables expected to be associated with IAFs\u2019 DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs\u2019 ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs\u2019 soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs\u2019 involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.","Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza","https:\/\/doi.org\/10.1016\/j.intaccaudtax.2020.100357","2021","Correlates of the internal audit function\u2019s use of data analytics in the big data era: Global evidence"],["Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these \u201cbig data\u201d, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.","Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti","https:\/\/doi.org\/10.1016\/j.cirp.2020.05.002","2020","Big data analytics for smart factories of the future"],["As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.","Devarshi Shah and Jin Wang and Q. Peter He","https:\/\/doi.org\/10.1016\/j.compchemeng.2020.106970","2020","Feature engineering in big data analytics for IoT-enabled smart manufacturing \u2013 Comparison between deep learning and statistical learning"],["Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called \u201cBig Services\u201d. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.","Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid","https:\/\/doi.org\/10.1016\/j.jnca.2020.102732","2020","On the use of big data frameworks for big service composition"],["Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.","Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu","https:\/\/doi.org\/10.1016\/j.im.2019.103231","2022","Orchestrating big data analytics capability for sustainability: A study of air pollution management in China"],["Collecting useful and informative data play an essential role in ensuring the performance of data-driven solutions for intelligent maintenance. However, there is still a lack of methodology to systematically assess the data usefulness (or data suitability) for modeling. This lack of data suitability assessment becomes a more pressing issue in the big data environment where a large volume of machine data is generated at a high velocity. Therefore, there are imperative needs for standardized procedures and systematic solutions that can scan through a large amount of data to quantify the data suitability and locate the useful datasets for model development. To fill in this gap, this paper proposes a novel methodology to evaluate the data suitability for PHM modeling from the aspects of detectability assessment, diagnosability assessment, and prognosability assessment. In the discussion, new assessment procedures and algorithms are proposed by using a series of similarity metrics between data vectors or data distribution. Also, the proposed methods provide both visualization tools and quantitative metrics to assess the data suitability. The effectiveness of the methodology is demonstrated by using real-world examples about the ball screw degradation and boring tool degradation. The results successfully demonstrate the effectiveness and practicality of the proposed methodology and analytics.","Xiaodong Jia and Da-Yan Ji and Takanobu Minami and Jay Lee","https:\/\/doi.org\/10.1016\/j.ifacol.2022.09.183","2022","Data Quality and Usability Assessment Methodology for Prognostics and Health Management: A Systematic Framework"],["Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.","Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He","https:\/\/doi.org\/10.1016\/j.jclepro.2020.123646","2020","Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data"],["With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.","Dongfang Ren and Xiaopeng Guo and Cunbin Li","https:\/\/doi.org\/10.1016\/j.jclepro.2021.128154","2021","Research on big data analysis model of multi energy power generation considering pollutant emission\u2014Empirical analysis from Shanxi Province"],["This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.","Victor Chang and Mohamed Aleem Ali and Alamgir Hossain","https:\/\/doi.org\/10.1016\/B978-0-323-90054-6.00007-6","2022","Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning"],["The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.","Matthew Russell and Peng Wang","https:\/\/doi.org\/10.1016\/j.ymssp.2021.108709","2022","Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring"],["Zusammenfassung\nHintergrund\nIn Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden k\u00f6nnen. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept f\u00fcr den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-gef\u00f6rderten Projekts \u201eBIDA-SE\u201c wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einflie\u00dfen k\u00f6nnen.\nMethode\nZiel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, \u00f6konomischer Implikationen sowie Grenzen und Barrieren f\u00fcr dessen mittelfristige Umsetzung zu evaluieren. F\u00fcr die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N\u00a0=\u00a09 \u00c4rzt*innen, N\u00a0=\u00a069 Patient*innen mit seltenen Erkrankungen\/Patientenvertreter*innen, N\u00a0=\u00a014 IT-Expert*innen und N\u00a0=\u00a021 Versorgungsforscher*innen durchgef\u00fchrt. F\u00fcr die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte prim\u00e4r deskriptiv durch eine Analyse von H\u00e4ufigkeiten, Mittelwerten und Standardabweichungen.\nErgebnisse\nDie Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (\u00c4rzt*innen, Patient*innen\/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erf\u00e4hrt. Aus Sicht der \u00c4rzt*innen, Patient*innen\/Patientenvertreter*innen und Versorgungsforscher*innen h\u00e4tte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektoren\u00fcbergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung w\u00fcrden sich aus Sicht der \u00c4rzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios m\u00fcsste jedoch eine Anpassung der Verg\u00fctungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren f\u00fcr eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards\/Datenquellen\/Datenqualit\u00e4t, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verf\u00fcgbarkeit sowie (7) Gewohnheiten und Pr\u00e4ferenzen\/Arztrolle.\nDiskussion\nMit der vorliegenden Studie wurde ein erstes fach\u00fcbergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen\/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zuk\u00fcnftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erf\u00e4hrt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) n\u00fctzlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren f\u00fcr dessen mittelfristige Umsetzung \u00fcberwunden werden m\u00fcssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gew\u00e4hrleiten und den Zugang zu den Zentren f\u00fcr Seltene Erkrankungen zuk\u00fcnftig zu kanalisieren.\nSchlussfolgerung\nAuf nationaler Ebene wurden zahlreiche Aktivit\u00e4ten angesto\u00dfen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts \u201eBIDA-SE\u201c entwickelte Szenario erg\u00e4nzt diese Forschungsaktivit\u00e4ten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden k\u00f6nnen, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu k\u00f6nnen.\nIntroduction\nIn Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project \u201cBIDA-SE\u201d, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.\nMethods\nThe aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October\/November 2019 amongst a total of N\u00a0=\u00a09 physicians, N\u00a0=\u00a069 patients with rare diseases\/patient representatives, N\u00a0=\u00a014 IT experts and N\u00a0=\u00a021 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.\nResults\nThe results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients\/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients\/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician\u2019s and health care researcher\u2019s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards\/data sources\/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences\/physician's role.\nDiscussion\nWith the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations\/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.\nConclusion\nMany activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the \u201cBIDA-SE\u201d project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.","Brita Sedlmayr and Andreas Knapp and Mich\u00e9le K\u00fcmmel and Franziska Bathelt and Martin Sedlmayr","https:\/\/doi.org\/10.1016\/j.zefq.2020.11.002","2020","Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen f\u00fcr die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen"],["This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.","Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris","https:\/\/doi.org\/10.1016\/j.ijinfomgt.2021.102347","2021","Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda"],["Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and\/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.","Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}","https:\/\/doi.org\/10.1016\/j.elerap.2019.100921","2020","The impact of big data on firm performance in hotel industry"],["Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72\u20134.6 and the coefficients of determination of 0.932\u20130.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.","Sukwon Ji and Bumho Lee and Mun Yong Yi","https:\/\/doi.org\/10.1016\/j.buildenv.2021.108267","2021","Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach"],["The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.","Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright","https:\/\/doi.org\/10.1016\/j.semradonc.2019.05.006","2019","Use of Big Data for Quality Assurance in Radiation Therapy"]]}